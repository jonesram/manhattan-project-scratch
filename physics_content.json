[{"topic": "Abbe number", "content": "In optics and lens design the Abbe number also known as the Vnumber or constringence of a transparent material is a measure of the materials dispersion variation of refractive index versus wavelength with high values of V indicating low dispersion. It is named after Ernst Abbe 18401905 the German physicist who defined it. The Abbe number VD of a material is defined as V D  n D 1 n F n C   takes the difference between the refractive indices of the blue and red cadmium lines at 480.0 nm and 643.8 nm with ne referring to the wavelength of the mercury Eline 546.073 nm. Other definitions can similarly be employed the following table lists standard wavelengths at which n is commonly determined including the standard subscripts employed.  Abbe graph and data for 356 glasses from Ohara Hoya and Schott"}, {"topic": "Absolute electrode potential", "content": "Absolute electrode potential in electrochemistry according to an IUPAC definition is the electrode potential of a metal measured with respect to a universal reference system without any additional metalsolution interface. "}, {"topic": "Absolute humidity", "content": "Humidity is the amount of water vapor in the air. Water vapor is the gaseous state of water and is invisible. Humidity indicates the likelihood of precipitation dew or fog. Higher humidity reduces the effectiveness of sweating in cooling the body by reducing the rate of evaporation of moisture from the skin. This effect is calculated in a heat index table or humidex. The amount of water vapor that is needed to achieve saturation increases as the temperature increases. As temperature decreases the amount of water vapor needed to reach saturation also decreases. As the temperature of a parcel of air becomes lower it will eventually reach the point of saturation without adding or losing water mass. The differences in the amount of water vapor in a parcel of air can be quite large for example A parcel of air that is near saturation may contain 28 grams of water per cubic meter of air at 1 C but only 8 grams of water per cubic meter of air at 12 C There are three main measurements of humidity absolute relative and specific. Absolute humidity is the water content of air at a given temperature expressed in gram per cubic metre. Relative humidity expressed as a percent measures the current absolute humidity relative to the maximum highest point for that temperature. Specific humidity is a ratio of the water vapor content of the mixture to the total air content on a mass basis.  Glossary definition of absolute humidity National Science Digital Library Glossary definition of psychrometric tables National Snow and Ice Data Center Glossary definition of specific humidity National Snow and Ice Data Center FREE Humidity  Dewpoint Calculator Vaisala Free Windows Program Dewpoint Units Conversion Calculator PhyMetrix Free Online Humidity Calculator Calculate about 16 parameters online with the Rotronic Humidity Calculator Calculator for humidity sensor and ebook Calculator for humidity sensor for rapid conversion of humidity measurements. Uniquely includes measurement uncertainties and ebook Principles of Humidity Measurement"}, {"topic": "Absolute magnitude", "content": "Absolute magnitude is the measure of intrinsic brightness of a celestial object. It is the hypothetical apparent magnitude of an object at a standard distance of exactly 10 parsecs 32.6 light years from the observer assuming no astronomical extinction of starlight. This places the objects on a common basis and allows the true energy output of astronomical objects to be compared without the distortion introduced by distance. As with all astronomical magnitudes the absolute magnitude can be specified for different wavelength intervals for stars the most commonly quoted absolute magnitude is the absolute visual magnitude which uses only the visual V band of the spectrum UBV system. Also commonly used is the absolute bolometric magnitude which is the total luminosity expressed in magnitude units that takes into account energy radiated at all wavelengths whether visible or not. The brighter the celestial object the smaller its absolute magnitude. The magnitude scale extends downward through the positive numbers and into the negative numbers as brightness increases. A difference of 1.0 in absolute magnitude corresponds to a ratio of 2.512 100.4 of absolute brightness. Therefore a star of magnitude 2 is 100 or 2.5125 times brighter than a star of magnitude 3. The Milky Way for example has an absolute magnitude of about 20.5 so a quasar with an absolute magnitude of 25.5 is 100 times brighter than the Milky Way. If this particular quasar and the Milky Way could be seen side by side at the same distance of one parsec and the Milky Ways stars reduced to a single point the quasar would be 5 magnitudes or 100 times brighter than the Milky Way. Similarly Canopus has an absolute visual magnitude of about 5.5 whereas Ross 248 has an absolute visual magnitude of 14.8 for a difference of about 20 magnitudes i.e. Canopus would be seen as about 20 magnitudes brighter stated another way Canopus emits more than 100 million 108 times more visual power than Ross 248.  Reference zeromagnitude fluxes International Astronomical Union Absolute Magnitude of a Star calculator The Magnitude system About stellar magnitudes Obtain the magnitude of any star SIMBAD Converting magnitude of minor planets to diameter Another table for converting asteroid magnitude to estimated diameter"}, {"topic": "Absolute motion", "content": "Absolute space and time is a concept in physics and philosophy about the properties of the universe. In physics absolute space and time may be a preferred frame. In philosophy and religion it may represent Gods view of the universe. "}, {"topic": "Absolute pressure", "content": "Many techniques have been developed for the measurement of pressure and vacuum. Instruments used to measure pressure are called pressure gauges or vacuum gauges. A manometer is an instrument that uses a column of liquid to measure pressure although the term is currently often used to mean any pressure measuring instrument. A vacuum gauge is used to measure the pressure in a vacuumwhich is further divided into two subcategories high and low vacuum and sometimes ultrahigh vacuum. The applicable pressure ranges of many of the techniques used to measure vacuums have an overlap. Hence by combining several different types of gauge it is possible to measure system pressure continuously from 10 mbar down to 1011 mbar.  Home Made Manometer Manometer"}, {"topic": "Absolute scale", "content": "An absolute scale is a system of measurement that begins at a minimum or zero point and progresses in only one direction. An absolute scale differs from an arbitrary or relative scale which begins at some point selected by a person and can progress in both directions. An absolute scale begins at a natural minimum leaving only one direction in which to progress. An absolute scale can only be applied to measurements in which a true minimum is known to exist. Time for example which does not have a clearly known beginning is measured on a relative scale with an arbitrary zeropoint such as the conventional date of the birth of Jesus of Nazareth or the accession of an emperor. Temperature on the other hand has a known minimum absolute zero where all vibrational motion of atoms ceases and therefore can be measured either in absolute terms kelvins or degrees Rankine or relative to a reference temperature such as the freezing point of water at a specified pressure Celsius and Reaumur or the lowest temperature attainable in 1724 Fahrenheit. Pressure is a force that can be measured absolutely because the natural minimum of pressure is total vacuum. Pressure is frequently measured with reference to atmospheric pressure rather than on any absolute scale relative to complete and perfect vacuum it is technologically simpler and cheaper. It may also be more convenient to use relative scales because with things like pneumatics and hydraulics the amount of energy transferred is reduced by the relative backpressure of the atmosphere. e.g. 15 psi of air in a tank at sea level will become 30 psi in the vacuum of space. Therefore with measurements of things like blood pressure or tire pressure a measurement relative to air pressure is a better indication of burst pressure damage threshold than an absolute scale. Absolute scales are typically used in science deep vacuum measurements where the fluctuating pressure of the atmosphere becomes a nuisance aeronautics where precise measurements of the atmosphere are needed to determine altitude or lighting construction where the relative pressure of the atmosphere is inconsequential and are measured in units of atmospheres or torr. Barometers do measure absolute pressure by holding a vacuum at the top of the mercury column or one side of a diaphragm but that vacuum is awkward to achieve and maintain. Thus while the general public may be familiar with measurements of absolute pressure from weather forecasts most pressures such as tire pressures and water pressures are measured relative to atmospheric pressure using cheaper and simpler pressure gauges. For this reason the pressure relative to atmospheric pressure is called gauge pressure and measurements given in units like pounds per square inch abbreviated lbfin2 or psi are often shown as psig the g standing for gauge or psia a for absolute. Absolute scales are used when precise values are needed in comparison to a natural unchanging zero point. Measurements of length area and volume are inherently absolute although measurements of distance are often based on an arbitrary starting point. Measurements of weight can be absolute such as atomic weight but more often they are measurements of the relationship between two masses while measurements of speed are relative to an arbitrary reference frame. Unlike many other measurements without a known absolute minimum speed has a known maximum and can be measured from a purely relative scale. Absolute scales can be used for measuring a variety of things from the flatness of an optical flat to neuroscientific tests. "}, {"topic": "Absolute zero", "content": "Absolute zero is the lower limit of the thermodynamic temperature scale a state at which the enthalpy and entropy of a cooled ideal gas reaches its minimum value taken as 0. The theoretical temperature is determined by extrapolating the ideal gas law by international agreement absolute zero is taken as 273.15 on the Celsius scale International System of Units which equates to 459.67 on the Fahrenheit scale United States customary units or Imperial units. The corresponding Kelvin and Rankine temperature scales set their zero points at absolute zero by definition. It is commonly thought of as the lowest temperature possible but it is not the lowest enthalpy state possible because all real substances begin to depart from the ideal gas when cooled as they approach the change of state to liquid and then to solid and the sum of the enthalpy of vaporization gas to liquid and enthalpy of fusion liquid to solid exceeds the ideal gass change in enthalpy to absolute zero. In the quantummechanical description matter solid at absolute zero is in its ground state the point of lowest internal energy. The laws of thermodynamics dictate that absolute zero cannot be reached using only thermodynamic means as the temperature of the substance being cooled approaches the temperature of the cooling agent asymptotically. A system at absolute zero still possesses quantum mechanical zeropoint energy the energy of its ground state at absolute zero. The kinetic energy of the ground state cannot be removed. Scientists have achieved temperatures close to absolute zero where matter exhibits quantum effects such as superconductivity and superfluidity.  Absolute zero a two part NOVA episode originally aired January 2008 What is absolute zero Lansing state journal"}, {"topic": "Absorbance", "content": "Optical density redirects here. Optical density can also refer to index of refraction. In chemistry absorbance or decadic absorbance is the common logarithm of the ratio of incident to transmitted radiant power through a material and spectral absorbance or spectral decadic absorbance is the common logarithm of the ratio of incident to transmitted spectral radiant power through a material. Absorbance is dimensionless and in particular is not a length though it is a monotonically increasing function of path length and approaches zero as the path length approaches zero. The use of the term optical density for absorbance is discouraged. In physics a closely related quantity called optical depth is used instead of absorbance the natural logarithm of the ratio of incident to transmitted radiant power through a material. The optical depth equals the absorbance times ln10. The term absorption refers to the physical process of absorbing light while absorbance does not always measure absorption it measures attenuation of transmitted radiant power. Attenuation can be caused by absorption but also reflection scattering and other physical processes. "}, {"topic": "Absorption spectroscopy", "content": "Absorption spectroscopy refers to spectroscopic techniques that measure the absorption of radiation as a function of frequency or wavelength due to its interaction with a sample. The sample absorbs energy i.e. photons from the radiating field. The intensity of the absorption varies as a function of frequency and this variation is the absorption spectrum. Absorption spectroscopy is performed across the electromagnetic spectrum. Absorption spectroscopy is employed as an analytical chemistry tool to determine the presence of a particular substance in a sample and in many cases to quantify the amount of the substance present. Infrared and ultravioletvisible spectroscopy are particularly common in analytical applications. Absorption spectroscopy is also employed in studies of molecular and atomic physics astronomical spectroscopy and remote sensing. There are a wide range of experimental approaches for measuring absorption spectra. The most common arrangement is to direct a generated beam of radiation at a sample and detect the intensity of the radiation that passes through it. The transmitted energy can be used to calculate the absorption. The source sample arrangement and detection technique vary significantly depending on the frequency range and the purpose of the experiment.  Solar absorption spectrum Visible Absorption Spectrum Simulation Plot Absorption Intensity for many molecules in HITRAN database"}, {"topic": "Accelerating universe", "content": "The accelerating expansion of the universe is the observation that the universe appears to be expanding at an increasing rate. In formal terms this means that the cosmic scale factor a  t    70 kms1Mpc1 the time remaining before the universe ends in this Big Rip is 22 billion years. "}, {"topic": "Acceleration", "content": "Acceleration in physics is the rate of change of velocity of an object with respect to time. An objects acceleration is the net result of any and all forces acting on the object as described by Newtons Second Law. The SI unit for acceleration is metre per second squared m s2. Accelerations are vector quantities they have magnitude and direction and add according to the parallelogram law. As a vector the calculated net force is equal to the product of the objects mass a scalar quantity and its acceleration. For example when a car starts from a standstill zero relative velocity and travels in a straight line at increasing speeds it is accelerating in the direction of travel. If the car turns there is an acceleration toward the new direction. In this example we can call the forward acceleration of the car a linear acceleration which passengers in the car might experience as a force pushing them back into their seats. When changing direction we might call this nonlinear acceleration which passengers might experience as a sideways force. If the speed of the car decreases this is an acceleration in the opposite direction from the direction of the vehicle sometimes called deceleration. Passengers may experience deceleration as a force lifting them forwards. Mathematically there is no separate formula for deceleration both are changes in velocity. Each of these accelerations linear nonlinear deceleration might be felt by passengers until their velocity speed and direction matches that of the car.  Acceleration Calculator Simple acceleration unit converter Measurespeed.com  Acceleration Calculator Based on starting  ending speed and time elapsed."}, {"topic": "Accelerator physics", "content": "Accelerator physics is a branch of applied physics concerned with designing building and operating particle accelerators. As such it can be described as the study of motion manipulation and observation of relativistic charged particle beams and their interaction with accelerator structures by electromagnetic fields. It is also related to other fields Microwave engineering for accelerationdeflection structures in the radio frequency range. Optics with an emphasis on geometrical optics beam focusing and bending and laser physics laserparticle interaction. Computer technology with an emphasis on digital signal processing e.g. for automated manipulation of the particle beam. The experiments conducted with particle accelerators are not regarded as part of accelerator physics but belong according to the objectives of the experiments to e.g. particle physics nuclear physics condensed matter physics or materials physics. The types of experiments done at a particular accelerator facility are determined by characteristics of the generated particle beam such as average energy particle type intensity and dimensions.  UCBLBL Beam Physics site BNL page on The Alternating Gradient Concept"}, {"topic": "Accelerometer", "content": "An accelerometer is a device that measures proper acceleration proper acceleration is not the same as coordinate acceleration rate of change of velocity. For example an accelerometer at rest on the surface of the Earth will measure an acceleration due to Earths gravity straight upwards by definition of g 9.81 ms2. By contrast accelerometers in free fall falling toward the center of the Earth at a rate of about 9.81 ms2 will measure zero. Accelerometers have multiple applications in industry and science. Highly sensitive accelerometers are components of inertial navigation systems for aircraft and missiles. Accelerometers are used to detect and monitor vibration in rotating machinery. Accelerometers are used in tablet computers and digital cameras so that images on screens are always displayed upright. Accelerometers are used in drones for flight stabilisation. Coordinated accelerometers can be used to measure differences in proper acceleration particularly gravity over their separation in space i.e. gradient of the gravitational field. This gravity gradiometry is useful because absolute gravity is a weak effect and depends on local density of the Earth which is quite variable. Single and multiaxis models of accelerometer are available to detect magnitude and direction of the proper acceleration as a vector quantity and can be used to sense orientation because direction of weight changes coordinate acceleration vibration shock and falling in a resistive medium a case where the proper acceleration changes since it starts at zero then increases. Micromachined accelerometers are increasingly present in portable electronic devices and video game controllers to detect the position of the device or provide for game input. "}, {"topic": "Acoustics", "content": "Acoustics is the interdisciplinary science that deals with the study of all mechanical waves in gases liquids and solids including topics such as vibration sound ultrasound and infrasound. A scientist who works in the field of acoustics is an acoustician while someone working in the field of acoustics technology may be called an acoustical engineer. The application of acoustics is present in almost all aspects of modern society with the most obvious being the audio and noise control industries. Hearing is one of the most crucial means of survival in the animal world and speech is one of the most distinctive characteristics of human development and culture. Accordingly the science of acoustics spreads across many facets of human societymusic medicine architecture industrial production warfare and more. Likewise animal species such as songbirds and frogs use sound and hearing as a key element of mating rituals or marking territories. Art craft science and technology have provoked one another to advance the whole as in many other fields of knowledge. Robert Bruce Lindsays Wheel of Acoustics is a well accepted overview of the various fields in acoustics. The word acoustic is derived from the Greek word akoustikos meaning of or for hearing ready to hear and that from akoustos heard audible which in turn derives from the verb akouo I hear. The Latin synonym is sonic after which the term sonics used to be a synonym for acoustics and later a branch of acoustics. Frequencies above and below the audible range are called ultrasonic and infrasonic respectively.  Acoustical Society of America Institute of Acoustic in UK National Council of Acoustical Consultants International Commission for Acoustics Institute of Noise Control Engineers"}, {"topic": "Adhesion", "content": "Adhesion is the tendency of dissimilar particles or surfaces to cling to one another cohesion refers to the tendency of similar or identical particlessurfaces to cling to one another. The forces that cause adhesion and cohesion can be divided into several types. The intermolecular forces responsible for the function of various kinds of stickers and sticky tape fall into the categories of chemical adhesion dispersive adhesion and diffusive adhesion. In addition to the cumulative magnitudes of these intermolecular forces there are certain emergent mechanical effects.  John Comyn Adhesion Science Royal Society of Chemistry Paperbacks 1997 A.J. Kinloch Adhesion and Adhesives Science and Technology Chapman and Hall 1987"}, {"topic": "Adiabatic cooling", "content": "An adiabatic process is one that occurs without transfer of heat or matter between a thermodynamic system and its surroundings. In an adiabatic process energy is transferred only as work. The adiabatic process provides a rigorous conceptual basis for the theory used to expound the first law of thermodynamics and as such it is a key concept in thermodynamics. Some chemical and physical processes occur so rapidly that they may be conveniently described by the adiabatic approximation meaning that there is not enough time for the transfer of energy as heat to take place to or from the system. By way of example the adiabatic flame temperature is an idealization that uses the adiabatic approximation so as to provide an upper limit calculation of temperatures produced by combustion of a fuel. The adiabatic flame temperature is the temperature that would be achieved by a flame if the process of combustion took place in the absence of heat loss to the surroundings.  Article in HyperPhysics Encyclopaedia"}, {"topic": "Adiabatic heating", "content": "An adiabatic process is one that occurs without transfer of heat or matter between a thermodynamic system and its surroundings. In an adiabatic process energy is transferred only as work. The adiabatic process provides a rigorous conceptual basis for the theory used to expound the first law of thermodynamics and as such it is a key concept in thermodynamics. Some chemical and physical processes occur so rapidly that they may be conveniently described by the adiabatic approximation meaning that there is not enough time for the transfer of energy as heat to take place to or from the system. By way of example the adiabatic flame temperature is an idealization that uses the adiabatic approximation so as to provide an upper limit calculation of temperatures produced by combustion of a fuel. The adiabatic flame temperature is the temperature that would be achieved by a flame if the process of combustion took place in the absence of heat loss to the surroundings.  Article in HyperPhysics Encyclopaedia"}, {"topic": "Aerodynamics", "content": "Aerodynamics from Greek aer air  dynamics is a branch of fluid dynamics concerned with studying the motion of air particularly when it interacts with a solid object such as an airplane wing. Aerodynamics is a subfield of fluid dynamics and gas dynamics and many aspects of aerodynamics theory are common to these fields. The term aerodynamics is often used synonymously with gas dynamics with the difference being that gas dynamics applies to the study of the motion of all gases not limited to air. Formal aerodynamics study in the modern sense began in the eighteenth century although observations of fundamental concepts such as aerodynamic drag have been recorded much earlier. Most of the early efforts in aerodynamics worked towards achieving heavierthanair flight which was first demonstrated by Wilbur and Orville Wright in 1903. Since then the use of aerodynamics through mathematical analysis empirical approximations wind tunnel experimentation and computer simulations has formed the scientific basis for ongoing developments in heavierthanair flight and a number of other technologies. Recent work in aerodynamics has focused on issues related to compressible flow turbulence and boundary layers and has become increasingly computational in nature.  NASA Beginners Guide to Aerodynamics Smithsonian National Air and Space Museums How Things Fly website Aerodynamics for Students Aerodynamics for Pilots Aerodynamics and Race Car Tuning Aerodynamic Related Projects eFluids Bicycle Aerodynamics Application of Aerodynamics in Formula One F1 Aerodynamics in Car Racing Aerodynamics of Birds Aerodynamics and dragonfly wings"}, {"topic": "Afocal system", "content": "In optics an afocal system a system without focus is an optical system that produces no net convergence or divergence of the beam i.e. has an infinite effective focal length. This type of system can be created with a pair of optical elements where the distance between the elements is equal to the sum of each elements focal length d  f1f2. A simple example of an afocal optical system is an optical telescope imaging a star the light entering the system is at infinity and the image it forms is at infinity the light is collimated. Although the system does not alter the divergence of a collimated beam it does alter the width of the beam increasing magnification. The magnification of such a telescope is given by M  f 2 f 1   Afocal systems are used in laser optics for instance as beam expanders Infrared and forward looking infrared systems camera zoom lenses and telescopic lens attachments such as teleside converters and photography setups combining cameras and telescopes Afocal photography. "}, {"topic": "Agrophysics", "content": "Agrophysics is a branch of science bordering on agronomy and physics whose objects of study are the agroecosystem  the biological objects biotope and biocoenosis affected by human activity studied and described using the methods of physical sciences. Using the achievements of the exact sciences to solve major problems in agriculture agrophysics involves the study of materials and processes occurring in the production and processing of agricultural crops with particular emphasis on the condition of the environment and the quality of farming materials and food production. Agrophysics is closely related to biophysics but is restricted to the biology of the plants animals soil and an atmosphere involved in agricultural activities and biodiversity. It is different from biophysics in having the necessity of taking into account the specific features of biotope and biocoenosis which involves the knowledge of nutritional science and agroecology agricultural technology biotechnology genetics etc. The needs of agriculture concerning the past experience study of the local complex soil and next plantatmosphere systems lay at the root of the emergence of a new branch agrophysics dealing this with experimental physics. The scope of the branch starting from soil science physics and originally limited to the study of relations within the soil environment expanded over time onto influencing the properties of agricultural crops and produce as foods and raw postharvest materials and onto the issues of quality safety and labeling concerns considered distinct from the field of nutrition for application in food science. Research centres focused on the development of the agrophysical sciences include the Institute of Agrophysics Polish Academy of Sciences in Lublin and the Agrophysical Research Institute Russian Academy of Sciences in St. Petersburg.  Agrophysical Research Institute of the Russian Academy of Agricultural Sciences Bohdan Dobrzaski Institute of Agrophysics Polish Academy of Sciences in Lublin Free Association of PMA Labs Czech University of Agriculture Prague International Agrophysics International Agrophysics  quarterly journal focused on applications of physics in environmental and agricultural sciences Polish Society of Agrophysics Sustainable Agriculture Definitions and Terms"}, {"topic": "Air mass", "content": "In meteorology an air mass is a volume of air defined by its temperature and water vapor content. Air masses cover many hundreds or thousands of square miles and adapt to the characteristics of the surface below them. They are classified according to latitude and their continental or maritime source regions. Colder air masses are termed polar or arctic while warmer air masses are deemed tropical. Continental and superior air masses are dry while maritime and monsoon air masses are moist. Weather fronts separate air masses with different density temperature andor moisture characteristics. Once an air mass moves away from its source region underlying vegetation and water bodies can quickly modify its character. Classification schemes tackle an air mass characteristics and well as modification. "}, {"topic": "Air mass (astronomy)", "content": "In astronomy air mass or airmass is the optical path length through Earths atmosphere for light from a celestial source. As it passes through the atmosphere light is attenuated by scattering and absorption the more atmosphere through which it passes the greater the attenuation. Consequently celestial bodies at the horizon appear less bright than when at the zenith. The attenuation known as atmospheric extinction is described quantitatively by the BeerLambertBouguer law. Air mass normally indicates relative air mass the path length relative to that at the zenith at sea level so by definition the sealevel air mass at the zenith is 1. Air mass increases as the angle between the source and the zenith increases reaching a value of approximately 38 at the horizon. Air mass can be less than one at an elevation greater than sea level however most closedform expressions for air mass do not include the effects of elevation so adjustment must usually be accomplished by other means. In some fields such as solar energy and photovoltaics air mass is indicated by the acronym AM additionally the value of the air mass is often given by appending its value to AM so that AM1 indicates an air mass of 1 AM2 indicates an air mass of 2 and so on. The region above Earths atmosphere where there is no atmospheric attenuation of solar radiation is considered to have air mass zero AM0. Tables of air mass have been published by numerous authors including Bemporad 1904 Allen 1976 and Kasten and Young 1989.  Reed Meyers downloadable airmass calculator written in C notes in the source code describe the theory in detail NASA Astrophysics Data System A source for electronic copies of some of the references."}, {"topic": "Air mass (solar energy)", "content": "The air mass coefficient defines the direct optical path length through the Earths atmosphere expressed as a ratio relative to the path length vertically upwards i.e. at the zenith. The air mass coefficient can be used to help characterize the solar spectrum after solar radiation has traveled through the atmosphere. The air mass coefficient is commonly used to characterize the performance of solar cells under standardized conditions and is often referred to using the syntax AM followed by a number. AM1.5 is almost universal when characterizing terrestrial powergenerating panels. "}, {"topic": "Albedo", "content": "Albedo lbido or reflection coefficient derived from Latin albedo whiteness or reflected sunlight in turn from albus white is the diffuse reflectivity or reflecting power of a surface. It is the ratio of reflected radiation from the surface to incident radiation upon it. Its dimensionless nature lets it be expressed as a percentage and is measured on a scale from zero for no reflection of a perfectly black surface to 1 for perfect reflection of a white surface. Because albedo is the ratio of all reflected radiation to incident radiation it will include both the diffuse and specular radiation reflected from an object. It is however common to assume a surface reflects in either a totally specular manner or a totally diffuse manner as this can simplify calculations. Albedo depends on the frequency of the radiation. When quoted unqualified it usually refers to some appropriate average across the spectrum of visible light. In general the albedo depends on the directional distribution of incident radiation except for Lambertian surfaces which scatter radiation in all directions according to a cosine function and therefore have an albedo that is independent of the incident distribution. In practice a bidirectional reflectance distribution function BRDF may be required to accurately characterize the scattering properties of a surface but albedo is very useful as a first approximation. The albedo is an important concept in climatology astronomy and calculating reflectivity of surfaces in LEED sustainablerating systems for buildings. The average overall albedo of Earth its planetary albedo is 30 to 35 because of cloud cover but widely varies locally across the surface because of different geological and environmental features. The term was introduced into optics by Johann Heinrich Lambert in his 1760 work Photometria.  Official Website of Albedo Project Global Albedo Project Center for Clouds Chemistry and Climate Albedo Encyclopedia of Earth NASA MODIS BRDFalbedo product site Surface albedo derived from Meteosat observations A discussion of Lunar albedos reflectivity of metals chart"}, {"topic": "Alloy", "content": "An alloy is a mixture of metals or a mixture of a metal and another element. Alloys are defined by metallic bonding character. An alloy may be a solid solution of metal elements a single phase or a mixture of metallic phases two or more solutions. Intermetallic compounds are alloys with a defined stoichiometry and crystal structure. Zintl phases are also sometimes considered alloys depending on bond types see also Van ArkelKetelaar triangle for information on classifying bonding in binary compounds. Alloys are used in a wide variety of applications. In some cases a combination of metals may reduce the overall cost of the material while preserving important properties. In other cases the combination of metals imparts synergistic properties to the constituent metal elements such as corrosion resistance or mechanical strength. Examples of alloys are steel solder brass pewter duralumin phosphor bronze and amalgams. The alloy constituents are usually measured by mass. Alloys are usually classified as substitutional or interstitial alloys depending on the atomic arrangement that forms the alloy. They can be further classified as homogeneous consisting of a single phase or heterogeneous consisting of two or more phases or intermetallic.  RobertsAusten William Chandler 1911. Alloys  Francis Henry Neville. Encyclopdia Britannica 11th ed.. Surface Alloys Alloy. The American Cyclopdia. 1879."}, {"topic": "Alpha particle", "content": "Alpha particles consist of two protons and two neutrons bound together into a particle identical to a helium nucleus. They are generally produced in the process of alpha decay but may also be produced in other ways. Alpha particles are named after the first letter in the Greek alphabet . The symbol for the alpha particle is or 2. Because they are identical to helium nuclei they are also sometimes written as He2 or 4 2He2 indicating a helium ion with a 2 charge missing its two electrons. If the ion gains electrons from its environment the alpha particle can be written as a normal electrically neutral helium atom 4 2He. Some science authors may use doubly ionized helium nuclei He2 and alpha particles as interchangeable terms. The nomenclature is not well defined and thus not all highvelocity helium nuclei are considered by all authors to be alpha particles. As with beta and gamma raysparticles the name used for the particle carries some mild connotations about its production process and energy but these are not rigorously applied. Thus alpha particles may be loosely used as a term when referring to stellar helium nuclei reactions for example the alpha processes and even when they occur as components of cosmic rays. A higher energy version of alphas than produced in alpha decay is a common product of an uncommon nuclear fission result called ternary fission. However helium nuclei produced by particle accelerators cyclotrons synchrotrons and the like are less likely to be referred to as alpha particles. Alpha particles like helium nuclei have a net spin of zero. Due to the mechanism of their production in standard alpha radioactive decay alpha particles generally have a kinetic energy of about 5 MeV and a velocity in the vicinity of 5 the speed of light. See discussion below for the limits of these figures in alpha decay. They are a highly ionizing form of particle radiation and when resulting from radioactive alpha decay have low penetration depth. They are able to be stopped by a few centimeters of air or by the skin. However socalled long range alpha particles from ternary fission are three times as energetic and penetrate three times as far. As noted the helium nuclei that form 1012 of cosmic rays are also usually of much higher energy than those produced by nuclear decay processes and are thus capable of being highly penetrating and able to traverse the human body and also many meters of dense solid shielding depending on their energy. To a lesser extent this is also true of very highenergy helium nuclei produced by particle accelerators. When alpha particle emitting isotopes are ingested they are far more dangerous than their halflife or decay rate would suggest due to the high relative biological effectiveness of alpha radiation to cause biological damage. Alpha radiation is an average of about 20 times more dangerous and in experiments with inhaled alpha emitter up to 1000 times more dangerous than an equivalent activity of beta emitting or gamma emitting radioisotopes. In computer technology dynamic random access memory DRAM soft errors were linked to alpha particles in 1978 in Intels DRAM chips. The discovery led to strict control of radioactive elements in the packaging of semiconductor materials and the problem is largely considered to be solved. "}, {"topic": "Alternating current", "content": "Alternating current AC is an electric current in which the flow of electric charge periodically reverses direction whereas in direct current DC also dc the flow of electric charge is only in one direction. The abbreviations AC and DC are often used to mean simply alternating and direct as when they modify current or voltage. AC is the form in which electric power is delivered to businesses and residences. The usual waveform of alternating current in most electric power circuits is a sine wave. In certain applications different waveforms are used such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. These types of alternating current carry information encoded or modulated onto the AC signal such as sound audio or images video. These currents typically alternate at higher frequencies than those used in power transmission.  Alternating Current. Interactive Java tutorial explaining alternating current. National High Magnetic Field Laboratory ACDC Whats the Difference. Edisons Miracle of Light American Experience. PBS ACDC Inside the AC Generator. Edisons Miracle of Light American Experience. PBS Kuphaldt Tony R. Lessons In Electric Circuits  Volume II  AC. March 8 2003. Design Science License Nave C. R. Alternating Current Circuits Concepts. HyperPhysics. Alternating Current AC. Magnetic Particle Inspection Nondestructive Testing Encyclopedia. Alternating current. Analog Process Control Services. Hiob Eric An Application of Trigonometry and Vectors to Alternating Current. British Columbia Institute of Technology 2004. Introduction to alternating current and transformers. Integrated Publishing. Chan. Keelin Alternating current Tools. JC Physics 2002. Williams Trip Kingpin Understanding Alternating Current Some more power concepts. Table of Voltage Frequency TV Broadcasting system Radio Broadcasting by Country. Professor Mark Cseles tour of the 25 Hz Rankine generating station 5060 hertz information AC circuits Animations and explanations of vector phasor representation of RLC circuits Blalock Thomas J. The Frequency Changer Era Interconnecting Systems of Varying Cycles. The history of various frequencies and interconversion schemes in the US at the beginning of the 20th century Italian Generating an AC voltage. Interactive."}, {"topic": "Alternative hypothesis", "content": "In statistical hypothesis testing the alternative hypothesis or maintained hypothesis or research hypothesis and the null hypothesis are the two rival hypotheses which are compared by a statistical hypothesis test. In the domain of science two rival hypotheses can be compared by explanatory power and predictive power. "}, {"topic": "Ammeter", "content": "An ammeter is a measuring instrument used to measure the current in a circuit. Electric currents are measured in amperes A hence the name. Instruments used to measure smaller currents in the milliampere or microampere range are designated as milliammeters or microammeters. Early ammeters were laboratory instruments which relied on the Earths magnetic field for operation. By the late 19th century improved instruments were designed which could be mounted in any position and allowed accurate measurements in electric power systems. "}, {"topic": "Amorphous solid", "content": "In condensed matter physics and materials science an amorphous from the Greek a without morph shape form or noncrystalline solid is a solid that lacks the longrange order characteristic of a crystal. In some older books the term has been used synonymously with glass. Nowadays amorphous solid is considered to be the overarching concept and glass the more special case A glass is an amorphous solid that exhibits a glass transition. Polymers are often amorphous. Other types of amorphous solids include gels thin films and nanostructured materials such as glass. Amorphous materials have an internal structure made of interconnected structural blocks. Whether a material is liquid or solid depends primarily on the connectivity between its elementary building blocks so that solids are characterized by a high degree of connectivity whereas structural blocks in fluids have lower connectivity see figure on amorphous material states.  Journal of noncrystalline solids Elsevier"}, {"topic": "Ampere", "content": "The ampere SI unit symbol A often shortened to amp is the SI unit of electric current dimension symbol I and is one of the seven SI base units. It is named after AndrMarie Ampre 17751836 French mathematician and physicist considered the father of electrodynamics. The ampere is equivalent to one coulomb roughly 70186241000000000006.2411018 times the elementary charge per second. Amperes are used to express flow rate of electric charge. For any point experiencing a current if the number of charged particles passing through it or the charge on the particles passing through it is increased the amperes of current at that point will proportionately increase. The ampere should not be confused with the coulomb also called amperesecond or the amperehour Ah. The ampere is a unit of current the amount of charge transiting per unit time and the coulomb is a unit of charge. When SI units are used constant instantaneous and average current are expressed in amperes as in the charging current is 1.2 A and the charge accumulated or passed through a circuit over a period of time is expressed in coulombs as in the battery charge is 700430000000000000030000 C. The relation of the ampere Cs to the coulomb is the same as that of the watt Js to the joule.  The NIST Reference on Constants Units and Uncertainty NIST Definition of ampere and 0 Tutorial video explaining amperes and current"}, {"topic": "Amplifier", "content": "An amplifier electronic amplifier or informally amp is an electronic component that can increase the power of a signal. An amplifier functions by taking power from a power supply and controlling the output to match the input signal shape but with a larger amplitude. In this sense an amplifier modulates the output of the power supply based upon the properties of the input signal. An amplifier is effectively the opposite of an attenuator while an amplifier provides gain an attenuator provides loss. An amplifier can either be a discrete piece of equipment or an electrical circuit contained within another device. Amplification is fundamental to modern electronics and amplifiers are widely used in almost all electronic equipment. Amplifiers can be categorized in different ways. One is by the frequency of the electronic signal being amplified audio amplifiers amplify signals in the audio sound range of less than 20 kHz RF amplifiers amplify frequencies in the radio frequency range between 20 kHz and 300 GHz. Another is which quantity voltage or current is being amplified amplifiers can be divided into voltage amplifiers current amplifiers transconductance amplifiers and transresistance amplifiers. A further distinction is whether the output is a linear or nonlinear representation of the input. Amplifiers can also be categorized by their physical placement in the signal chain.  Rane audios guide to amplifier classes Design and analysis of a basic class D amplifier Conversion distortion factor to distortion attenuation and THD An alternate topology called the grounded bridge amplifier  pdf Contains an explanation of different amplifier classes  pdf Reinventing the power amplifier  pdf Anatomy of the power amplifier including information about classes Tons of Tones  Site explaining non linear distortion stages in Amplifier Models Class D audio amplifiers white paper  pdf Class E Radio Transmitters  Tutorials Schematics Examples and Construction Details"}, {"topic": "Amplitude", "content": "The amplitude of a periodic variable is a measure of its change over a single period such as time or spatial period. There are various definitions of amplitude see below which are all functions of the magnitude of the difference between the variables extreme values. In older texts the phase is sometimes called the amplitude. "}, {"topic": "Angle of incidence (optics)", "content": "In geometric optics the angle of incidence is the angle between a ray incident on a surface and the line perpendicular to the surface at the point of incidence called the normal. The ray can be formed by any wave optical acoustic microwave Xray and so on. In the figure below the line representing a ray makes an angle with the normal dotted line. The angle of incidence at which light is first totally internally reflected is known as the critical angle. The angle of reflection and angle of refraction are other angles related to beams. Determining the angle of reflection with respect to a planar surface is trivial but the computation for almost any other surface is significantly more difficult. The exact solution for a sphere which has important applications in astronomy and computer graphics was an open problem for nearly 50 years until a closedform result was derived by mathematicians Allen R Miller and Emanuel Vegh in 1991.  Weisstein Eric W. Angle of incidence MathWorld. geometry  rebound on the strip billiards Flash animation"}, {"topic": "Angle of incidence (optics)", "content": "In geometric optics the angle of incidence is the angle between a ray incident on a surface and the line perpendicular to the surface at the point of incidence called the normal. The ray can be formed by any wave optical acoustic microwave Xray and so on. In the figure below the line representing a ray makes an angle with the normal dotted line. The angle of incidence at which light is first totally internally reflected is known as the critical angle. The angle of reflection and angle of refraction are other angles related to beams. Determining the angle of reflection with respect to a planar surface is trivial but the computation for almost any other surface is significantly more difficult. The exact solution for a sphere which has important applications in astronomy and computer graphics was an open problem for nearly 50 years until a closedform result was derived by mathematicians Allen R Miller and Emanuel Vegh in 1991.  Weisstein Eric W. Angle of incidence MathWorld. geometry  rebound on the strip billiards Flash animation"}, {"topic": "Angle of reflection", "content": "Reflection is the change in direction of a wavefront at an interface between two different media so that the wavefront returns into the medium from which it originated. Common examples include the reflection of light sound and water waves. The law of reflection says that for specular reflection the angle at which the wave is incident on the surface equals the angle at which it is reflected. Mirrors exhibit specular reflection. In acoustics reflection causes echoes and is used in sonar. In geology it is important in the study of seismic waves. Reflection is observed with surface waves in bodies of water. Reflection is observed with many types of electromagnetic wave besides visible light. Reflection of VHF and higher frequencies is important for radio transmission and for radar. Even hard Xrays and gamma rays can be reflected at shallow angles with special grazing mirrors.  Acoustic reflection Animations demonstrating optical reflection by QED Simulation on Laws of Reflection of Sound By Amrita University"}, {"topic": "Angstrom", "content": "The ngstrm Swedish strm or angstrom is a unit of length equal to 69901000000000000001010 m one tenbillionth of a metre or 69901000000000000000.1 nm. Its symbol is  a letter in the Swedish alphabet. The natural sciences and technology often use ngstrm to express sizes of atoms molecules microscopic biological structures and lengths of chemical bonds arrangement of atoms in crystals wavelengths of electromagnetic radiation and dimensions of integrated circuit parts. Atoms of phosphorus sulfur and chlorine are about an ngstrm in covalent radius while a hydrogen atom is about half an ngstrm see atomic radius. The unit is named after the Swedish physicist Anders Jonas ngstrm 18141874. The symbol is always written with a ring diacritic as the letter in the Swedish alphabet. The units name is often written in English without the diacritics but the official definitions contain diacritics. "}, {"topic": "Angular acceleration", "content": "Angular acceleration is the rate of change of angular velocity. In SI units it is measured in radians per second squared rads2 and is usually denoted by the Greek letter alpha . "}, {"topic": "Angular displacement", "content": "Angular displacement of a body is the angle in radians degrees revolutions through which a point or line has been rotated in a specified sense about a specified axis. When an object rotates about its axis the motion cannot simply be analyzed as a particle since in circular motion it undergoes a changing velocity and acceleration at any time t. When dealing with the rotation of an object it becomes simpler to consider the body itself rigid. A body is generally considered rigid when the separations between all the particles remains constant throughout the objects motion so for example parts of its mass are not flying off. In a realistic sense all things can be deformable however this impact is minimal and negligible. Thus the rotation of a rigid body over a fixed axis is referred to as rotational motion.  Second moment of area Linear elasticity Infinitesimal rotation Angular distance Angular velocity"}, {"topic": "Angular frequency", "content": "In physics angular frequency also referred to by the terms angular speed radial frequency circular frequency orbital frequency radian frequency and pulsatance is a scalar measure of rotation rate. It refers to the angular displacement per unit time e.g. in rotation or the rate of change of the phase of a sinusoidal waveform e.g. in oscillations and waves or as the rate of change of the argument of the sine function. Angular frequency or angular speed is the magnitude of the vector quantity angular velocity. The term angular frequency vector  "}, {"topic": "Angular momentum", "content": "In physics angular momentum rarely moment of momentum or rotational momentum is the rotational analog of linear momentum. It is an important quantity in physics because it is a conserved quantity the angular momentum of a system remains constant unless acted on by an external torque. The definition of angular momentum for a point particle is a pseudovector rp the cross product of the particles position vector r relative to some origin and its momentum vector p  mv. This definition can be applied to each point in continua like solids or fluids or physical fields. Unlike momentum angular momentum does depend on where the origin is chosen since the particles position is measured from it. The angular momentum of an object can also be connected to the angular velocity of the object how fast it rotates about an axis via the moment of inertia I which depends on the shape and distribution of mass about the axis of rotation. However while always points in the direction of the rotation axis the angular momentum L may point in a different direction depending on how the mass is distributed. Angular momentum is additive the total angular momentum of a system is the pseudovector sum of the angular momenta. For continua or fields one uses integration. The total angular momentum of anything can always be split into the sum of two main components orbital angular momentum about an axis outside the object plus spin angular momentum through the centre of mass of the object. Torque can be defined as the rate of change of angular momentum analogous to force. The conservation of angular momentum helps explain many observed phenomena for example the increase in rotational speed of a spinning figure skater as the skaters arms are contracted the high rotational rates of neutron stars the falling cat problem and precession of tops and gyros. Applications include the gyrocompass control moment gyroscope inertial guidance systems reaction wheels flying discs or Frisbees and Earths rotation to name a few. In general conservation does limit the possible motion of a system but does not uniquely determine what the exact motion is. In quantum mechanics angular momentum is an operator with quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle meaning only one component can be measured with definite precision the other two cannot. Also the spin of elementary particles does not correspond to literal spinning motion.  Conservation of Angular Momentum a chapter from an online textbook Angular Momentum in a Collision Process derivation of the threedimensional case Angular Momentum and Rolling Motion  more momentum theory"}, {"topic": "Angular velocity", "content": "In physics the angular velocity is defined as the rate of change of angular displacement and is a vector quantity more precisely a pseudovector which specifies the angular speed rotational speed of an object and the axis about which the object is rotating. This speed can be measured in the SI unit of angular velocity radians per second or in terms of degrees per second degrees per hour etc. Angular velocity is usually represented by the symbol omega  rarely . The direction of the angular velocity vector is perpendicular to the plane of rotation in a direction which is usually specified by the righthand rule.  A college textbook of physics By Arthur Lalanne Kimball Angular Velocity of a particle Pickering Steve 2009.  Speed of Rotation Angular Velocity. Sixty Symbols. Brady Haran for the University of Nottingham."}, {"topic": "Annihilation", "content": "Annihilation is defined as total destruction or complete obliteration of an object having its root in the Latin nihil nothing. A literal translation is to make into nothing. In physics the word is used to denote the process that occurs when a subatomic particle collides with its respective antiparticle such as an electron colliding with a positron. Energy and momentum are conserved and the annihilated particles are replaced by photons electromagnetic wave quanta with zero rest mass. Antiparticles have exactly opposite additive quantum numbers from particles so the sums of all quantum numbers of the original pair are zero. Hence any set of particles may be produced whose total quantum numbers are also zero as long as conservation of energy and conservation of momentum are obeyed. When a particle and its antiparticle collide their energy is converted into a force carrier particle such as a gluon WZ force carrier particle or a photon. These particles are afterwards transformed into other particles. During a lowenergy annihilation photon production is favored since these particles have no mass. However highenergy particle colliders produce annihilations where a wide variety of exotic heavy particles are created. "}, {"topic": "Anode", "content": "An anode is an electrode through which conventional current flows into a polarized electrical device. A common mnemonic is ACID for anode current into device. The direction of positive electric current is opposite to the direction of electron flow negatively charged electrons flow out the anode to the outside circuit. The polarity of voltage on an anode with respect to an associated cathode varies depending on the device type and on its operating mode. In the following examples the anode is negative in a device that provides power and positive in a device that consumes power In a discharging battery or galvanic cell diagram at right the anode is the negative terminal because it is where current flows into the device i.e. the battery cell. This inward current is carried externally by electrons moving outwards negative charge flowing in one direction being electrically equivalent to positive charge flowing in the opposite direction. In a recharging battery or an electrolytic cell the anode is the positive terminal which receives current from an external generator. The current through a recharging battery is opposite to the direction of current during discharge in other words the electrode which was the cathode during battery discharge becomes the anode while the battery is recharging. In a diode the anode is the positive terminal at the tail of the arrow symbol flat side of the triangle where current flows into the device. Note electrode naming for diodes is always based on the direction of the forward current that of the arrow in which the current flows most easily even for types such as Zener diodes or solar cells where the current of interest is the reverse current. In a cathode ray tube the anode is the positive terminal where electrons flow out of the device i.e. where positive electric current flows in.  The Cathode Ray Tube site How to define anode and cathode Valence Technologies Inc. battery education page Cathodic Protection Technical Library"}, {"topic": "Antigravity", "content": "Antigravity is an idea of creating a place or object that is free from the force of gravity. It does not refer to the lack of weight under gravity experienced in free fall or orbit or to balancing the force of gravity with some other force such as electromagnetism or aerodynamic lift. Antigravity is a recurring concept in science fiction particularly in the context of spacecraft propulsion. Examples are the gravity blocking substance Cavorite in H. G. Wells The First Men in the Moon and the Spindizzy machines in James Blishs Cities in Flight. In Newtons law of universal gravitation gravity was an external force transmitted by unknown means. In the 20th century Newtons model was replaced by general relativity where gravity is not a force but the result of the geometry of spacetime. Under general relativity antigravity is impossible except under contrived circumstances. Quantum physicists have postulated the existence of gravitons a set of massless elementary particles that transmit the force and the possibility of creating or destroying these is unclear. Antigravity is often used colloquially to refer to devices that look as if they reverse gravity even though they operate through other means such as lifters which fly in the air by moving air with electromagnetic fields.  Responding to Mechanical Antigravity a NASA paper debunking a wide variety of gyroscopic and related devices Gde Scientific Foundation"}, {"topic": "Antimatter", "content": "In particle physics antimatter is a material composed of antiparticles which have the same mass as particles of ordinary matter but opposite charges as well as other particle properties such as lepton and baryon numbers. Collisions between particles and antiparticles lead to the annihilation of both giving rise to variable proportions of intense photons gamma rays neutrinos and less massive particleantiparticle pairs. The total consequence of annihilation is a release of energy available for work proportional to the total matter and antimatter mass in accord with the massenergy equivalence equation E  mc2. Antiparticles bind with each other to form antimatter just as ordinary particles bind to form normal matter. For example a positron the antiparticle of the electron and an antiproton the antiparticle of the proton can form an antihydrogen atom. Physical principles indicate that complex antimatter atomic nuclei are possible as well as antiatoms corresponding to the known chemical elements. Studies of cosmic rays have identified both positrons and antiprotons presumably produced by collisions between particles of ordinary matter. Satellitebased searches of cosmic rays for antideuteron and antihelium particles have yielded nothing. There is considerable speculation as to why the observable universe is composed almost entirely of ordinary matter as opposed to an even mixture of matter and antimatter. This asymmetry of matter and antimatter in the visible universe is one of the great unsolved problems in physics. The process by which this inequality between particles and antiparticles developed is called baryogenesis. Antimatter in the form of antiatoms is one of the most difficult materials to produce. Antimatter in the form of individual antiparticles however is commonly produced by particle accelerators and in some types of radioactive decay. The nuclei of antihelium both helium3 and helium4 have been artificially produced with difficulty. These are the most complex antinuclei so far observed.  Antimatter physics at Encyclopdia Britannica Antimatter on In Our Time at the BBC. listen now Freeview Video Antimatter by the Vega Science Trust and the BBCOU CERN Webcasts RealPlayer required What is Antimatter from the Frequently Asked Questions at the Center for AntimatterMatter Studies Angels and Demons. CERN. Archived from the original on 27 March 2014. FAQ from CERN with information about antimatter aimed at the general reader posted in response to antimatters fictional portrayal in Angels  Demons Antimatter at Angels and Demons CERN What is direct CPviolation Animated illustration of antihydrogen production at CERN from the Exploratorium."}, {"topic": "Antineutron", "content": "The antineutron is the antiparticle of the neutron with symbol n. It differs from the neutron only in that some of its properties have equal magnitude but opposite sign. It has the same mass as the neutron and no net electric charge but has opposite baryon number 1 for neutron 1 for the antineutron. This is because the antineutron is composed of antiquarks while neutrons are composed of quarks. The antineutron consists of one up antiquark and two down antiquarks. Since the antineutron is electrically neutral it cannot easily be observed directly. Instead the products of its annihilation with ordinary matter are observed. In theory a free antineutron should decay into an antiproton a positron and a neutrino in a process analogous to the beta decay of free neutrons. There are theoretical proposals that neutronantineutron oscillations exist a process which would occur only if there is an undiscovered physical process that violates baryon number conservation. The antineutron was discovered in protonantiproton collisions at the Bevatron Lawrence Berkeley National Laboratory by Bruce Cork in 1956 one year after the antiproton was discovered.  LBL Particle Data Group summary tables suppression of neutronantineutron oscillation Elementary particles includes information about antineutron discovery archived link Is Antineutron the Same as Neutron explains how the antineutron differs from the regular neutron despite having the same that is zero charge."}, {"topic": "Antiparticle", "content": "Corresponding to most kinds of particles there is an associated antimatter antiparticle with the same mass and opposite charge including electric charge. For example the antiparticle of the electron is the positively charged positron which is produced naturally in certain types of radioactive decay. The laws of nature are very nearly symmetrical with respect to particles and antiparticles. For example an antiproton and a positron can form an antihydrogen atom which is believed to have the same properties as a hydrogen atom. This leads to the question of why the formation of matter after the Big Bang resulted in a universe consisting almost entirely of matter rather than being a halfandhalf mixture of matter and antimatter. The discovery of Charge Parity violation helped to shed light on this problem by showing that this symmetry originally thought to be perfect was only approximate. Particleantiparticle pairs can annihilate each other producing photons since the charges of the particle and antiparticle are opposite total charge is conserved. For example the positrons produced in natural radioactive decay quickly annihilate themselves with electrons producing pairs of gamma rays a process exploited in positron emission tomography. Antiparticles are produced naturally in beta decay and in the interaction of cosmic rays in the Earths atmosphere. Because charge is conserved it is not possible to create an antiparticle without either destroying a particle of the same charge as in  decay when a proton positive charge is destroyed a neutron created and a positron positive charge antiparticle is also created and emitted or by creating a particle of the opposite charge. The latter is seen in many processes in which both a particle and its antiparticle are created simultaneously as in particle accelerators. This is the inverse of the particleantiparticle annihilation process. Although particles and their antiparticles have opposite charges electrically neutral particles need not be identical to their antiparticles. The neutron for example is made out of quarks the antineutron from antiquarks and they are distinguishable from one another because neutrons and antineutrons annihilate each other upon contact. However other neutral particles are their own antiparticles such as photons hypothetical gravitons and some WIMPs.  Feynman R. P. 1987. The reason for antiparticles. In R. P. Feynman S. Weinberg. The 1986 Dirac memorial lectures. Cambridge University Press. ISBN 0521340004. Weinberg S. 1995. The Quantum Theory of Fields Volume 1 Foundations. Cambridge University Press. ISBN 0521550017."}, {"topic": "Antiproton", "content": "The antiproton p pronounced pbar is the antiparticle of the proton. Antiprotons are stable but they are typically shortlived since any collision with a proton will cause both particles to be annihilated in a burst of energy. The existence of the antiproton with 1 electric charge opposite to the 1 electric charge of the proton was predicted by Paul Dirac in his 1933 Nobel Prize lecture. Dirac received the Nobel Prize for his previous 1928 publication of his Dirac Equation that predicted the existence of positive and negative solutions to the Energy Equation  E  m c 2   of Einstein and the existence of the positron the antimatter analog to the electron with positive charge and opposite spin. The antiproton was first experimentally confirmed in 1955 at the Bevatron particle accelerator by University of California Berkeley physicists Emilio Segr and Owen Chamberlain for which they were awarded the 1959 Nobel Prize in Physics. An antiproton consists of two up antiquarks and one down antiquark uud. The properties of the antiproton that have been measured all match the corresponding properties of the proton with the exception that the antiproton has electric charge and magnetic moment that are the opposites of those in the proton. The questions of how matter is different from antimatter and the relevance of antimatter in explaining how our universe survived the Big Bang remain open problemsopen in part due to the relative dearth of antimatter in todays universe. "}, {"topic": "Applied physics", "content": "Applied physics is physics which is intended for a particular technological or practical use. It is usually considered as a bridge or a connection between physics and engineering. Applied is distinguished from PUNANEY by a subtle combination of factors such as the motivation and attitude of researchers and the nature of the relationship to the technology or science that may be affected by the work. It usually differs from engineering in that an applied physicist may not be designing something in particular but rather is using physics or conducting physics research with the aim of developing new technologies or solving an engineering problem. This approach is similar to that of applied mathematics. In other words applied physics is rooted in the fundamental truths and basic concepts of the physical sciences but is concerned with the utilization of these scientific principles in practical devices and systems. Applied physicists can also be interested in the use of physics for scientific research. For instance the field of accelerator physics can contribute to research in theoretical physics by enabling design and construction of highenergy colliders. "}, {"topic": "Arc length", "content": "Determining the length of an irregular arc segment is also called rectification of a curve. Historically many methods were used for specific curves. The advent of infinitesimal calculus led to a general formula that provides closedform solutions in some cases.  Hazewinkel Michiel ed. 2001 Rectifiable curve Encyclopedia of Mathematics Springer ISBN 9781556080104 Math Before Calculus The History of Curvature Weisstein Eric W. Arc Length MathWorld. Arc Length by Ed Pegg Jr. The Wolfram Demonstrations Project 2007. Calculus Study Guide Arc Length Rectification Famous Curves Index The MacTutor History of Mathematics archive Arc Length Approximation by Chad Pierson Josh Fritz and Angela Sharp The Wolfram Demonstrations Project. Length of a Curve Experiment Illustrates numerical solution of finding length of a curve."}, {"topic": "Archimedes' principle", "content": "Archimedes principle indicates that the upward buoyant force that is exerted on a body immersed in a fluid whether fully or partially submerged is equal to the weight of the fluid that the body displaces and it acts in the upward direction at the centre of mass of the displaced fluid. Archimedes principle is a law of physics fundamental to fluid mechanics. It was formulated by Archimedes of Syracuse. "}, {"topic": "Area moment of inertia", "content": "This article is about the geometrical property of an area termed the second moment of area. For the moment of inertia dealing with the rotation of an object with mass see mass moment of inertia. For a list see list of second moments of area. The second moment of area also known as moment of inertia of plane area area moment of inertia or second area moment is a geometrical property of an area which reflects how its points are distributed with regard to an arbitrary axis. The second moment of area is typically denoted with either an I  .  Calculator for Second Moment of Area"}, {"topic": "Astronomical unit", "content": "The astronomical unit symbol au AU or ua is a unit of length roughly the distance from Earth to the Sun. However that distance varies as Earth orbits the Sun from a maximum aphelion to a minimum perihelion and back again once a year. Originally conceived as the average of Earths aphelion and perihelion it is now defined as exactly 7011149597870700000149597870700 metres about 150 million kilometres or 93 million miles. The astronomical unit is used primarily as a convenient yardstick for measuring distances within the Solar System or around other stars. However it is also a fundamental component in the definition of another unit of astronomical length the parsec.  The IAU and astronomical units Recommendations concerning Units HTML version of the IAU Style Manual Chasing Venus Observing the Transits of Venus Transit of Venus"}, {"topic": "Astronomy", "content": "Astronomy a natural science is the study of celestial objects such as stars galaxies planets moons asteroids comets and nebulae and processes such as supernovae explosions gamma ray bursts and cosmic microwave background radiation the physics chemistry and evolution of such objects and processes and more generally all phenomena that originate outside the atmosphere of Earth. A related but distinct subject physical cosmology is concerned with studying the Universe as a whole. Astronomy is one of the oldest sciences. The early civilizations in recorded history such as the Babylonians Greeks Indians Egyptians Nubians Iranians Chinese and Maya performed methodical observations of the night sky. Historically astronomy has included disciplines as diverse as astrometry celestial navigation observational astronomy and the making of calendars but professional astronomy is nowadays often considered to be synonymous with astrophysics. During the 20th century the field of professional astronomy split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects which is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. The two fields complement each other with theoretical astronomy seeking to explain the observational results and observations being used to confirm theoretical results. Astronomy is one of the few sciences where amateurs can still play an active role especially in the discovery and observation of transient phenomena. Amateur astronomers have made and contributed to many important astronomical discoveries.  International Year of Astronomy 2009 IYA2009 Main website Cosmic Journey A History of Scientific Cosmology from the American Institute of Physics Southern Hemisphere Astronomy Celestia Motherlode Educational site for Astronomical journeys through space Prof. Sir Harry Kroto NL Astrophysical Chemistry Lecture Series. 8 Freeview Lectures provided by the Vega Science Trust. Core books and core journals in Astronomy from the SmithsonianNASA Astrophysics Data System A Journey with Fred Hoyle Second Edition by Chandra Wickramasinghe. Early astronomy books from the History of Science Collection at the Linda Hall Library"}, {"topic": "Astroparticle physics", "content": "Astroparticle physics also called particle astrophysics is a branch of particle physics that studies elementary particles of astronomical origin and their relation to astrophysics and cosmology. It is a relatively new field of research emerging at the intersection of particle physics astronomy astrophysics detector physics relativity solid state physics and cosmology. Partly motivated by the discovery of neutrino oscillation the field has undergone rapid development both theoretically and experimentally since the early 2000s.  Aspera European network portal www.astroparticle.org all about astroparticle physics... Aspera news Astroparticle physics news on Twitter Virtual Institute of Astroparticle Physics Helmholtz Alliance for Astroparticle Physics UCLA AstroParticle Physics at UCLA Journal of Cosmology and Astroparticle Physics Astroparticle Physics in the Netherlands Astroparticle and High Energy Physics ASD Astroparticle Physics Laboratory at NASA"}, {"topic": "Astrophysics", "content": "Astrophysics is the branch of astronomy that employs the principles of physics and chemistry to ascertain the nature of the heavenly bodies rather than their positions or motions in space. Among the objects studied are the Sun other stars galaxies extrasolar planets the interstellar medium and the cosmic microwave background. Their emissions are examined across all parts of the electromagnetic spectrum and the properties examined include luminosity density temperature and chemical composition. Because astrophysics is a very broad subject astrophysicists typically apply many disciplines of physics including mechanics electromagnetism statistical mechanics thermodynamics quantum mechanics relativity nuclear and particle physics and atomic and molecular physics. In practice modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter dark energy and black holes whether or not time travel is possible wormholes can form or the multiverse exists and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists include Solar System formation and evolution stellar dynamics and evolution galaxy formation and evolution magnetohydrodynamics largescale structure of matter in the universe origin of cosmic rays general relativity and physical cosmology including string cosmology and astroparticle physics. Astrophysics can be studied at the bachelors masters and Ph.D. levels in physics or astronomy departments at many universities.  International Journal of Modern Physics D from World Scientific Cosmic Journey A History of Scientific Cosmology from the American Institute of Physics Prof. Sir Harry Kroto NL Astrophysical Chemistry Lecture Series. 8 Freeview Lectures provided by the Vega Science Trust. Stanford Linear Accelerator Center Stanford California Institute for Space Astrophysics and Cosmic Physics Astrophysical Journal Astronomy and Astrophysics a European Journal List and directory of peerreviewed Astronomy  Astrophysics Journals Master of Science in Astronomy and Astrophysics Ned Wrights Cosmology Tutorial UCLA UNLV Astronomy  Astrophysics department Hot and Active Stars Research astrophysicist Philippe Stees homepage Astronomy and Geophysics The Royal Astronomical Society house journal presenting scientific articles on major developing themes in astronomy and geophysics in succinct readable and accessible form"}, {"topic": "Atmospheric physics", "content": "Atmospheric physics is the application of physics to the study of the atmosphere. Atmospheric physicists attempt to model Earths atmosphere and the atmospheres of the other planets using fluid flow equations chemical models radiation budget and energy transfer processes in the atmosphere as well as how these tie into other systems such as the oceans. In order to model weather systems atmospheric physicists employ elements of scattering theory wave propagation models cloud physics statistical mechanics and spatial statistics which are highly mathematical and related to physics. It has close links to meteorology and climatology and also covers the design and construction of instruments for studying the atmosphere and the interpretation of the data they provide including remote sensing instruments. At the dawn of the space age and the introduction of sounding rockets aeronomy became a subdiscipline concerning the upper layers of the atmosphere where dissociation and ionization are important.  J. V. Iribarne H. R. Cho Atmospheric Physics D. Reidel Publishing Company 1980"}, {"topic": "Atom", "content": "An atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element. Every solid liquid gas and plasma is composed of neutral or ionized atoms. Atoms are very small typical sizes are around 100 pm a tenbillionth of a meter in the short scale. However atoms do not have welldefined boundaries and there are different ways to define their size that give different but close values. Atoms are small enough that attempting to predict their behavior using classical physics  as if they were billiard balls for example  gives noticeably incorrect predictions due to quantum effects. Through the development of physics atomic models have incorporated quantum principles to better explain and predict the behavior. Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and typically a similar number of neutrons. Protons and neutrons are called nucleons. More than 99.94 of an atoms mass is in the nucleus. The protons have a positive electric charge the electrons have a negative electric charge and the neutrons have no electric charge. If the number of protons and electrons are equal that atom is electrically neutral. If an atom has more or fewer electrons than protons then it has an overall negative or positive charge respectively and it is called an ion. The electrons of an atom are attracted to the protons in an atomic nucleus by this electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by a different force the nuclear force which is usually stronger than the electromagnetic force repelling the positively charged protons from one another. Under certain circumstances the repelling electromagnetic force becomes stronger than the nuclear force and nucleons can be ejected from the nucleus leaving behind a different element nuclear decay resulting in nuclear transmutation. The number of protons in the nucleus defines to what chemical element the atom belongs for example all copper atoms contain 29 protons. The number of neutrons defines the isotope of the element. The number of electrons influences the magnetic properties of an atom. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature and is the subject of the discipline of chemistry.  Quantum Mechanics and the Structure of Atoms on YouTube The actual physics lesson begins 220 into the video. Freudenrich Craig C. How Atoms Work. How Stuff Works. Archived from the original on 8 January 2007. Retrieved 9 January 2007. The Atom. Free High School Science Texts Physics. Wikibooks. Retrieved 10 July 2010. Anonymous 2007. The atom. Science aid. Retrieved 10 July 2010. a guide to the atom for teens. Anonymous 3 January 2006. Atoms and Atomic Structure. BBC. Archived from the original on 2 January 2007. Retrieved 11 January 2007. Various 3 January 2006. Physics 2000 Table of Contents. University of Colorado. Archived from the original on 14 January 2008. Retrieved 11 January 2008. Various 3 February 2006. What does an atom look like. University of Karlsruhe. Retrieved 12 May 2008."}, {"topic": "Atomic, molecular, and optical physics", "content": "Atomic molecular and optical physics AMO is the study of mattermatter and lightmatter interactions at the scale of one or a few atoms and energy scales around several electron volts. The three areas are closely interrelated. AMO theory includes classical semiclassical and quantum treatments. Typically the theory and applications of emission absorption scattering of electromagnetic radiation light from excited atoms and molecules analysis of spectroscopy generation of lasers and masers and the optical properties of matter in general fall into these categories.  MITHarvard Center for Ultracold Atoms Lorentz and Drude Models see and listen to Lecture 2 Nonlinear and Anisotropic Materials see and listen to Lecture 3 Joint Quantum Institute at University of Maryland and NIST JILA Atomic Physics ORNL Physics Division 1 Institute of physics 2 American Physical Society 3 National Science Foundation 4 ScienceDirect 5 Center for Theoretical Atomic Molecular and Optical Physics Queens University Belfast 6 European Physical Society 7 Department of Physics  University of California Berkeley"}, {"topic": "Atomic line filter", "content": "An atomic line filter ALF is an advanced optical bandpass filter used in the physical sciences for filtering electromagnetic radiation with precision accuracy and minimal signal strength loss. Atomic line filters work via the absorption or resonance lines of atomic vapors and so may also be designated an atomic resonance filter ARF. The three major types of atomic line filters are absorptionreemission ALFs Faraday filters and Voigt filters. Absorptionreemission filters were the first type developed and so are commonly called simply atomic line filters the other two types are usually referred to specifically as Faraday filters or Voigt filters. Atomic line filters use different mechanisms and designs for different applications but the same basic strategy is always employed by taking advantage of the narrow lines of absorption or resonance in a metallic vapor a specific frequency of light bypasses a series of filters that block all other light. Atomic line filters can be considered the optical equivalent of lockin amplifiers they are used in scientific applications requiring the effective detection of a narrowband signal almost always laser light that would otherwise be obscured by broadband sources such as daylight. They are used regularly in Laser Imaging Detection and Ranging LIDAR and are being studied for their potential use in laser communication systems. Atomic line filters are superior to conventional dielectric optical filters such as interference filters and Lyot filters but their greater complexity makes them practical only in backgroundlimited detection where a weak signal is detected while suppressing a strong background. Compared to etalons another highend optical filter Faraday filters are significantly sturdier and may be six times cheaper at around US15000 per unit.  H. Chen M. A. White D. A. Krueger and C. Y. She. Daytime mesopause temperature measurements with a sodiumvapor dispersive Faraday filter in a lidar receiver. Opt. Letters 211510931095 1996. H. Chen C. Y. She P. Searcy and E. Korevaar. Sodiumvapor dispersive Faraday filter. Optics Letters 1810191021 June 1993."}, {"topic": "Atomic mass", "content": "The atomic mass ma is the mass of an atomic particle subatomic particle or molecule. It is commonly expressed in unified atomic mass units u where by international agreement 1 unified atomic mass unit is defined as 112 of the mass of a single carbon12 atom at rest. For atoms the protons and neutrons of the nucleus account for almost all of the mass and the atomic mass measured in u has nearly the same value as the mass number. When divided by unified atomic mass units or daltons to form a pure number ratio the atomic mass of an atom becomes a dimensionless number called the relative isotopic mass see section below. Thus the atomic mass of a carbon12 atom is 12 u or 12 daltons Da but the relative isotopic mass of a carbon12 atom is simply 12. The atomic mass or relative isotopic mass refers to the mass of a single particle and is fundamentally different from the quantities elemental atomic weight also called relative atomic mass and standard atomic weight both of which refer to averages mathematical means of naturallyoccurring atomic mass values for samples of elements. Most elements have more than one stable nuclide for those elements such an average depends on the mix of nuclides present which may vary to some limited extent depending on the source of the sample as each nuclide has a different mass. However a typical value can be established which is called the standard atomic weight. By contrast atomic mass figures refer to an individual particle species as atoms of the same species are identical atomic mass values are expected to have no intrinsic variance at all. Atomic mass figures are thus commonly reported to many more significant figures than atomic weights. Standard atomic weight is related to atomic mass by the abundance ranking of isotopes for each element. It is usually about the same value as the atomic mass of the most abundant isotope other than what looks like but is not actually a rounding difference. The atomic mass of atoms ions or atomic nuclei is slightly less than the sum of the masses of their constituent protons neutrons and electrons due to binding energy mass loss as per Emc2.  NIST relative atomic masses of all isotopes and the standard atomic weights of the elements AME2003 Atomic Mass Evaluation from the National Nuclear Data Center"}, {"topic": "Atomic mass unit", "content": "The unified atomic mass unit symbol u or dalton symbol Da is the standard unit that is used for indicating mass on an atomic or molecular scale atomic mass. One unified atomic mass unit is approximately the mass of one nucleon either a single proton or neutron and is numerically equivalent to 1 gmol. It is defined as one twelfth of the mass of an unbound neutral atom of carbon12 in its nuclear and electronic ground state and has a value of 69731660539040000001.660539040201027 kg. The CIPM has categorised it as a nonSI unit accepted for use with the SI and whose value in SI units must be obtained experimentally. The amu without the unified prefix is technically an obsolete unit based on oxygen which was replaced in 1961. However many sources still use the term amu but now define it in the same way as u i.e. based on carbon12. In this sense most uses of the terms atomic mass units and amu today actually refer to unified atomic mass unit. For standardization a specific atomic nucleus carbon12 vs. oxygen16 had to be chosen because the average mass of a nucleon depends on the count of the nucleons in the atomic nucleus due to mass defect. This is also why the mass of a proton or neutron by itself is more than and not equal to 1 u. The atomic mass unit is not the unit of mass in the atomic units system which is rather the electron rest mass me.  atomic mass unit at sizes.com"}, {"topic": "Atomic number", "content": "In chemistry and physics the atomic number of a chemical element also known as its proton number is the number of protons found in the nucleus of an atom of that element and therefore identical to the charge number of the nucleus. It is conventionally represented by the symbol Z. The atomic number uniquely identifies a chemical element. In an uncharged atom the atomic number is also equal to the number of electrons. The atomic number Z should not be confused with the mass number A which is the number of nucleons the total number of protons and neutrons in the nucleus of an atom. The number of neutrons N is known as the neutron number of the atom thus A  Z  N these quantities are always whole numbers. Since protons and neutrons have approximately the same mass and the mass of the electrons is negligible for many purposes and the mass defect of nucleon binding is always small compared to the nucleon mass the atomic mass of any atom when expressed in unified atomic mass units making a quantity called the relative isotopic mass is roughly to within 1 equal to the whole number A. Atoms with the same atomic number Z but different neutron numbers N and hence different atomic masses are known as isotopes. A little more than threequarters of naturally occurring elements exist as a mixture of isotopes see monoisotopic elements and the average isotopic mass of an isotopic mixture for an element called the relative atomic mass in a defined environment on Earth determines the elements standard atomic weight. Historically it was these atomic weights of elements in comparison to hydrogen that were the quantities measurable by chemists in the 19th century. The conventional symbol Z comes from the German word Zahl meaning numbernumeralfigure which prior to the modern synthesis of ideas from chemistry and physics merely denoted an elements numerical place in the periodic table whose order is approximately but not completely consistent with the order of the elements by atomic weights. Only after 1915 with the suggestion and evidence that this Z number was also the nuclear charge and a physical characteristic of atoms did the word Atomzahl and its English equivalent atomic number come into common use in this context. "}, {"topic": "Atomic orbital", "content": "In quantum mechanics an atomic orbital is a mathematical function that describes the wavelike behavior of either one electron or a pair of electrons in an atom. This function can be used to calculate the probability of finding any electron of an atom in any specific region around the atoms nucleus. The term atomic orbital may also refer to the physical region or space where the electron can be calculated to be present as defined by the particular mathematical form of the orbital. Each orbital in an atom is characterized by a unique set of values of the three quantum numbers n  and m which respectively correspond to the electrons energy angular momentum and an angular momentum vector component the magnetic quantum number. Each such orbital can be occupied by a maximum of two electrons each with its own spin quantum number s. The simple names s orbital p orbital d orbital and f orbital refer to orbitals with angular momentum quantum number  0 1 2 and 3 respectively. These names together with the value of n are used to describe the electron configurations of atoms. They are derived from the description by early spectroscopists of certain series of alkali metal spectroscopic lines as sharp principal diffuse and fundamental. Orbitals for  3 continue alphabetically omitting j g h i k . Atomic orbitals are the basic building blocks of the atomic orbital model alternatively known as the electron cloud or wave mechanics model a modern framework for visualizing the submicroscopic behavior of electrons in matter. In this model the electron cloud of a multielectron atom may be seen as being built up in approximation in an electron configuration that is a product of simpler hydrogenlike atomic orbitals. The repeating periodicity of the blocks of 2 6 10 and 14 elements within sections of the periodic table arises naturally from the total number of electrons that occupy a complete set of s p d and f atomic orbitals respectively although for higher values of the quantum number n particularly when the atom in question bears a positive charge the energies of certain subshells become very similar and so the order in which they are said to be populated by electrons e.g. Cr  Ar4s13d5 and Cr2  Ar3d4 can only be rationalized somewhat arbitrarily.  Guide to atomic orbitals Covalent Bonds and Molecular Structure Animation of the time evolution of an hydrogenic orbital The Orbitron a visualization of all common and uncommon atomic orbitals from 1s to 7g Grand table Still images of many orbitals David Mantheys Orbital Viewer renders orbitals with n 30 Java orbital viewer applet What does an atom look like Orbitals in 3D Atom Orbitals v.1.5 visualization software CrystalStyle Orbital Viewer"}, {"topic": "Atomic packing factor", "content": "In crystallography atomic packing factor APF packing efficiency or packing fraction is the fraction of volume in a crystal structure that is occupied by constituent particles. It is dimensionless and always less than unity. In atomic systems by convention the APF is determined by assuming that atoms are rigid spheres. The radius of the spheres is taken to be the maximal value such that the atoms do not overlap. For onecomponent crystals those that contain only one type of particle the packing fraction is represented mathematically by A P F  N p a r t i c l e V p a r t i c l e V u n i t c e l l   Schaffer Saxena Antolovich Sanders and Warner 1999. The Science and Design of Engineering Materials Second ed.. New York WCBMcGrawHill. pp. 8188. Callister W. 2002. Materials Science and Engineering Sixth ed.. San Francisco John Wiley and Sons. pp. 105114."}, {"topic": "Atomic physics", "content": "Atomic physics is the field of physics that studies atoms as an isolated system of electrons and an atomic nucleus. It is primarily concerned with the arrangement of electrons around the nucleus and the processes by which these arrangements change. This includes ions as well as neutral atoms and unless otherwise stated for the purposes of this discussion it should be assumed that the term atom includes ions. The term atomic physics is often associated with nuclear power and nuclear weapons due to the synonymous use of atomic and nuclear in standard English. However physicists distinguish between atomic physics which deals with the atom as a system consisting of a nucleus and electrons and nuclear physics which considers atomic nuclei alone. As with many scientific fields strict delineation can be highly contrived and atomic physics is often considered in the wider context of atomic molecular and optical physics. Physics research groups are usually so classified.  MITHarvard Center for Ultracold Atoms Joint Quantum Institute at University of Maryland and NIST Atomic Physics on the Internet JILA Atomic Physics ORNL Physics Division"}, {"topic": "Atomic structure", "content": "An atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element. Every solid liquid gas and plasma is composed of neutral or ionized atoms. Atoms are very small typical sizes are around 100 pm a tenbillionth of a meter in the short scale. However atoms do not have welldefined boundaries and there are different ways to define their size that give different but close values. Atoms are small enough that attempting to predict their behavior using classical physics  as if they were billiard balls for example  gives noticeably incorrect predictions due to quantum effects. Through the development of physics atomic models have incorporated quantum principles to better explain and predict the behavior. Every atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and typically a similar number of neutrons. Protons and neutrons are called nucleons. More than 99.94 of an atoms mass is in the nucleus. The protons have a positive electric charge the electrons have a negative electric charge and the neutrons have no electric charge. If the number of protons and electrons are equal that atom is electrically neutral. If an atom has more or fewer electrons than protons then it has an overall negative or positive charge respectively and it is called an ion. The electrons of an atom are attracted to the protons in an atomic nucleus by this electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by a different force the nuclear force which is usually stronger than the electromagnetic force repelling the positively charged protons from one another. Under certain circumstances the repelling electromagnetic force becomes stronger than the nuclear force and nucleons can be ejected from the nucleus leaving behind a different element nuclear decay resulting in nuclear transmutation. The number of protons in the nucleus defines to what chemical element the atom belongs for example all copper atoms contain 29 protons. The number of neutrons defines the isotope of the element. The number of electrons influences the magnetic properties of an atom. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature and is the subject of the discipline of chemistry.  Quantum Mechanics and the Structure of Atoms on YouTube The actual physics lesson begins 220 into the video. Freudenrich Craig C. How Atoms Work. How Stuff Works. Archived from the original on 8 January 2007. Retrieved 9 January 2007. The Atom. Free High School Science Texts Physics. Wikibooks. Retrieved 10 July 2010. Anonymous 2007. The atom. Science aid. Retrieved 10 July 2010. a guide to the atom for teens. Anonymous 3 January 2006. Atoms and Atomic Structure. BBC. Archived from the original on 2 January 2007. Retrieved 11 January 2007. Various 3 January 2006. Physics 2000 Table of Contents. University of Colorado. Archived from the original on 14 January 2008. Retrieved 11 January 2008. Various 3 February 2006. What does an atom look like. University of Karlsruhe. Retrieved 12 May 2008."}, {"topic": "Atomic weight", "content": "Relative atomic mass symbol Ar is a dimensionless physical quantity the ratio of the average mass of atoms of an element from a single given sample or source to 112 of the mass of an atom of carbon12 known as the unified atomic mass unit. The relative atomic mass is a statistical term referring to an abundanceweighted figure involving measurement of many atoms. As in all related terms the word relative refers to making the figure relative to carbon12 so that the final figure is dimensionless. The term relative atomic mass is exactly equivalent to atomic weight which is the older term. In technical usage these values are samplespecific i.e. element sourcespecific when a natural element source is composed of more than one isotope. Thus two samples of a chemical element which is naturally found as being composed of more than one isotope collected from two substantially different sources are expected to give slightly different relative atomic masses atomic weights because isotopic concentrations typically vary slightly due to the history origin of the source. These values differences are real and repeatable and can be used to identify specific samples. For example a sample of elemental carbon from volcanic methane will have a different relative atomic mass atomic weight than one collected from plant or animal tissues for more see isotope geochemistry. In short the atomic weight relative atomic mass of carbon varies slightly from place to place and from source to source a fact that can be useful. However a typical standard figure also can be useful as follows. Both the terms relative atomic mass and atomic weight are sometimes loosely used to refer to a technically different standardized expectation value called the standard atomic weight. This value is the mean value of atomic weights of a number of normal samples of the element in question. For this definition a normal sample is any reasonably possible source of the element or its compounds in commerce for industry and science and has not been subject to significant modification of isotopic composition within a geologically brief period. These standard atomic weights are published at regular intervals by the Commission on Isotopic Abundances and Atomic Weights of the International Union of Pure and Applied Chemistry IUPAC The standard values are intended as mean values that compensate for small variances in the isotopic composition of the chemical elements across a range of ordinary samples on Earth and thus to be applicable to normal laboratory materials. However they may not accurately reflect values from samples from unusual locations or extraterrestrial objects which often have more widely variant isotopic compositions. The standard atomic weights are reprinted in a wide variety of textbooks commercial catalogues Periodic Table wall charts etc. and in the table below. They are what chemists loosely call atomic weights. The continued use of the term atomic weight of any element as opposed to relative atomic mass has attracted considerable controversy since at least the 1960s mainly due to the technical difference between weight and mass in physics. see below. Both terms are officially sanctioned by IUPAC. The term relative atomic mass now seems to be gaining as the preferred term over atomic weight although in the case of standard atomic weight this shorter term as opposed to standard relative atomic mass continues to be preferred.  IUPAC Commission on Isotopic Abundances and Atomic Weights NIST relative atomic masses of all isotopes and the standard atomic weights of the elements Atomic Weights of the Elements 2011"}, {"topic": "Attenuation coefficient", "content": "For attenuation coefficient as it applies to electromagnetic theory and telecommunications see propagation constant. For the mass attenuation coefficient see the article mass attenuation coefficient. Attenuation coefficient or narrow beam attenuation coefficient of the volume of a material characterizes how easily it can be penetrated by a beam of light sound particles or other energy or matter. A large attenuation coefficient means that the beam is quickly attenuated weakened as it passes through the medium and a small attenuation coefficient means that the medium is relatively transparent to the beam. The SI unit of attenuation coefficient is the reciprocal metre m1. Extinction coefficient is an old term for this quantity but still used in meteorology and climatology  Absorption Coefficients of Building Materials and Finishes Sound Absorption Coefficients for Some Common Materials Computations for Radio Wave Propagation Tables of XRay Mass Attenuation Coefficients and Mass EnergyAbsorption Coefficients from 1 keV to 20 MeV for Elements Z  1 to 92 and 48 Additional Substances of Dosimetric Interest"}, {"topic": "Audio frequency", "content": "An audio frequency abbreviation AF or audible frequency is characterized as a periodic vibration whose frequency is audible to the average human. The SI unit of audio frequency is the hertz Hz. It is the property of sound that most determines pitch. The generally accepted standard range of audible frequencies is 20 to 20000 Hz although the range of frequencies individuals hear is greatly influenced by environmental factors. Frequencies below 20 Hz are generally felt rather than heard assuming the amplitude of the vibration is great enough. Frequencies above 20000 Hz can sometimes be sensed by young people. High frequencies are the first to be affected by hearing loss due to age andor prolonged exposure to very loud noises. "}, {"topic": "Avogadro's Law", "content": "Avogadros law sometimes referred to as Avogadros hypothesis or Avogadros principle is an experimental gas law relating volume of a gas to the amount of substance of gas present. A modern statement of Avogadros law is Avogadros law states that equal volumes of all gases at the same temperature and pressure have the same number of molecules. For a given mass of an ideal gas the volume and amount moles of the gas are directly proportional if the temperature and pressure are constant. which can be written as V n  For 100.00 kPa and 273.15 K the molar volume of an ideal gas is 22.712 dm3mol1. Note that the universal gas constant R is given by the product of Avogadros number and Boltzmanns constant. See Gas Constant.  Avogadros law at the University of Fribourg Avogadros law at the Royal Society of Chemistry"}, {"topic": "Avogadro's Number", "content": "In chemistry and physics the Avogadro constant named after the scientist Amedeo Avogadro is the number of constituent particles usually atoms or molecules that are contained in the amount of substance given by one mole. Thus it is the proportionality factor that relates the molar mass of a compound to the mass of a sample. Avogadros constant often designated with the symbol NA or L has the value 70236022140857000006.022140857741023 mol1 in the International System of Units SI. Previous definitions of chemical quantity involved Avogadros number a historical term closely related to the Avogadro constant but defined differently Avogadros number was initially defined by Jean Baptiste Perrin as the number of atoms in one grammolecule of atomic hydrogen meaning one gram of hydrogen. This number is also known as Loschmidt constant in German literature. The constant was later redefined as the number of atoms in 12 grams of the isotope carbon12 12C and still later generalized to relate amounts of a substance to their molecular weight. For instance to a first approximation 1 gram of hydrogen element H having the atomic mass number 1 has 70236022000000000006.0221023 hydrogen atoms. Similarly 12 grams of 12C with the mass number 12 atomic number 6 has the same number of carbon atoms 70236022000000000006.0221023. Avogadros number is a dimensionless quantity and has the same numerical value of the Avogadro constant given in base units. In contrast the Avogadro constant has the dimension of reciprocal amount of substance. Revisions in the base set of SI units necessitated redefinitions of the concepts of chemical quantity. Avogadros number and its definition was deprecated in favor of the Avogadro constant and its definition. Changes in the SI units are proposed to fix the value of the constant to exactly 6.02214X1023 when it is expressed in the unit mol1 in which an X at the end of a number means one or more final digits yet to be agreed upon.  1996 definition of the Avogadro constant from the IUPAC Compendium of Chemical Terminology Gold Book Some Notes on Avogadros Number 70236022000000000006.0221023 historical notes An Exact Value for Avogadros Number American Scientist Avogadro and molar Planck constants for the redefinition of the kilogram Poliakoff Martyn. Avogadros Number NA  70236022140000000006.022141023. Numberphile. Brady Haran. Murrell J. 2001 Avogadro and his Constant Helvitica Chemica Acta 84 6 p.13141327 Scanned version of Two hypothesis of Avogadro 1811 Avogadros article on BibNum"}, {"topic": "Avogadro constant", "content": "In chemistry and physics the Avogadro constant named after the scientist Amedeo Avogadro is the number of constituent particles usually atoms or molecules that are contained in the amount of substance given by one mole. Thus it is the proportionality factor that relates the molar mass of a compound to the mass of a sample. Avogadros constant often designated with the symbol NA or L has the value 70236022140857000006.022140857741023 mol1 in the International System of Units SI. Previous definitions of chemical quantity involved Avogadros number a historical term closely related to the Avogadro constant but defined differently Avogadros number was initially defined by Jean Baptiste Perrin as the number of atoms in one grammolecule of atomic hydrogen meaning one gram of hydrogen. This number is also known as Loschmidt constant in German literature. The constant was later redefined as the number of atoms in 12 grams of the isotope carbon12 12C and still later generalized to relate amounts of a substance to their molecular weight. For instance to a first approximation 1 gram of hydrogen element H having the atomic mass number 1 has 70236022000000000006.0221023 hydrogen atoms. Similarly 12 grams of 12C with the mass number 12 atomic number 6 has the same number of carbon atoms 70236022000000000006.0221023. Avogadros number is a dimensionless quantity and has the same numerical value of the Avogadro constant given in base units. In contrast the Avogadro constant has the dimension of reciprocal amount of substance. Revisions in the base set of SI units necessitated redefinitions of the concepts of chemical quantity. Avogadros number and its definition was deprecated in favor of the Avogadro constant and its definition. Changes in the SI units are proposed to fix the value of the constant to exactly 6.02214X1023 when it is expressed in the unit mol1 in which an X at the end of a number means one or more final digits yet to be agreed upon.  1996 definition of the Avogadro constant from the IUPAC Compendium of Chemical Terminology Gold Book Some Notes on Avogadros Number 70236022000000000006.0221023 historical notes An Exact Value for Avogadros Number American Scientist Avogadro and molar Planck constants for the redefinition of the kilogram Poliakoff Martyn. Avogadros Number NA  70236022140000000006.022141023. Numberphile. Brady Haran. Murrell J. 2001 Avogadro and his Constant Helvitica Chemica Acta 84 6 p.13141327 Scanned version of Two hypothesis of Avogadro 1811 Avogadros article on BibNum"}, {"topic": "Axion", "content": "The axion is a hypothetical elementary particle postulated by the PecceiQuinn theory in 1977 to resolve the strong CP problem in quantum chromodynamics QCD. If axions exist and have low mass within a specific range they are of interest as a possible component of cold dark matter.  November 24 2008 article in APS Physics January 28 2007 news article by newscientist.com December 06 2006 news article by physorg.com July 17 2006 news article from Scientific American March 27 2006 news article by PhysicsWeb.org November 24 2004 news article by PhysicsWeb.org CAST Experiment CAST at UNIZAR CAST at University of Technology Darmstadt ADMX at University of Washington"}, {"topic": "Azimuthal quantum number", "content": "The azimuthal quantum number is a quantum number for an atomic orbital that determines its orbital angular momentum and describes the shape of the orbital. The azimuthal quantum number is the second of a set of quantum numbers which describe the unique quantum state of an electron the others being the principal quantum number following spectroscopic notation the magnetic quantum number and the spin quantum number. It is also known as the orbital angular momentum quantum number orbital quantum number or second quantum number and is symbolized as .  Development of the Bohr atom Pictures of atomic orbitals Detailed explanation of the Orbital Quantum Number l The azimuthal equation explained"}, {"topic": "Babinet's principle", "content": "In physics Babinets principle is a theorem concerning diffraction that states that the diffraction pattern from an opaque body is identical to that from a hole of the same size and shape except for the overall forward beam intensity. "}, {"topic": "Background radiation", "content": "Background radiation is the ubiquitous ionizing radiation present in the environment. Background radiation originates from a variety of sources both natural and artificial. Sources include cosmic radiation naturally occurring radioactive materials such as radon and fallout from nuclear weapons testing and nuclear accidents.  Background radiation description from the Radiation Effects Research Foundation Environmental and Background Radiation FAQ from the Health Physics Society Radiation Dose Chart from the American Nuclear Society Radiation Dose Calculator from the US Environmental Protection Agency"}, {"topic": "Ballistics", "content": "Ballistics is the science of mechanics that deals with the launching flight behavior and effects of projectiles especially bullets gravity bombs rockets or the like the science or art of designing and accelerating projectiles so as to achieve a desired performance. A ballistic body is a body with momentum which is free to move subject to forces such as the pressure of gases in a gun or a propulsive nozzle by rifling in a barrel by gravity or by air drag. A ballistic missile is a missile only guided during the relatively brief initial powered phase of flight whose trajectory is subsequently governed by the laws of classical mechanics in contrast for example to a cruise missile which is aerodynamically guided in powered flight.  Association of Firearm and Tool Mark Examiners Ballistic Trajectories by Jeff Bryant The Wolfram Demonstrations Project Forensic Firearms and Tool Marks Time Line International Ballistics Society The Bullets Flight from Powder to Target Franklin Weston Mann"}, {"topic": "Balmer series", "content": "The Balmer series or Balmer lines in atomic physics is the designation of one of a set of six named series describing the spectral line emissions of the hydrogen atom. The Balmer series is calculated using the Balmer formula an empirical equation discovered by Johann Balmer in 1885. The visible spectrum of light from hydrogen displays four wavelengths 410 nm 434 nm 486 nm and 656 nm that correspond to emissions of photons by electrons in excited states transitioning to the quantum level described by the principal quantum number n equals 2. There are also a number of ultraviolet Balmer lines with wavelengths shorter than 400 nm. "}, {"topic": "Barometer", "content": "A barometer is a scientific instrument used in meteorology to measure atmospheric pressure. Pressure tendency can forecast short term changes in the weather. Numerous measurements of air pressure are used within surface weather analysis to help find surface troughs high pressure systems and frontal boundaries. Barometers and pressure altimeters the most basic and common type of altimeter are essentially the same instrument but used for different purposes. An altimeter is intended to be transported from place to place matching the atmospheric pressure to the corresponding altitude while a barometer is kept stationary and measures subtle pressure changes caused by weather. The main exception to this is ships at sea which can use a barometer because their elevation does not change. Due to the presence of weather systems aircraft altimeters may need to be adjusted as they fly between regions of varying normalized atmospheric pressure.  Media related to Barometer at Wikimedia Commons Barometer. Encyclopdia Britannica 3 11th ed.. 1911. Burch David F. The Barometer Handbook a modern look at barometers and applications of barometric pressure. Seattle Starpath Publications 2009 ISBN 9780914025122. Middleton W.E. Knowles. 1964. The history of the barometer. Baltimore Johns Hopkins Press. New edition 2002 ISBN 0801871549."}, {"topic": "Battery (electricity)", "content": "An electric battery is a device consisting of one or more electrochemical cells with external connections provided to power electrical devices. When a battery is supplying power its positive terminal is the cathode and its negative terminal is the anode. The terminal marked negative is the source of electrons that when connected to an external circuit will flow and deliver energy to an external device. When a battery is connected to an external circuit electrolytes are able to move as ions within allowing the chemical reactions to be completed at the separate terminals and so deliver energy to the external circuit. It is the movement of those ions within the battery which allows current to flow out of the battery to perform work. Historically the term battery specifically referred to a device composed of multiple cells however the usage has evolved to additionally include devices composed of a single cell. Primary singleuse or disposable batteries are used once and discarded the electrode materials are irreversibly changed during discharge. Common examples are the alkaline battery used for flashlights and a multitude of portable devices. Secondary rechargeable batteries can be discharged and recharged multiple times the original composition of the electrodes can be restored by reverse current. Examples include the leadacid batteries used in vehicles and lithiumion batteries used for portable electronics. Batteries come in many shapes and sizes from miniature cells used to power hearing aids and wristwatches to battery banks the size of rooms that provide standby power for telephone exchanges and computer data centers. According to a 2005 estimate the worldwide battery industry generates US48 billion in sales each year with 6 annual growth. Batteries have much lower specific energy energy per unit mass than common fuels such as gasoline. This is somewhat offset by the higher efficiency of electric motors in producing mechanical work compared to combustion engines.  Batteries at DMOZ Nonrechargeable batteries HowStuffWorks How batteries work DoITPoMS Teaching and Learning Package Batteries The Physics arXiv Blog 17 August 2013. First Atomic Level Simulation of a Whole Battery  MIT Technology Review. Technologyreview.com. Retrieved 21 August 2013."}, {"topic": "Beam (structure)", "content": "A beam is a structural element that is capable of withstanding load primarily by resisting against bending. The bending force induced into the material of the beam as a result of the external loads own weight span and external reactions to these loads is called a bending moment. Beams are characterized by their profile shape of crosssection their length and their material. Beams are traditionally descriptions of building or civil engineering structural elements but smaller structures such as truck or automobile frames machine frames and other mechanical or structural systems contain beam structures that are designed and analyzed in a similar fashion.  1 Common Beam Support Types 2 ISMB Beam Sizes and weights David Childs Ltd Consulting Civil Engineers Tutorials Steel Beam Design Prestressed Concrete Beam Design Tutorial American Wood Council Free Download Library Wood Construction Data Wood Structural Design Data pdf file online Span Calculator Introduction to Structural Design U. Virginia Dept. Architecture Online Beam Calculator Free Version SkyCivs Online Beam Design Software Glossary Course Sampler Lectures Projects Tests Beams and Bending review points follow using next buttons Structural Behavior and Design Approaches lectures follow using next buttons U. Maryland J.A. Clark School of Engineering HAMLET engineering simulations and models BeamAnalysis U. WisconsinStout Strength of Materials online lectures problems testssolutions links software Beams I  Shear Forces and Bending Moments Beams II  Bending Stress Free Online Calculator for Beam Bending Moment  Shear Force Beam calculations in MS Excel from ExcelCalcs.com Beam Calculation Software for Windows from beams.com Timber Frame Engineering Counsel library design information for wood joinery M.A.D. Propz Free downloadable desktop application for calculating section properties and stressstrain analysis of beam crosssections Medeek Beam Calculator Online Beam Calculator and Engineering Wood"}, {"topic": "Bending", "content": "In applied mechanics bending also known as flexure characterizes the behavior of a slender structural element subjected to an external load applied perpendicularly to a longitudinal axis of the element. The structural element is assumed to be such that at least one of its dimensions is a small fraction typically 110 or less of the other two. When the length is considerably longer than the width and the thickness the element is called a beam. For example a closet rod sagging under the weight of clothes on clothes hangers is an example of a beam experiencing bending. On the other hand a shell is a structure of any geometric form where the length and the width are of the same order of magnitude but the thickness of the structure known as the wall is considerably smaller. A large diameter but thinwalled short tube supported at its ends and loaded laterally is an example of a shell experiencing bending. In the absence of a qualifier the term bending is ambiguous because bending can occur locally in all objects. Therefore to make the usage of the term more precise engineers refer to a specific object such as the bending of rods the bending of beams the bending of plates the bending of shells and so on.  Flexure formulae Beam flexure stress formulae and calculators"}, {"topic": "Bending moment", "content": "A bending moment is the reaction induced in a structural element when an external force or moment is applied to the element causing the element to bend. The most common or simplest structural element subjected to bending moments is the beam. The example shows a beam which is simply supported at both ends. Simply supported means that each end of the beam can rotate therefore each end support has no bending moment. The ends can only react to the shear load. Other beams can have both ends fixed therefore each end support has both bending moment and shear reaction loads. Beams can also have one end fixed and one end simply supported. The simplest type of beam is the cantilever which is fixed at one end and is free at the other end neither simple or fixed. In reality beam supports are usually neither absolutely fixed nor absolutely rotating freely. The internal reaction loads in a crosssection of the structural element can be resolved into a resultant force and a resultant couple. For equilibrium the moment created by external forces and external moments must be balanced by the couple induced by the internal loads. The resultant internal couple is called the bending moment while the resultant internal force is called the shear force if it is transverse to the plane of element or the normal force if it is along the plane of the element. The bending moment at a section through a structural element may be defined as the sum of the moments about that section of all external forces acting to one side of that section. The forces and moments on either side of the section must be equal in order to counteract each other and maintain a state of equilibrium so the same bending moment will result from summing the moments regardless of which side of the section is selected. If clockwise bending moments are taken as negative then a negative bending moment within an element will cause sagging and a positive moment will cause hogging. It is therefore clear that a point of zero bending moment within a beam is a point of contraflexurethat is the point of transition from hogging to sagging or vice versa. Moments and torques are measured as a force multiplied by a distance so they have as unit newtonmetres Nm or poundfoot or footpound ftlb. The concept of bending moment is very important in engineering particularly in civil and mechanical engineering and physics.  Stress resultants for beams Bending Moment Calculator online"}, {"topic": "Bernoulli's equation", "content": "In fluid dynamics Bernoullis principle states that an increase in the speed of a fluid occurs simultaneously with a decrease in pressure or a decrease in the fluids potential energy. The principle is named after Daniel Bernoulli who published it in his book Hydrodynamica in 1738. Bernoullis principle can be applied to various types of fluid flow resulting in various forms of Bernoullis equation there are different forms of Bernoullis equation for different types of flow. The simple form of Bernoullis equation is valid for incompressible flows e.g. most liquid flows and gases moving at low Mach number. More advanced forms may be applied to compressible flows at higher Mach numbers see the derivations of the Bernoulli equation. Bernoullis principle can be derived from the principle of conservation of energy. This states that in a steady flow the sum of all forms of energy in a fluid along a streamline is the same at all points on that streamline. This requires that the sum of kinetic energy potential energy and internal energy remains constant. Thus an increase in the speed of the fluid implying an increase in both its dynamic pressure and kinetic energy occurs with a simultaneous decrease in the sum of its static pressure potential energy and internal energy. If the fluid is flowing out of a reservoir the sum of all forms of energy is the same on all streamlines because in a reservoir the energy per unit volume the sum of pressure and gravitational potential g h is the same everywhere. Bernoullis principle can also be derived directly from Newtons 2nd law. If a small volume of fluid is flowing horizontally from a region of high pressure to a region of low pressure then there is more pressure behind than in front. This gives a net force on the volume accelerating it along the streamline. Fluid particles are subject only to pressure and their own weight. If a fluid is flowing horizontally and along a section of a streamline where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure and if its speed decreases it can only be because it has moved from a region of lower pressure to a region of higher pressure. Consequently within a fluid flowing horizontally the highest speed occurs where the pressure is lowest and the lowest speed occurs where the pressure is highest.  Bernoulli equation calculator Denver University Bernoullis equation and pressure measurement Millersville University Applications of Eulers equation NASA Beginners guide to aerodynamics Misinterpretations of Bernoullis equation Weltner and IngelmanSundberg"}, {"topic": "Bernoulli's principle", "content": "In fluid dynamics Bernoullis principle states that an increase in the speed of a fluid occurs simultaneously with a decrease in pressure or a decrease in the fluids potential energy. The principle is named after Daniel Bernoulli who published it in his book Hydrodynamica in 1738. Bernoullis principle can be applied to various types of fluid flow resulting in various forms of Bernoullis equation there are different forms of Bernoullis equation for different types of flow. The simple form of Bernoullis equation is valid for incompressible flows e.g. most liquid flows and gases moving at low Mach number. More advanced forms may be applied to compressible flows at higher Mach numbers see the derivations of the Bernoulli equation. Bernoullis principle can be derived from the principle of conservation of energy. This states that in a steady flow the sum of all forms of energy in a fluid along a streamline is the same at all points on that streamline. This requires that the sum of kinetic energy potential energy and internal energy remains constant. Thus an increase in the speed of the fluid implying an increase in both its dynamic pressure and kinetic energy occurs with a simultaneous decrease in the sum of its static pressure potential energy and internal energy. If the fluid is flowing out of a reservoir the sum of all forms of energy is the same on all streamlines because in a reservoir the energy per unit volume the sum of pressure and gravitational potential g h is the same everywhere. Bernoullis principle can also be derived directly from Newtons 2nd law. If a small volume of fluid is flowing horizontally from a region of high pressure to a region of low pressure then there is more pressure behind than in front. This gives a net force on the volume accelerating it along the streamline. Fluid particles are subject only to pressure and their own weight. If a fluid is flowing horizontally and along a section of a streamline where the speed increases it can only be because the fluid on that section has moved from a region of higher pressure to a region of lower pressure and if its speed decreases it can only be because it has moved from a region of lower pressure to a region of higher pressure. Consequently within a fluid flowing horizontally the highest speed occurs where the pressure is lowest and the lowest speed occurs where the pressure is highest.  Bernoulli equation calculator Denver University Bernoullis equation and pressure measurement Millersville University Applications of Eulers equation NASA Beginners guide to aerodynamics Misinterpretations of Bernoullis equation Weltner and IngelmanSundberg"}, {"topic": "Bessel function", "content": "Bessel functions first defined by the mathematician Daniel Bernoulli and then generalized by Friedrich Bessel are the canonical solutions yx of the differential equation x 2 d 2 y d x 2  x d y d x   x 2 2  y  0   Lizorkin P.I. 2001 Bessel functions in Hazewinkel Michiel Encyclopedia of Mathematics Springer ISBN 9781556080104 Karmazina L.N. Prudnikov A.P. 2001 Cylinder function in Hazewinkel Michiel Encyclopedia of Mathematics Springer ISBN 9781556080104 Rozov N.Kh. 2001 Bessel equation in Hazewinkel Michiel Encyclopedia of Mathematics Springer ISBN 9781556080104 Wolfram function pages on Bessel J and Y functions and modified Bessel I and K functions. Pages include formulas function evaluators and plotting calculators. Wolfram Mathworld Bessel functions of the first kind Bessel functions J Y I and K in Librow Function handbook. F. W. J. Olver L. C. Maximon Bessel Functions chapter 10 of the Digital Library of Mathematical Functions."}, {"topic": "Beta particle", "content": "A beta particle sometimes called beta ray denoted by the lowercase Greek letter beta  is a highenergy highspeed electron or positron emitted in the radioactive decay of an atomic nucleus such as a potassium40 nucleus in the process of beta decay. Two forms of beta decay and  respectively produce electrons and positrons. Beta particles are a type of ionizing radiation.  Radioactivity and alpha beta gamma and Xrays Rays and Particles University of Virginia Lecture Notes History of Radiation at Idaho State University Betavoltic Battery Scientists Invent 30 Year Continuous Power Laptop Battery at NextEnergyNews.com Radioactive laptops Perhaps not... at the Wayback Machine Basic Nuclear Science Information at the Lawrence Berkeley National Laboratory"}, {"topic": "Big Bang", "content": "The Big Bang theory is the prevailing cosmological model for the universe from the earliest known periods through its subsequent largescale evolution. The model accounts for the fact that the universe expanded from a very high density and high temperature state and offers a comprehensive explanation for a broad range of phenomena including the abundance of light elements the cosmic microwave background large scale structure and Hubbles Law. If the known laws of physics are extrapolated beyond where they have been verified there is a singularity. Some estimates place this moment at approximately 13.8 billion years ago which is thus considered the age of the universe. After the initial expansion the universe cooled sufficiently to allow the formation of subatomic particles and later simple atoms. Giant clouds of these primordial elements later coalesced through gravity to form stars and galaxies. Since Georges Lematre first noted in 1927 that an expanding universe might be traced back in time to an originating single point scientists have built on his idea of cosmic expansion. While the scientific community was once divided between supporters of two different expanding universe theories the Big Bang and the Steady State theory accumulated empirical evidence provides strong support for the former. In 1929 from analysis of galactic redshifts Edwin Hubble concluded that galaxies are drifting apart this is important observational evidence consistent with the hypothesis of an expanding universe. In 1965 the cosmic microwave background radiation was discovered which was crucial evidence in favor of the Big Bang model since that theory predicted the existence of background radiation throughout the universe before it was discovered. More recently measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating an observation attributed to dark energys existence. The known physical laws of nature can be used to calculate the characteristics of the universe in detail back in time to an initial state of extreme density and temperature.  bigbang model at Encyclopdia Britannica The Story of the Big Bang  STFC funded project explaining the history of the universe in easytounderstand language Big Bang Cosmology WMAP The Big Bang  NASA Science Big bang model with animated graphics Cosmology at DMOZ Evidence for the Big Bang"}, {"topic": "Binary star", "content": "A binary star is a star system consisting of two stars orbiting around their common barycenter. Systems of two three four or even more stars are called multiple star systems. These systems especially when more distant often appear to the unaided eye as a single point of light and are then revealed as double or more by other means. Research over the last two centuries suggests that half or more of visible stars are part of multiple star systems. The term double star is often used synonymously with binary star however double star can also mean optical double star. Optical doubles are so called because the two stars appear close together in the sky as seen from the Earth they are almost on the same line of sight. Nevertheless their doubleness depends only on this optical effect the stars themselves are distant from one another and share no physical connection. A double star can be revealed as optical by means of differences in their parallax measurements proper motions or radial velocities. Most known double stars have not been studied sufficiently closely to determine whether they are optical doubles or they are doubles physically bound through gravitation into a multiple star system. Binary star systems are very important in astrophysics because calculations of their orbits allow the masses of their component stars to be directly determined which in turn allows other stellar parameters such as radius and density to be indirectly estimated. This also determines an empirical massluminosity relationship MLR from which the masses of single stars can be estimated. Binary stars are often detected optically in which case they are called visual binaries. Many visual binaries have long orbital periods of several centuries or millennia and therefore have orbits which are uncertain or poorly known. They may also be detected by indirect techniques such as spectroscopy spectroscopic binaries or astrometry astrometric binaries. If a binary star happens to orbit in a plane along our line of sight its components will eclipse and transit each other these pairs are called eclipsing binaries or as they are detected by their changes in brightness during eclipses and transits photometric binaries. If components in binary star systems are close enough they can gravitationally distort their mutual outer stellar atmospheres. In some cases these close binary systems can exchange mass which may bring their evolution to stages that single stars cannot attain. Examples of binaries are Sirius and Cygnus X1 Cygnus X1 being a well known black hole. Binary stars are also common as the nuclei of many planetary nebulae and are the progenitors of both novae and type Ia supernovae.  The Double Star Library at the U.S. Naval Observatory ianridpath.com List of the best visual binaries for amateurs with orbital elements Pictures of binaries at Hubblesite.org Chandra Xray Observatory Binary Stars at DMOZ An extensive simulation for the Algol system by North Carolina State University Selected visual double stars and their relative position as a function of time Artistic representations of binary stars by Mark A. Garlick Orbits and Velocity Curves of Spectroscopic Binaries J. Miller Barr 1908 Eclipsing Binaries in the 21st CenturyOpportunities for Amateur Astronomers"}, {"topic": "Binding energy", "content": "Binding energy is the energy required to disassemble a whole system into separate parts. A bound system typically has a lower potential energy than the sum of its constituent parts this is what keeps the system together. Often this means that energy is released upon the creation of a bound state. This definition corresponds to a positive binding energy.  Nuclear Binding energy Mass and Nuclide Stability Experimental atomic mass data compiled Nov. 2003"}, {"topic": "Binomial random variable", "content": "In probability theory and statistics the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent yesno experiments each of which yields success with probability p. A successfailure experiment is also called a Bernoulli experiment or Bernoulli trial when n  1 the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance. The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement the draws are not independent and so the resulting distribution is a hypergeometric distribution not a binomial one. However for N much larger than n the binomial distribution is a good approximation and widely used.  Interactive graphic Univariate Distribution Relationships Binomial distribution formula calculator Binomial distribution calculator Difference of two binomial variables XY or XY Querying the binomial probability distribution in WolframAlpha"}, {"topic": "Biocatalysis", "content": "Biocatalysis baoktlss is catalysis in living biological systems. In biocatalytic processes natural catalysts such as protein enzymes perform chemical transformations on organic compounds. Both enzymes that have been more or less isolated and enzymes still residing inside living cells are employed for this task.  The Centre of Excellence for Biocatalysis  CoEBio3 The University of Exeter  Biocatalysis Centre Center for Biocatalysis and Bioprocessing  The University of Iowa TU Delft  Biocatalysis  Organic Chemistry BOC KTH Stockholm  Biocatalysis Research Group Institute of Technical Biocatalysis at the Hamburg University of Technology TUHH"}, {"topic": "Biomechanics", "content": "Biomechanics is the study of the structure and function of biological systems such as humans animals plants organs fungi and cells by means of the methods of mechanics.  Biomechanics and Movement Science Listserver BiomchL Biomechanics Links A Genealogy of Biomechanics"}, {"topic": "Biophysics", "content": "Biophysics is an interdisciplinary science that applies the approaches and methods of physics to study biological systems. Biophysics covers all scales of biological organization from molecular to organismic and populations. Biophysical research shares significant overlap with biochemistry nanotechnology bioengineering computational biology and systems biology. The term biophysics was originally introduced by Karl Pearson in 1892.  Biophysical Society Journal of Physiology 2012 virtual issue Biophysics and Beyond biophysicswiki Link archive of learning resources for students biophysika.de 60 English 40 German Journal of Medicine Physiology and BiophysicsIISTE USA. Chief Editor of the journal is Ignat Ignatov. Chief editor of all IISTE journals is Alexander Decker."}, {"topic": "Black-body radiation", "content": "Blackbody radiation is the type of electromagnetic radiation within or surrounding a body in thermodynamic equilibrium with its environment or emitted by a black body an opaque and nonreflective body assumed for the sake of calculations and theory to be held at constant uniform temperature. The radiation has a specific spectrum and intensity that depends only on the temperature of the body. The thermal radiation spontaneously emitted by many ordinary objects can be approximated as blackbody radiation. A perfectly insulated enclosure that is in thermal equilibrium internally contains blackbody radiation and will emit it through a hole made in its wall provided the hole is small enough to have negligible effect upon the equilibrium. A blackbody at room temperature appears black as most of the energy it radiates is infrared and cannot be perceived by the human eye. Because the human eye cannot perceive color at very low light intensities a black body viewed in the dark at the lowest just faintly visible temperature subjectively appears grey but only because the human eye is sensitive only to black and white at very low intensities  in reality the frequency of the light in the visible range would still be red although the intensity would be too low to discern as red even though its objective physical spectrum peaks in the infrared range. When it becomes a little hotter it appears dull red. As its temperature increases further it eventually becomes bluewhite. Although planets and stars are neither in thermal equilibrium with their surroundings nor perfect black bodies blackbody radiation is used as a first approximation for the energy they emit. Black holes are nearperfect black bodies in the sense that they absorb all the radiation that falls on them. It has been proposed that they emit blackbody radiation called Hawking radiation with a temperature that depends on the mass of the black hole. The term black body was introduced by Gustav Kirchhoff in 1860. Blackbody radiation is also called complete radiation or temperature radiation or thermal radiation.  Calculating Blackbody Radiation Interactive calculator with Doppler Effect. Includes most systems of units. ColortoTemperature demonstration at Academo.org Cooling Mechanisms for Human Body From Hyperphysics Descriptions of radiation emitted by many different objects BlackBody Emission Applet Blackbody Spectrum by Jeff Bryant Wolfram Demonstrations Project 2007."}, {"topic": "Black hole", "content": "A black hole is a region of spacetime exhibiting such strong gravitational effects that nothingincluding particles and electromagnetic radiation such as lightcan escape from inside it. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of the region from which no escape is possible is called the event horizon. Although crossing the event horizon has enormous effect on the fate of the object crossing it it appears to have no locally detectable features. In many ways a black hole acts like an ideal black body as it reflects no light. Moreover quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is on the order of billionths of a kelvin for black holes of stellar mass making it essentially impossible to observe. Objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and PierreSimon Laplace. The first modern solution of general relativity that would characterize a black hole was found by Karl Schwarzschild in 1916 although its interpretation as a region of space from which nothing can escape was first published by David Finkelstein in 1958. Black holes were long considered a mathematical curiosity it was during the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery of neutron stars sparked interest in gravitationally collapsed compact objects as a possible astrophysical reality. Black holes of stellar mass are expected to form when very massive stars collapse at the end of their life cycle. After a black hole has formed it can continue to grow by absorbing mass from its surroundings. By absorbing other stars and merging with other black holes supermassive black holes of millions of solar masses M may form. There is general consensus that supermassive black holes exist in the centers of most galaxies. Despite its invisible interior the presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Matter that falls onto a black hole can form an external accretion disk heated by friction forming some of the brightest objects in the universe. If there are other stars orbiting a black hole their orbits can be used to determine the black holes mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A at the core of our own Milky Way galaxy contains a supermassive black hole of about 4.3 million solar masses. On 11 February 2016 the LIGO collaboration announced the first observation of gravitational waves because these waves were generated from a black hole merger it was the first ever direct detection of a binary black hole merger. On 15 June 2016 a second detection of a gravitational wave event from colliding black holes was announced.  Black Holes on In Our Time at the BBC. listen now Stanford Encyclopedia of Philosophy Singularities and Black Holes by Erik Curiel and Peter Bokulich. Black Holes Gravitys Relentless PullInteractive multimedia Web site about the physics and astronomy of black holes from the Space Telescope Science Institute Frequently Asked Questions FAQs on Black Holes Schwarzschild Geometry Hubble site Videos 16yearlong study tracks stars orbiting Milky Way black hole Movie of Black Hole Candidate from Max Planck Institute Nature.com 20150420 3D simulations of colliding black holes Computer visualisation of the signal detected by LIGO Two Black Holes Merge into One based upon the signal GW150914"}, {"topic": "Block and tackle", "content": "A block and tackle is a system of two or more pulleys with a rope or cable threaded between them usually used to lift or pull heavy loads. The pulleys are assembled together to form blocks and then blocks are paired so that one is fixed and one moves with the load. The rope is threaded or rove through the pulleys to provide mechanical advantage that amplifies that force applied to the rope. Hero of Alexandria described cranes formed from assemblies of pulleys in the first century. Illustrated versions of Heros book on raising heavy weights show early block and tackle systems.  Fendt Walter March 1998. Pulley System Model and demonstration JAVA applet. Retrieved 20090825. Sailing tackle animations nomenclature and table of mechanical advantages Smack Dock Soundings 16. 1997. REEVING OF BLOCKS"}, {"topic": "Bohr model", "content": "In atomic physics the RutherfordBohr model or Bohr model introduced by Niels Bohr in 1913 depicts the atom as a small positively charged nucleus surrounded by electrons that travel in circular orbits around the nucleussimilar in structure to the solar system but with attraction provided by electrostatic forces rather than gravity. After the cubic model 1902 the plumpudding model 1904 the Saturnian model 1904 and the Rutherford model 1911 came the RutherfordBohr model or just Bohr model for short 1913. The improvement to the Rutherford model is mostly a quantum physical interpretation of it. The Bohr model has been superseded but the quantum theory remains sound. The models key success lay in explaining the Rydberg formula for the spectral emission lines of atomic hydrogen. While the Rydberg formula had been known experimentally it did not gain a theoretical underpinning until the Bohr model was introduced. Not only did the Bohr model explain the reason for the structure of the Rydberg formula it also provided a justification for its empirical results in terms of fundamental physical constants. The Bohr model is a relatively primitive model of the hydrogen atom compared to the valence shell atom. As a theory it can be derived as a firstorder approximation of the hydrogen atom using the broader and much more accurate quantum mechanics and thus may be considered to be an obsolete scientific theory. However because of its simplicity and its correct results for selected systems see below for application the Bohr model is still commonly taught to introduce students to quantum mechanics or energy level diagrams before moving on to the more accurate but more complex valence shell atom. A related model was originally proposed by Arthur Erich Haas in 1910 but was rejected. The quantum theory of the period between Plancks discovery of the quantum 1900 and the advent of a fullblown quantum mechanics 1925 is often referred to as the old quantum theory.  Standing waves in Bohrs atomic model An interactive simulation to intuitively explain the quantization condition of standing waves in Bohrs atomic model"}, {"topic": "Boiling point", "content": "The boiling point of a substance is the temperature at which the vapor pressure of the liquid equals the pressure surrounding the liquid and the liquid changes into a vapor. The boiling point of a liquid varies depending upon the surrounding environmental pressure. A liquid in a partial vacuum has a lower boiling point than when that liquid is at atmospheric pressure. A liquid at high pressure has a higher boiling point than when that liquid is at atmospheric pressure. For a given pressure different liquids boil at different temperatures. The normal boiling point also called the atmospheric boiling point or the atmospheric pressure boiling point of a liquid is the special case in which the vapor pressure of the liquid equals the defined atmospheric pressure at sea level 1 atmosphere. At that temperature the vapor pressure of the liquid becomes sufficient to overcome atmospheric pressure and allow bubbles of vapor to form inside the bulk of the liquid. The standard boiling point has been defined by IUPAC since 1982 as the temperature at which boiling occurs under a pressure of 1 bar. The heat of vaporization is the energy required to transform a given quantity a mol kg pound etc. of a substance from a liquid into a gas at a given pressure often atmospheric pressure. Liquids may change to a vapor at temperatures below their boiling points through the process of evaporation. Evaporation is a surface phenomenon in which molecules located near the liquids edge not contained by enough liquid pressure on that side escape into the surroundings as vapor. On the other hand boiling is a process in which molecules anywhere in the liquid escape resulting in the formation of vapor bubbles within the liquid.  BoilingPoint. The New Students Reference Work. 1914."}, {"topic": "Boiling point elevation", "content": "Boilingpoint elevation describes the phenomenon that the boiling point of a liquid a solvent will be higher when another compound is added meaning that a solution has a higher boiling point than a pure solvent. This happens whenever a nonvolatile solute such as a salt is added to a pure solvent such as water. The boiling point can be measured accurately using an ebullioscope. "}, {"topic": "Boltzmann constant", "content": "The Boltzmann constant kB or k named after Ludwig Boltzmann is a physical constant relating energy at the individual particle level with temperature. It is the gas constant R divided by the Avogadro constant NA k  R N A .  where Pi is the probability of each microstate. The value chosen for a unit of the Planck temperature is that corresponding to the energy of the Planck mass or 70321416833000000001.416833851032 K.  Draft Chapter 2 for SI Brochure following redefinitions of the base units prepared by the Consultative Committee for Units Big Step Towards Redefining the Kelvin Scientists Find New Way to Determine Boltzmann Constant"}, {"topic": "Bose-Einstein condensate", "content": "A BoseEinstein condensate BEC is a state of matter of a dilute gas of bosons cooled to temperatures very close to absolute zero that is very near 50000000000000000000 K or 5000000000000000000273.15 C. Under such conditions a large fraction of bosons occupy the lowest quantum state at which point macroscopic quantum phenomena become apparent. It is formed by cooling a gas of extremely low density about onehundredthousandth the density of normal air to ultralow temperatures. This state was first predicted generally in 192425 by Satyendra Nath Bose and Albert Einstein.  BoseEinstein Condensation 2009 Conference BoseEinstein Condensation 2009 Frontiers in Quantum Gases BEC Homepage General introduction to BoseEinstein condensation Nobel Prize in Physics 2001 for the achievement of BoseEinstein condensation in dilute gases of alkali atoms and for early fundamental studies of the properties of the condensates Physics Today Cornell Ketterle and Wieman Share Nobel Prize for BoseEinstein Condensates BoseEinstein Condensates at JILA Atomcool at Rice University Alkali Quantum Gases at MIT Atom Optics at UQ Einsteins manuscript on the BoseEinstein condensate discovered at Leiden University BoseEinstein condensate on arxiv.org Bosons The Birds That Flock and Sing Together Easy BEC machine information on constructing a BoseEinstein condensate machine. Verging on absolute zero Cosmos Online Lecture by W Ketterle at MIT in 2001 BoseEinstein Condensation at NIST NIST resource on BEC"}, {"topic": "Boson", "content": "In quantum mechanics a boson bosn bozn is a particle that follows BoseEinstein statistics. Bosons make up one of the two classes of particles the other being fermions. The name boson was coined by Paul Dirac to commemorate the contribution of the Indian physicist Satyendra Nath Bose in developing with Einstein BoseEinstein statisticswhich theorizes the characteristics of elementary particles. Examples of bosons include fundamental particles such as photons gluons and W and Z bosons the four forcecarrying gauge bosons of the Standard Model the recently discovered Higgs boson and the hypothetical graviton of quantum gravity composite particles e.g. mesons and stable nuclei of even mass number such as deuterium with one proton and one neutron mass number  2 helium4 or lead208 and some quasiparticles e.g. Cooper pairs plasmons and phonons. An important characteristic of bosons is that their statistics do not restrict the number of them that occupy the same quantum state. This property is exemplified by helium4 when it is cooled to become a superfluid. Unlike bosons two identical fermions cannot occupy the same quantum space. Whereas the elementary particles that make up matter i.e. leptons and quarks are fermions the elementary bosons are force carriers that function as the glue holding matter together. This property holds for all particles with integer spin s  0 1 2 etc. as a consequence of the spinstatistics theorem. When a gas of Bose particles is cooled down to temperatures very close to absolute zero then the kinetic energy of the particles decreases to a negligible amount and they condense into a lowest energy level state. This state is called BoseEinstein condensation. It is believed that this property is the explanation of superfluidity. "}, {"topic": "Boyle's law", "content": "Boyles law sometimes referred to as the BoyleMariotte law or Mariottes law is an experimental gas law that describes how the pressure of a gas tends to increase as the volume of a gas decreases. A modern statement of Boyles law is The absolute pressure exerted by a given mass of an ideal gas is inversely proportional to the volume it occupies if the temperature and amount of gas remain unchanged within a closed system. Mathematically Boyles law can be stated as P 1 V  Here P1 and V1 represent the original pressure and volume respectively and P2 and V2 represent the second pressure and volume. Boyles law Charless law and GayLussacs law form the combined gas law. The three gas laws in combination with Avogadros law can be generalized by the ideal gas law.  Britannica Educational Publishing 1 December 2012. Scientists and Inventors of the Renaissance. Britannica Educational Publishing. ISBN 9781615308842."}, {"topic": "Bragg's law", "content": "In physics Braggs Law or WulffBraggs condition in postSoviet countries a special case of Laue diffraction gives the angles for coherent and incoherent scattering from a crystal lattice. When Xrays are incident on an atom they make the electronic cloud move as does any electromagnetic wave. The movement of these charges reradiates waves with the same frequency blurred slightly due to a variety of effects this phenomenon is known as Rayleigh scattering or elastic scattering. The scattered waves can themselves be scattered but this secondary scattering is assumed to be negligible. A similar process occurs upon scattering neutron waves from the nuclei or by a coherent spin interaction with an unpaired electron. These reemitted wave fields interfere with each other either constructively or destructively overlapping waves either add up together to produce stronger peaks or are subtracted from each other to some degree producing a diffraction pattern on a detector or film. The resulting wave interference pattern is the basis of diffraction analysis. This analysis is called Bragg diffraction.  Nobel Prize in Physics  1915 httpwww.citycollegiate.cominterferencebraggs.htm httpwww.physics.uoguelph.cadetongphys35104500xray.pdf Learning crystallography"}, {"topic": "Bra-ket notation", "content": "In quantum mechanics braket notation is a standard notation for describing quantum states composed of angle brackets and vertical bars. It can also be used to denote abstract vectors and linear functionals in mathematics. In such terms the scalar product or action of a linear functional on a vector in a complex vector space is denoted by   Richard Fitzpatrick Quantum Mechanics A graduate level course The University of Texas at Austin. Includes 1. Ket space 2. Bra space 3. Operators 4. The outer product 5. Eigenvalues and eigenvectors Robert Littlejohn Lecture notes on The Mathematical Formalism of Quantum mechanics including braket notation. Unviversity of California Berkeley. Gieres F. 2000. Mathematical surprises and Diracs formalism in quantum mechanics. Rep. Prog. Phys. 63 18931931. arXivquantph9907069. Bibcode2000RPPh...63.1893G. doi10.1088003448856312201."}, {"topic": "Brewster's angle", "content": "Brewsters angle also known as the polarization angle is an angle of incidence at which light with a particular polarization is perfectly transmitted through a transparent dielectric surface with no reflection. When unpolarized light is incident at this angle the light that is reflected from the surface is therefore perfectly polarized. This special angle of incidence is named after the Scottish physicist Sir David Brewster 17811868.  Brewsters Angle Extraction from Wolfram Research Brewster window at RPphotonics.com TETM Reflection Coefficients interactive phase and magnitude plots showing Brewsters angle"}, {"topic": "British thermal unit", "content": "The British thermal unit BTU or Btu is a traditional unit of work equal to about 1.055 kilojoules. It is the amount of work needed to raise the temperature of one pound of water by one degree Fahrenheit. One fourinch wooden kitchen match consumed completely generates approximately 1 BTU. In science and engineering the joule the SI unit of energy has largely replaced the BTU. The BTUh is most often used as a measure of power in the electric power steam generation heating and air conditioning industries. It is still used in some metric Englishspeaking countries such as Canada but notably not the United Kingdom. In North America the heat value energy content of fuels is often expressed in BTUs. The notation kBtu or KBTU is often used for thousand BTU in sizing of heating systems and in the Energy Use Index EUI expressed as thousand BTU annual energy use per square foot of building. MBTU represents one million Btu although the atypical notation MMBtu or mmBtu is sometimes used to represent one million BTU.see definitions below  British thermal unit BTU unit of measurement at Encyclopdia Britannica The Units of Measurement Regulations 1995 HMSO Natural Gas A Primer Natural Resources Canada 20150617 NIST Special Publication 811 Guide for the Use of the International System of Units SI 2008 Edition by Ambler Thompson and Barry N. Taylor. from NIST particularly conversions on p.58"}, {"topic": "Brittle", "content": "A material is brittle if when subjected to stress it breaks without significant deformation strain. Brittle materials absorb relatively little energy prior to fracture even those of high strength. Breaking is often accompanied by a snapping sound. Brittle materials include most ceramics and glasses which do not deform plastically and some polymers such as PMMA and polystyrene. Many steels become brittle at low temperatures see ductilebrittle transition temperature depending on their composition and processing. When used in materials science it is generally applied to materials that fail when there is little or no evidence of plastic deformation before failure. One proof is to match the broken halves which should fit exactly since no plastic deformation has occurred. When a material has reached the limit of its strength it usually has the option of either deformation or fracture. A naturally malleable metal can be made stronger by impeding the mechanisms of plastic deformation reducing grain size precipitation hardening work hardening etc. but if this is taken to an extreme fracture becomes the more likely outcome and the material can become brittle. Improving material toughness is therefore a balancing act.  Lewis Peter Rhys Reynolds K Gagg C 2004. Forensic Materials Engineering Case studies. CRC Press. ISBN 9780849311826. Rsler Joachim Harders Harald Bker Martin 2007. Mechanical behaviour of engineering materials metals ceramics polymers and composites. Springer. ISBN 9783642092527."}, {"topic": "Brownian motion", "content": "Brownian motion or pedesis from Ancient Greek pdsis leaping is the random motion of particles suspended in a fluid a liquid or a gas resulting from their collision with the fastmoving atoms or molecules in the gas or liquid. This transport phenomenon is named after the botanist Robert Brown. In 1827 while looking through a microscope at particles trapped in cavities inside pollen grains in water he noted that the particles moved through the water but he was not able to determine the mechanisms that caused this motion. Atoms and molecules had long been theorized as the constituents of matter and Albert Einstein published a paper in 1905 that explained in precise detail how the motion that Brown had observed was a result of the pollen being moved by individual water molecules. This explanation of Brownian motion served as convincing evidence that atoms and molecules exist and was further verified experimentally by Jean Perrin in 1908. Perrin was awarded the Nobel Prize in Physics in 1926 for his work on the discontinuous structure of matter Einstein had received the award five years earlier for his services to theoretical physics with specific citation of different research. The direction of the force of atomic bombardment is constantly changing and at different times the particle is hit more on one side than another leading to the seemingly random nature of the motion. Brownian motion is among the simplest of the continuoustime stochastic or probabilistic processes and it is a limit of both simpler and more complicated stochastic processes see random walk and Donskers theorem. This universality is closely related to the universality of the normal distribution. In both cases it is often mathematical convenience rather than the accuracy of the models that motivates their use.  Brownian motion java simulation A singlemolecule brownian motion diffusion simulator Article for the schoolgoing child Einstein on Brownian Motion Brownian Motion Diverse and Undulating Discusses history botany and physics of Browns original observations with videos Einsteins prediction finally witnessed one century later  a test to observe the velocity of Brownian motion Caf math  brownian motion Part 1  A blog article describing brownian motion definition symmetries simulation Finite different algorithm to simulate the Brownian motion of a particle subscription required"}, {"topic": "Bulk modulus", "content": "The bulk modulus  K  have very similar values. Solids can also sustain transverse waves for these materials one additional elastic modulus for example the shear modulus is needed to determine wave speeds.  De Jong Maarten Chen Wei 2015. Charting the complete elastic properties of inorganic crystalline compounds. Scientific Data 2 150009. doi10.1038sdata.2015.9."}, {"topic": "Buoyancy", "content": "In science buoyancy pronunciation b.nsi or bujnsi also known as upthrust is an upward force exerted by a fluid that opposes the weight of an immersed object. In a column of fluid pressure increases with depth as a result of the weight of the overlying fluid. Thus the pressure at the bottom of a column of fluid is greater than at the top of the column. Similarly the pressure at the bottom of an object submerged in a fluid is greater than at the top of the object. This pressure difference results in a net upwards force on the object. The magnitude of that force exerted is proportional to that pressure difference and as explained by Archimedes principle is equivalent to the weight of the fluid that would otherwise occupy the volume of the object i.e. the displaced fluid. For this reason an object whose density is greater than that of the fluid in which it is submerged tends to sink. If the object is either less dense than the liquid or is shaped appropriately as in a boat the force can keep the object afloat. This can occur only in a reference frame which either has a gravitational field or is accelerating due to a force other than gravity defining a downward direction that is a noninertial reference frame. In a situation of fluid statics the net upward buoyancy force is equal to the magnitude of the weight of fluid displaced by the body. The center of buoyancy of an object is the centroid of the displaced volume of fluid.  Falling in Water Archimedes Principle background and experiment BuoyancyQuest a website featuring buoyancy control videos W. H. Besant 1889 Elementary Hydrostatics from Google Books. NASAs definition of buoyancy"}, {"topic": "Calculus", "content": "Calculus from Latin calculus literally small pebble used for counting is the mathematical study of change in the same way that geometry is the study of shape and algebra is the study of operations and their application to solving equations. It has two major branches differential calculus concerning rates of change and slopes of curves and integral calculus concerning accumulation of quantities and the areas under and between curves these two branches are related to each other by the fundamental theorem of calculus. Both branches make use of the fundamental notions of convergence of infinite sequences and infinite series to a welldefined limit. Generally modern calculus is considered to have been developed in the 17th century by Isaac Newton and Gottfried Leibniz. Today calculus has widespread uses in science engineering and economics and can solve many problems that elementary algebra alone cannot. Calculus is a part of modern mathematics education. A course in calculus is a gateway to other more advanced courses in mathematics devoted to the study of functions and limits broadly called mathematical analysis. Calculus has historically been called the calculus of infinitesimals or infinitesimal calculus. Calculus plural calculi is also used for naming some methods of calculation or theories of computation such as propositional calculus calculus of variations lambda calculus and process calculus.  Hazewinkel Michiel ed. 2001 Calculus Encyclopedia of Mathematics Springer ISBN 9781556080104 Weisstein Eric W. Calculus MathWorld. Topics on Calculus at PlanetMath.org. Calculus Made Easy 1914 by Silvanus P. Thompson Full text in PDF Calculus on In Our Time at the BBC. listen now Calculus.org The Calculus page at University of California Davis contains resources and links to other sites COW Calculus on the Web at Temple University contains resources ranging from precalculus and associated algebra Earliest Known Uses of Some of the Words of Mathematics Calculus  Analysis Online Integrator WebMathematica from Wolfram Research The Role of Calculus in College Mathematics from ERICDigests.org OpenCourseWare Calculus from the Massachusetts Institute of Technology Infinitesimal Calculus an article on its historical development in Encyclopedia of Mathematics ed. Michiel Hazewinkel. Daniel Kleitman MIT. Calculus for Beginners and Artists. Calculus Problems and Solutions by D. A. Kouba Donald Allens notes on calculus Calculus training materials at imomath.com English Arabic The Excursion of Calculus 1772"}, {"topic": "Capacitance", "content": "Capacitance is the ability of a body to store an electrical charge. A material with a large capacitance holds more electric charge at a given voltage than one with low capacitance. Any object that can be electrically charged exhibits capacitance however the concept is particularly important for understanding the operations of the capacitor one of the three fundamental electronic components along with resistors and inductors. The SI unit of capacitance is the farad symbol F named after the English physicist Michael Faraday. A 1 farad capacitor when charged with 1 coulomb of electrical charge has a potential difference of 1 volt between its plates. "}, {"topic": "Capacitive reactance", "content": "In electrical and electronic systems reactance is the opposition of a circuit element to a change in current or voltage due to that elements inductance or capacitance. A builtup electric field resists the change of voltage on the element while a magnetic field resists the change of current. The notion of reactance is similar to electrical resistance but it differs in several respects. In phasor analysis reactance is used to compute amplitude and phase changes of sinusoidal alternating current going through a circuit element. It is denoted by the symbol X  phase difference with the sinusoidal current through the component. The component alternately absorbs energy from the circuit and then returns energy to the circuit thus a pure reactance does not dissipate power.  Interactive Java Tutorial on Inductive Reactance National High Magnetic Field Laboratory Reactance calculator"}, {"topic": "Cardiophysics", "content": "Cardiophysics is an interdisciplinary science that stands at the junction of cardiology and medical physics with researchers using the methods of and theories from physics to study cardiovascular system at different levels of its organisation from the molecular scale to whole organisms. Being formed historically as part of systems biology cardiophysics designed to reveal connections between the physical mechanisms underlying the organization of the cardiovascular system and biological features of its functioning. One can use interchangeably also the terms cardiovascular physics.  Bioelectric Information Processing Laboratory of the Institute for Information Transmission Problems RAS. Russian The Group of Experimental and Clinical Cardiology in the Laboratory of Physiology of emotion Research Institute of normal physiology by Anokhin RAMS Oxford Cardiac Electrophysiology Group led many years already by Prof. Denis Noble Cardiac Biophysics and Systems Biology group of National Heart  Lung Institute of Imperial College London German Group of Nonlinear Dynamics  Cardiovascular Physics of the 1st Faculty of Mathematics and Natural Sciences in the Institute of Physics of Humboldt University of Berlin"}, {"topic": "Carnot cycle", "content": "The Carnot cycle is a theoretical thermodynamic cycle proposed by Nicolas Lonard Sadi Carnot in 1824 and expanded upon by others in the 1830s and 1840s. It provides an upper limit on the efficiency that any classical thermodynamic engine can achieve during the conversion of heat into work or conversely the efficiency of a refrigeration system in creating a temperature difference e.g. refrigeration by the application of work to the system. It is not an actual thermodynamic cycle but is a theoretical construct. Every single thermodynamic system exists in a particular state. When a system is taken through a series of different states and finally returned to its initial state a thermodynamic cycle is said to have occurred. In the process of going through this cycle the system may perform work on its surroundings thereby acting as a heat engine. A system undergoing a Carnot cycle is called a Carnot heat engine although such a perfect engine is only a theoretical construct and cannot be built in practice.  Hyperphysics article on the Carnot cycle. Interactive Java applet showing behavior of a Carnot engine."}, {"topic": "Cartesian coordinates", "content": "A Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates which are the signed distances to the point from two fixed perpendicular directed lines measured in the same unit of length. Each reference line is called a coordinate axis or just axis of the system and the point where they meet is its origin usually at ordered pair 0 0. The coordinates can also be defined as the positions of the perpendicular projections of the point onto the two axes expressed as signed distances from the origin. One can use the same principle to specify the position of any point in threedimensional space by three Cartesian coordinates its signed distances to three mutually perpendicular planes or equivalently by its perpendicular projection onto three mutually perpendicular lines. In general n Cartesian coordinates an element of real nspace specify the point in an ndimensional Euclidean space for any dimension n. These coordinates are equal up to sign to distances from the point to n mutually perpendicular hyperplanes. The invention of Cartesian coordinates in the 17th century by Ren Descartes Latinized name Cartesius revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system geometric shapes such as curves can be described by Cartesian equations algebraic equations involving the coordinates of the points lying on the shape. For example a circle of radius 2 centered at the origin of the plane may be described as the set of all points whose coordinates x and y satisfy the equation x2  y2  4. Cartesian coordinates are the foundation of analytic geometry and provide enlightening geometric interpretations for many other branches of mathematics such as linear algebra complex analysis differential geometry multivariate calculus group theory and more. A familiar example is the concept of the graph of a function. Cartesian coordinates are also essential tools for most applied disciplines that deal with geometry including astronomy physics engineering and many more. They are the most common coordinate system used in computer graphics computeraided geometric design and other geometryrelated data processing.  Cartesian Coordinate System Printable Cartesian Coordinates Cartesian coordinates at PlanetMath.org. MathWorld description of Cartesian coordinates Coordinate Converter converts between polar Cartesian and spherical coordinates Coordinates of a point Interactive tool to explore coordinates of a point open source JavaScript class for 2D3D Cartesian coordinate system manipulation"}, {"topic": "Cathode", "content": "A cathode is the electrode from which a conventional current leaves a polarized electrical device. This definition can be recalled by using the mnemonic CCD for cathode current departs. A conventional current describes the direction in which positive electronic charges move. Electrons have a negative charge so the movement of electrons is opposite to the conventional current flow. Consequently the mnemonic cathode current departs also means that electrons flow into the devices cathode. Cathode polarity with respect to the anode can be positive or negative it depends on how the device operates. Although positively charged cations always move towards the cathode hence their name and negatively charged anions move away from it cathode polarity depends on the device type and can even vary according to the operating mode. In a device which takes energy such as recharging a battery the cathode is negative and in a device which provides energy such as discharging a battery the cathode is positive In a discharging battery or a galvanic cell the cathode is the positive terminal since that is where the current flows out of the device see drawing. This outward current is carried internally by positive ions moving from the electrolyte to the positive cathode chemical energy is responsible for this uphill motion. It is continued externally by electrons moving inwards this negative charge moving inwards constituting positive current flowing outwards. For example the Daniell galvanic cells copper electrode is the positive terminal and the cathode. In a recharging battery or an electrolytic cell performing electrolysis the cathode is the negative terminal from which current exits the device and returns to the external generator. For example reversing the current direction in a Daniell galvanic cell would produce an electrolytic cell where the copper electrode is the positive terminal and the anode. In a diode the cathode is the negative terminal at the pointed end of the arrow symbol where current flows out of the device. Note electrode naming for diodes is always based on the direction of the forward current that of the arrow in which the current flows most easily even for types such as Zener diodes or solar cells where the current of interest is the reverse current. In vacuum tubes including cathode ray tubes it is the negative terminal where electrons enter the device from the external circuit and proceed into the tubes near vacuum constituting a positive current flowing out of the device. An electrode through which current flows the other way into the device is termed an anode.  The Cathode Ray Tube site How to define anode and cathode"}, {"topic": "Cathode ray", "content": "Cathode rays also called an electron beam or ebeam are streams of electrons observed in vacuum tubes. If an evacuated glass tube is equipped with two electrodes and a voltage is applied the glass opposite of the negative electrode is observed to glow due to electrons emitted from and travelling perpendicular to the cathode the electrode connected to the negative terminal of the voltage supply. They were first observed in 1869 by German physicist Johann Hittorf and were named in 1876 by Eugen Goldstein Kathodenstrahlen or cathode rays. Electrons were first discovered as the constituents of cathode rays. In 1897 British physicist J. J. Thomson showed the rays were composed of a previously unknown negatively charged particle which was later named the electron. Cathode ray tubes CRTs use a focused beam of electrons deflected by electric or magnetic fields to create the image in a classic television set.  The simulation show electrons in crossed fields made by BIGS"}, {"topic": "Celestial mechanics", "content": "Celestial mechanics is the branch of astronomy that deals with the motions of celestial objects. Historically celestial mechanics applies principles of physics classical mechanics to astronomical objects such as stars and planets to produce ephemeris data. As an astronomical field of study celestial mechanics includes the subfields of Orbital mechanics astrodynamics which deals with the orbit of an artificial satellite and Lunar theory which deals with the orbit of the Moon.  Calvert James B. 20030328 Celestial Mechanics University of Denver retrieved 20060821 Astronomy of the Earths Motion in Space highschool level educational web site by David P. Stern Newtonian Dynamics Undergraduate level course by Richard Fitzpatrick. This includes Langrangian and Hamiltonian Dynamics and applications to celestial mechanics gravitational potential theory the 3body problem and Lunar motion an example of the 3body problem with the Sun Moon and the Earth. Research Marshall Hamptons research page Central configurations in the nbody problem Artwork Celestial Mechanics is a Planetarium Artwork created by D. S. Hessels and G. Dunne Course notes Professor Tatums course notes at the University of Victoria Associations Italian Celestial Mechanics and Astrodynamics Association Simulations"}, {"topic": "Celsius scale", "content": "Celsius also known as centigrade is a scale and unit of measurement for temperature. As an SI derived unit it is used by most countries in the world. It is named after the Swedish astronomer Anders Celsius 17011744 who developed a similar temperature scale. The degree Celsius C can refer to a specific temperature on the Celsius scale as well as a unit to indicate a temperature interval a difference between two temperatures or an uncertainty. Before being renamed to honour Anders Celsius in 1948 the unit was called centigrade from the Latin centum which means 100 and gradus which means steps. From 1744 until 1954 0 C was defined as the freezing point of water and 100 C was defined as the boiling point of water both at a pressure of one standard atmosphere with mercury being the working material. Although these defining correlations are commonly taught in schools today by international agreement the unit degree Celsius and the Celsius scale are currently defined by two different temperatures absolute zero and the triple point of VSMOW specially purified water. This definition also precisely relates the Celsius scale to the Kelvin scale which defines the SI base unit of thermodynamic temperature with symbol K. Absolute zero the lowest temperature possible is defined as being precisely 0 K and 273.15 C. The temperature of the triple point of water is defined as precisely 273.16 K and 0.01 C. This definition fixes the magnitude of both the degree Celsius and the kelvin as precisely 1 part in 273.16 approximately 0.00366 of the difference between absolute zero and the triple point of water. Thus it sets the magnitude of one degree Celsius and that of one kelvin as exactly the same. Additionally it establishes the difference between the two scales null points as being precisely 273.15 degrees Celsius 273.15 C  0 K and 0 C  273.15 K.  NIST Basic unit definitions Kelvin The Uppsala Astronomical Observatory History of the Celsius temperature scale London South Bank University Water scientific data BIPM SI brochure section 2.1.1.5 Unit of thermodynamic temperature TAMPILE Comparison of temperature scales C to F converter Celsius to Fahrenheit Converter Celsius to Fahrenheit converter"}, {"topic": "Center of curvature", "content": "In geometry the center of curvature of a curve is found at a point that is at a distance from the curve equal to the radius of curvature lying on the normal vector. It is the point at infinity if the curvature is zero. The osculating circle to the curve is centered at the centre of curvature. Cauchy defined the center of curvature C as the intersection point of two infinitely close normal lines to the curve. The locus of centers of curvature for each point on the curve comprise the evolute of the curve.  Hilbert David CohnVossen Stephan 1952 Geometry and the Imagination 2nd ed. New York Chelsea ISBN 9780828400879"}, {"topic": "Center of gravity", "content": "The Center of gravity of a body is that point through which the resultant of the system of parallel forces formed by the weights of all the particles constituting the body passes for all positions of the body. It is denoted as C.G or G. In a uniform gravitational field the center of gravity is identical to the center of mass. "}, {"topic": "Center of mass", "content": "In physics the center of mass of a distribution of mass in space is the unique point where the weighted relative position of the distributed mass sums to zero or the point where if a force is applied causes it to move in direction of force without rotation. The distribution of mass is balanced around the center of mass and the average of the weighted position coordinates of the distributed mass defines its coordinates. Calculations in mechanics are often simplified when formulated with respect to the center of mass. In the case of a single rigid body the center of mass is fixed in relation to the body and if the body has uniform density it will be located at the centroid. The center of mass may be located outside the physical body as is sometimes the case for hollow or openshaped objects such as a horseshoe. In the case of a distribution of separate bodies such as the planets of the Solar System the center of mass may not correspond to the position of any individual member of the system. The center of mass is a useful reference point for calculations in mechanics that involve masses distributed in space such as the linear and angular momentum of planetary bodies and rigid body dynamics. In orbital mechanics the equations of motion of planets are formulated as point masses located at the centers of mass. The center of mass frame is an inertial frame in which the center of mass of a system is at rest with respect to the origin of the coordinate system.  Motion of the Center of Mass shows that the motion of the center of mass of an object in free fall is the same as the motion of a point object. The Solar Systems barycenter simulations showing the effect each planet contributes to the Solar Systems barycenter. Center of Gravity at Work video showing bjects climbing up an incline by themselves."}, {"topic": "Center of pressure (fluid mechanics)", "content": "The center of pressure is the point where the total sum of a pressure field acts on a body causing a force to act through that point. The total force vector acting at the center of pressure is the value of the integrated vectorial pressure field. The resultant force and center of pressure location produce equivalent force and moment on the body as the original pressure field. Pressure fields occur in both static and dynamic fluid mechanics. Specification of the center of pressure the reference point from which the center of pressure is referenced and the associated force vector allows the moment generated about any point to be computed by a translation from the reference point to the desired new point. It is common for the center of pressure to be located on the body but in fluid flows it is possible for the pressure field to exert a moment on the body of such magnitude that the center of pressure is located outside the body.  Hurt Hugh H. Jr. January 1965. Aerodynamics for Naval Aviators. Washington D.C. Naval Air Systems Command United States Navy. pp. 1621. NAVWEPS 0080T80. CS1 maint Multiple names authors list link Smith Hubert 1992. The Illustrated Guide to Aerodynamics 2nd ed.. New York TAB Books. pp. 2427. ISBN 0830639012. Anderson John D. 1999 Aircraft Performance and Design McGrawHill. ISBN 0071160108 Clancy L.J. 1975 Aerodynamics Pitman Publishing Limited London. ISBN 0273011200"}, {"topic": "Centigrade (temperature)", "content": "Celsius also known as centigrade is a scale and unit of measurement for temperature. As an SI derived unit it is used by most countries in the world. It is named after the Swedish astronomer Anders Celsius 17011744 who developed a similar temperature scale. The degree Celsius C can refer to a specific temperature on the Celsius scale as well as a unit to indicate a temperature interval a difference between two temperatures or an uncertainty. Before being renamed to honour Anders Celsius in 1948 the unit was called centigrade from the Latin centum which means 100 and gradus which means steps. From 1744 until 1954 0 C was defined as the freezing point of water and 100 C was defined as the boiling point of water both at a pressure of one standard atmosphere with mercury being the working material. Although these defining correlations are commonly taught in schools today by international agreement the unit degree Celsius and the Celsius scale are currently defined by two different temperatures absolute zero and the triple point of VSMOW specially purified water. This definition also precisely relates the Celsius scale to the Kelvin scale which defines the SI base unit of thermodynamic temperature with symbol K. Absolute zero the lowest temperature possible is defined as being precisely 0 K and 273.15 C. The temperature of the triple point of water is defined as precisely 273.16 K and 0.01 C. This definition fixes the magnitude of both the degree Celsius and the kelvin as precisely 1 part in 273.16 approximately 0.00366 of the difference between absolute zero and the triple point of water. Thus it sets the magnitude of one degree Celsius and that of one kelvin as exactly the same. Additionally it establishes the difference between the two scales null points as being precisely 273.15 degrees Celsius 273.15 C  0 K and 0 C  273.15 K.  NIST Basic unit definitions Kelvin The Uppsala Astronomical Observatory History of the Celsius temperature scale London South Bank University Water scientific data BIPM SI brochure section 2.1.1.5 Unit of thermodynamic temperature TAMPILE Comparison of temperature scales C to F converter Celsius to Fahrenheit Converter Celsius to Fahrenheit converter"}, {"topic": "Central force motion", "content": "In classical mechanics a central force on an object is a force whose magnitude only depends on the distance r of the object from the origin and is directed along the line joining them F  F  r   F    r    r   "}, {"topic": "Central limit theorem", "content": "In probability theory the central limit theorem CLT states that given certain conditions the arithmetic mean of a sufficiently large number of iterates of independent random variables each with a welldefined finite expected value and finite variance will be approximately normally distributed regardless of the underlying distribution. To illustrate what this means suppose that a sample is obtained containing a large number of observations each observation being randomly generated in a way that does not depend on the values of the other observations and that the arithmetic average of the observed values is computed. If this procedure is performed many times the central limit theorem says that the computed values of the average will be distributed according to the normal distribution commonly known as a bell curve. A simple example of this is that if one flips a coin many times the probability of getting a given number of heads should follow a normal curve with mean equal to half the total number of flips. The central limit theorem has a number of variants. In its common form the random variables must be identically distributed. In variants convergence of the mean to the normal distribution also occurs for nonidentical distributions or for nonindependent observations given that they comply with certain conditions. In more general usage a central limit theorem is any of a set of weakconvergence theorems in probability theory. They all express the fact that a sum of many independent and identically distributed i.i.d. random variables or alternatively random variables with specific types of dependence will tend to be distributed according to one of a small set of attractor distributions. When the variance of the i.i.d. variables is finite the attractor distribution is the normal distribution. In contrast the sum of a number of i.i.d. random variables with power law tail distributions decreasing as x1 where 0   2 and therefore having infinite variance will tend to an alphastable distribution with stability parameter or index of stability of as the number of variables grows.  Simplified stepbystep explanation of the classical Central Limit Theorem. with histograms at every step. Handson explanation of the Central Limit Theorem in tutorial videos from Khan Academy with many examples Central Limit Theorem Visualized in D3 interactive HTML5 simulation of flipping coins. Hazewinkel Michiel ed. 2001 Central limit theorem Encyclopedia of Mathematics Springer ISBN 9781556080104 Animated examples of the CLT Central Limit Theorem interactive simulation to experiment with various parameters CLT in NetLogo Connected Probability ProbLab interactive simulation w a variety of modifiable parameters General Central Limit Theorem Activity  corresponding SOCR CLT Applet Select the Sampling Distribution CLT Experiment from the dropdown list of SOCR Experiments Generate sampling distributions in Excel Specify arbitrary population sample size and sample statistic. MIT OpenCourseWare Lecture 18.440 Probability and Random Variables Spring 2011 Scott Sheffield Another proof. Retrieved 20120408. CAUSEweb.org is a site with many resources for teaching statistics including the Central Limit Theorem The Central Limit Theorem by Chris Boucher Wolfram Demonstrations Project. Weisstein Eric W. Central Limit Theorem MathWorld. Animations for the Central Limit Theorem by Yihui Xie using the R package animation Teaching demonstrations of the CLT clt.examp function in Greg Snow 2012. TeachingDemos Demonstrations for teaching and learning. R package version 2.8."}, {"topic": "Centrifugal force", "content": "In Newtonian mechanics the term centrifugal force is used to refer to an inertial force also called a fictitious force directed away from the axis of rotation that appears to act on all objects when viewed in a rotating reference frame. The concept of centrifugal force can be applied in rotating devices such as centrifuges centrifugal pumps centrifugal governors centrifugal clutches etc. as well as in centrifugal railways planetary orbits banked curves etc. when they are analyzed in a rotating coordinate system. The name has historically sometimes also been used to refer to the reaction force to the centripetal force. "}, {"topic": "Centripetal force", "content": "A centripetal force from Latin centrum center and petere to seek is a force that makes a body follow a curved path. Its direction is always orthogonal to the motion of the body and towards the fixed point of the instantaneous center of curvature of the path. Isaac Newton described it as a force by which bodies are drawn or impelled or in any way tend towards a point as to a centre. In Newtonian mechanics gravity provides the centripetal force responsible for astronomical orbits. One common example involving centripetal force is the case in which a body moves with uniform speed along a circular path. The centripetal force is directed at right angles to the motion and also along the radius towards the centre of the circular path. The mathematical description was derived in 1659 by the Dutch physicist Christiaan Huygens.  Notes from University of Winnipeg Notes from Physics and Astronomy HyperPhysics at Georgia State University see also home page Notes from Britannica Notes from PhysicsNet NASA notes by David P. Stern Notes from U Texas. Analysis of smart yoyo The Inuit yoyo Kinematic Models for Design Digital Library KMODDL Movies and photos of hundreds of working mechanicalsystems models at Cornell University. Also includes an ebook library of classic texts on mechanical design and engineering."}, {"topic": "Chain reaction", "content": "A chain reaction is a sequence of reactions where a reactive product or byproduct causes additional reactions to take place. In a chain reaction positive feedback leads to a selfamplifying chain of events. Chain reactions are one way in which systems which are in thermodynamic nonequilibrium can release energy or increase entropy in order to reach a state of higher entropy. For example a system may not be able to reach a lower energy state by releasing energy into the environment because it is hindered or prevented in some way from taking the path that will result in the energy release. If a reaction results in a small energy release making way for more energy releases in an expanding chain then the system will typically collapse explosively until much or all of the stored energy has been released. A macroscopic metaphor for chain reactions is thus a snowball causing a larger snowball until finally an avalanche results snowball effect. This is a result of stored gravitational potential energy seeking a path of release over friction. Chemically the equivalent to a snow avalanche is a spark causing a forest fire. In nuclear physics a single stray neutron can result in a prompt critical event which may be finally be energetic enough for a nuclear reactor meltdown or in a bomb a nuclear explosion.  IUPAC Gold Book  Chain reaction"}, {"topic": "Change of base rule", "content": "In mathematics the logarithm is the inverse operation to exponentiation. That means the logarithm of a number is the exponent to which another fixed value the base must be raised to produce that number. In simple cases the logarithm counts repeated multiplication. For example the base 10 logarithm of 1000 is 3 as 10 to the power 3 is 1000 1000  101010  103 the multiplication is repeated three times. More generally exponentiation allows any positive real number to be raised to any real power always producing a positive result so the logarithm can be calculated for any two positive real numbers b and x where b is not equal to 1. The logarithm of x to base b denoted logbx is the unique real number y such that by  x. For example as 64  26 then log264  6 The logarithm to base 10 that is b  10 is called the common logarithm and has many applications in science and engineering. The natural logarithm has the number e  2.718 as its base its use is widespread in mathematics and physics because of its simpler derivative. The binary logarithm uses base 2 that is b  2 and is commonly used in computer science. Logarithms were introduced by John Napier in the early 17th century as a means to simplify calculations. They were rapidly adopted by navigators scientists engineers and others to perform computations more easily using slide rules and logarithm tables. Tedious multidigit multiplication steps can be replaced by table lookups and simpler addition because of the fact important in its own right that the logarithm of a product is the sum of the logarithms of the factors log b  x y   log b  x   log b  y    It is related to the natural logarithm by Li1z  ln1 z. Moreover Lis1 equals the Riemann zeta function s.  Media related to Logarithm at Wikimedia Commons The dictionary definition of logarithm at Wiktionary Khan Academy Logarithms free online micro lectures Hazewinkel Michiel ed. 2001 Logarithmic function Encyclopedia of Mathematics Springer ISBN 9781556080104 Colin Byfleet Educational video on logarithms retrieved 20101012 Edward Wright Translation of Napiers work on logarithms Archived from the original on 3 December 2002 retrieved 20101012"}, {"topic": "Charged particles", "content": "In physics a charged particle is a particle with an electric charge. It may be an ion such as a molecule or atom with a surplus or deficit of electrons relative to protons. It can be the electrons and protons themselves as well as other elementary particles like positrons. It may also be an atomic nucleus devoid of electrons such as an alpha particle a helium nucleus. Neutrons have no charge so they are not charged particles unless they are part of a positively charged nucleus. Plasmas are a collection of charged particles atomic nuclei and separated electrons but can also be a gas containing a significant proportion of charged particles. Plasma is called the fourth state of matter because its properties are quite different from solids liquids and gases.  1"}, {"topic": "Charles's law", "content": "Charles law also known as the law of volumes is an experimental gas law that describes how gases tend to expand when heated. A modern statement of Charles law is When the pressure on a sample of a dry gas is held constant the Kelvin temperature and the volume will be directly related. this directly proportional relationship can be written as V T   Charles law simulation from Davidson College Davidson North Carolina Charles law demonstration by Prof. Robert Burk Carleton University Ottawa Canada Charles law animation from the Leonardo Project GTEPCCHS UK"}, {"topic": "Chemical Physics", "content": "Chemical physics is a subdiscipline of chemistry and physics that investigates physicochemical phenomena using techniques from atomic and molecular physics and condensed matter physics it is the branch of physics that studies chemical processes from the point of view of physics. While at the interface of physics and chemistry chemical physics is distinct from physical chemistry in that it focuses more on the characteristic elements and theories of physics. Meanwhile physical chemistry studies the physical nature of chemistry. Nonetheless the distinction between the two fields is vague and workers often practice in both fields during the course of their research. The United States Department of Education defines chemical physics as A program that focuses on the scientific study of structural phenomena combining the disciplines of physical chemistry and atomicmolecular physics. Includes instruction in heterogeneous structures alignment and surface phenomena quantum theory mathematical physics statistical and classical mechanics chemical kinetics and laser physics. "}, {"topic": "Chemical element", "content": "A chemical element or element is a species of atoms having the same number of protons in their atomic nuclei i.e. the same atomic number Z. There are 118 elements that have been identified of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radioactive isotopes which decay over time into other elements. Iron is the most abundant element by mass making up Earth while oxygen is the most common element in the crust of Earth. Chemical elements constitute all of the ordinary matter of the universe. However astronomical observations suggest that ordinary observable matter is only approximately 15 of the matter in the universe the remainder is dark matter the composition of which is unknown but it is not composed of chemical elements. The two lightest elements hydrogen and helium were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements lithium beryllium and boron were formed mostly by cosmic ray spallation and are thus more rare than those that follow. Formation of elements with from six to twenty six protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen silicon and iron on Earth reflects their common production in such stars. Elements with greater than twentysix protons are formed by supernova nucleosynthesis in supernovae which when they explode blast these elements far into space as supernova remnants where they may become incorporated into planets when they are formed. The term element is used for a kind of atoms with a given number of protons regardless of whether they are or they are not ionized or chemically bonded e.g. hydrogen in water as well as for a pure chemical substance consisting of a single element e.g. hydrogen gas. For the second meaning the terms elementary substance and simple substance have been suggested but they have not gained much acceptance in the Englishlanguage chemical literature whereas in some other languages their equivalent is widely used e.g. French corps simple Russian . One element can form multiple substances different by their structure they are called allotropes of the element. When different elements are chemically combined with the atoms held together by chemical bonds they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such native elements are copper silver gold carbon as coal graphite or diamonds and sulfur. All but a few of the most inert elements such as noble gases and noble metals are usually found on Earth in chemically combined form as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms most of these occur as mixtures. For example atmospheric air is primarily a mixture of nitrogen oxygen and argon and native solid elements occur in alloys such as that of iron and nickel. The history of the discovery and use of the elements began with primitive human societies that found native elements like carbon sulfur copper and gold. Later civilizations extracted elemental copper tin lead and iron from their ores by smelting using charcoal. Alchemists and chemists subsequently identified many more with almost all of the naturallyoccurring elements becoming known by 1900. The properties of the chemical elements are summarized on the periodic table which organizes the elements by increasing atomic number into rows periods in which the columns groups share recurring periodic physical and chemical properties. Save for unstable radioactive elements with short halflives all of the elements are available industrially most of them in high degrees of purity.  Videos for each element by the University of Nottingham"}, {"topic": "Chemical physics", "content": "Chemical physics is a subdiscipline of chemistry and physics that investigates physicochemical phenomena using techniques from atomic and molecular physics and condensed matter physics it is the branch of physics that studies chemical processes from the point of view of physics. While at the interface of physics and chemistry chemical physics is distinct from physical chemistry in that it focuses more on the characteristic elements and theories of physics. Meanwhile physical chemistry studies the physical nature of chemistry. Nonetheless the distinction between the two fields is vague and workers often practice in both fields during the course of their research. The United States Department of Education defines chemical physics as A program that focuses on the scientific study of structural phenomena combining the disciplines of physical chemistry and atomicmolecular physics. Includes instruction in heterogeneous structures alignment and surface phenomena quantum theory mathematical physics statistical and classical mechanics chemical kinetics and laser physics. "}, {"topic": "Circle", "content": "A circle is a simple closed shape in Euclidean geometry. It is the set of all points in a plane that are at a given distance from a given point the centre equivalently it is the curve traced out by a point that moves so that its distance from a given point is constant. The distance between any of the points and the centre is called the radius. A circle is a simple closed curve which divides the plane into two regions an interior and an exterior. In everyday use the term circle may be used interchangeably to refer to either the boundary of the figure or to the whole figure including its interior in strict technical usage the circle is only the boundary and the whole figure is called a disk. A circle may also be defined as a special ellipse in which the two foci are coincident and the eccentricity is 0 or the twodimensional shape enclosing the most area per unit perimeter squared using calculus of variations. A circle is a plane figure bounded by one line and such that all right lines drawn from a certain point within it to the bounding line are equal. The bounding line is called its circumference and the point its centre.  Hazewinkel Michiel ed. 2001 Circle Encyclopedia of Mathematics Springer ISBN 9781556080104 Circle PlanetMath.org website Weisstein Eric W. Circle MathWorld. Interactive Java applets for the properties of and elementary constructions involving circles. Interactive Standard Form Equation of Circle Click and drag points to see standard form equation in action Munching on Circles at cuttheknot"}, {"topic": "Circular motion", "content": "In physics circular motion is a movement of an object along the circumference of a circle or rotation along a circular path. It can be uniform with constant angular rate of rotation and constant speed or nonuniform with a changing rate of rotation. The rotation around a fixed axis of a threedimensional body involves circular motion of its parts. The equations of motion describe the movement of the center of mass of a body. Examples of circular motion include an artificial satellite orbiting the Earth at constant height a stone which is tied to a rope and is being swung in circles a car turning through a curve in a race track an electron moving perpendicular to a uniform magnetic field and a gear turning inside a mechanism. Since the objects velocity vector is constantly changing direction the moving object is undergoing acceleration by a centripetal force in the direction of the center of rotation. Without this acceleration the object would move in a straight line according to Newtons laws of motion.  Physclips Mechanics with animations and video clips from the University of New South Wales Circular Motion a chapter from an online textbook Circular Motion Lecture a video lecture on CM"}, {"topic": "Classical mechanics", "content": "In physics classical mechanics is one of the two major subfields of mechanics along with quantum mechanics. Classical mechanics is concerned with the set of physical laws describing the motion of bodies under the influence of a system of forces. The study of the motion of bodies is an ancient one making classical mechanics one of the oldest and largest subjects in science engineering and technology. It is also widely known as Newtonian mechanics. Classical mechanics describes the motion of macroscopic objects from projectiles to parts of machinery as well as astronomical objects such as spacecraft planets stars and galaxies. Within classical mechanics are fields of study that describe the behavior of solids liquids and gases and other specific subtopics. Classical mechanics also provides extremely accurate results as long as the domain of study is restricted to large objects and the speeds involved do not approach the speed of light. When the objects being examined are sufficiently small it becomes necessary to introduce the other major subfield of mechanics quantum mechanics which adjusts the laws of physics of macroscopic objects for the atomic nature of matter by including the waveparticle duality of atoms and molecules. When both quantum mechanics and classical mechanics cannot apply such as at the quantum level with high speeds quantum field theory QFT becomes applicable. The term classical mechanics was coined in the early 20th century to describe the system of physics begun by Isaac Newton and many contemporary 17th century natural philosophers and is built upon the earlier astronomical theories of Johannes Kepler which in turn were based on the precise observations of Tycho Brahe and the studies of terrestrial projectile motion of Galileo. Since these aspects of physics were developed long before the emergence of quantum physics and relativity some sources exclude Einsteins theory of relativity from this category. However a number of modern sources do include relativistic mechanics which in their view represents classical mechanics in its most developed and most accurate form. The earliest development of classical mechanics is often referred to as Newtonian mechanics and is associated with the physical concepts employed by and the mathematical methods invented by Newton Leibniz and others. Later more abstract and general methods were developed leading to reformulations of classical mechanics known as Lagrangian mechanics and Hamiltonian mechanics. These advances were largely made in the 18th and 19th centuries and they extend substantially beyond Newtons work particularly through their use of analytical mechanics.  Crowell Benjamin. Newtonian Physics an introductory text uses algebra with optional sections involving calculus Fitzpatrick Richard. Classical Mechanics uses calculus Hoiland Paul 2004. Preferred Frames of Reference  Relativity Horbatsch Marko Classical Mechanics Course Notes. Rosu Haret C. Classical Mechanics. Physics Education. 1999. arxiv.org  physics9909035 Shapiro Joel A. 2003. Classical Mechanics Sussman Gerald Jay  Wisdom Jack  MayerMeinhard E. 2001. Structure and Interpretation of Classical Mechanics Tong David. Classical Dynamics Cambridge lecture notes on Lagrangian and Hamiltonian formalism Kinematic Models for Design Digital Library KMODDL Movies and photos of hundreds of working mechanicalsystems models at Cornell University. Also includes an ebook library of classic texts on mechanical design and engineering. MIT OpenCourseWare 8.01 Classical Mechanics Free videos of actual course lectures with links to lecture notes assignments and exams. Alejandro A. Torassa On Classical Mechanics"}, {"topic": "Cloud physics", "content": "Cloud physics is the study of the physical processes that lead to the formation growth and precipitation of atmospheric clouds. Clouds consist of microscopic droplets of liquid water warm clouds tiny crystals of ice cold clouds or both mixed phase clouds. Cloud droplets initially form by the condensation of water vapor onto condensation nuclei when the supersaturation of air exceeds a critical value according to Khler theory. Cloud condensation nuclei are necessary for cloud droplets formation because of the Kelvin effect which describes the change in saturation vapor pressure due to a curved surface. At small radii the amount of supersaturation needed for condensation to occur is so large that it does not happen naturally. Raoults Law describes how the vapor pressure is dependent on the amount of solute in a solution. At high concentrations when the cloud droplets are small the supersaturation required is smaller than without the presence of a nucleus. In warm clouds larger cloud droplets fall at a higher terminal velocity because the drag force on smaller droplets is larger than on large droplets. The large droplets can then collide with small droplets and combine to form even larger drops. When the drops become large enough so that the acceleration due to gravity is much larger than the acceleration due to drag the drops can fall to the earth as precipitation. The collision and coalescence is not as important in mixed phase clouds where the Bergeron process dominates. Other important processes that form precipitation are riming when a supercooled liquid drop collides with a solid snowflake and aggregation when two solid snowflakes collide and combine. The precise mechanics of how a cloud forms and grows is not completely understood but scientists have developed theories explaining the structure of clouds by studying the microphysics of individual droplets. Advances in weather radar and satellite technology have also allowed the precise study of clouds on a large scale. "}, {"topic": "Coherence (physics)", "content": "In physics two wave sources are perfectly coherent if they have a constant phase difference and the same frequency. It is an ideal property of waves that enables stationary i.e. temporally and spatially constant interference. It contains several distinct concepts which are limiting cases that never quite occur in reality but allow an understanding of the physics of waves and has become a very important concept in quantum physics. More generally coherence describes all properties of the correlation between physical quantities of a single wave or between several waves or wave packets. Interference is nothing more than the addition in the mathematical sense of wave functions. A single wave can interfere with itself but this is still an addition of two waves see Youngs slits experiment. Constructive or destructive interferences are limit cases and two waves always interfere even if the result of the addition is complicated or not remarkable. When interfering two waves can add together to create a wave of greater amplitude than either one constructive interference or subtract from each other to create a wave of lesser amplitude than either one destructive interference depending on their relative phase. Two waves are said to be coherent if they have a constant relative phase. The amount of coherence can readily be measured by the interference visibility which looks at the size of the interference fringes relative to the input waves as the phase offset is varied a precise mathematical definition of the degree of coherence is given by means of correlation functions. Spatial coherence describes the correlation or predictable relationship between waves at different points in space either lateral or longitudinal. Temporal coherence describes the correlation between waves observed at different moments in time. Both are observed in the MichelsonMorley experiment and Youngs interference experiment. Once the fringes are obtained in the MichelsonMorley experiment when one of the mirrors is moved away gradually the time for the beam to travel increases and the fringes become dull and finally are lost showing temporal coherence. Similarly if in Youngs double slit experiment the space between the two slits is increased the coherence dies gradually and finally the fringes disappear showing spatial coherence.  Dr. SkySkull. Optics basics Coherence. Skulls in the Stars."}, {"topic": "Cold fusion", "content": "Cold fusion is a hypothesized type of nuclear reaction that would occur at or near room temperature. This is compared with the hot fusion which takes place naturally within stars under immense pressure and at temperatures of millions of degrees and distinguished from muoncatalyzed fusion. There is currently no accepted theoretical model which would allow cold fusion to occur. In 1989 Martin Fleischmann then one of the worlds leading electrochemists and Stanley Pons reported that their apparatus had produced anomalous heat excess heat of a magnitude they asserted would defy explanation except in terms of nuclear processes. They further reported measuring small amounts of nuclear reaction byproducts including neutrons and tritium. The small tabletop experiment involved electrolysis of heavy water on the surface of a palladium Pd electrode. The reported results received wide media attention and raised hopes of a cheap and abundant source of energy. Many scientists tried to replicate the experiment with the few details available. Hopes faded due to the large number of negative replications the withdrawal of many reported positive replications the discovery of flaws and sources of experimental error in the original experiment and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989 most scientists considered cold fusion claims dead and cold fusion subsequently gained a reputation as pathological science. In 1989 the United States Department of Energy DOE concluded that the reported results of excess heat did not present convincing evidence of a useful source of energy and decided against allocating funding specifically for cold fusion. A second DOE review in 2004 which looked at new research reached similar conclusions and did not result in DOE funding of cold fusion. A small community of researchers continues to investigate cold fusion now often preferring the designation lowenergy nuclear reactions LENR or condensed matter nuclear science CMNS. Since cold fusion articles are rarely published in peerreviewed mainstream scientific journals they do not attract the level of scrutiny expected for mainstream scientific publications.  Cold fusion at DMOZ International Society for Condensed Matter Nuclear Science iscmns.org organizes the ICCF conferences and publishes the Journal of Condensed Matter Nuclear Science. See library.htm of published papers and proceedings. Low Energy Nuclear Reactions LENR Phenomena and Potential Applications Naval Surface Warfare Center report NSWCDDPN150040 by Louis F. DeChiaro Ph.D. September 23 2015 1 Current Science 25 Feb 2015 issue devoted to LENR contains 34 papers mostly review articles."}, {"topic": "Communication physics", "content": "Communication physics is one of the applied branches of physics. It deals with various kinds of communication systems  Electronic communication Optical communication Computer communication Telephone Telegraph Radio Television Mobile phone communication Nano scale network"}, {"topic": "Complex harmonic motion", "content": "In physics complex harmonic motion is a complicated realm based on the simple harmonic motion. The word complex refers to different situations. Unlike simple harmonic motion which is regardless of air resistance friction etc. complex harmonic motion often has additional forces to dissipate the initial energy and lessen the speed and amplitude of an oscillation until the energy of the system is totally drained and become rest at its equilibrium point. "}, {"topic": "Compton scattering", "content": "Compton scattering discovered by Arthur Holly Compton is the inelastic scattering of a photon by a charged particle usually an electron. It results in a decrease in energy increase in wavelength of the photon which may be an X ray or gamma ray photon called the Compton effect. Part of the energy of the photon is transferred to the recoiling electron. Inverse Compton scattering exists in which a charged particle transfers part of its energy to a photon.  Compton Scattering  Georgia State University Compton Scattering Data  Georgia State University Derivation of Compton shift equation"}, {"topic": "Computational physics", "content": "Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically computational physics was the first application of modern computers in science and is now a subset of computational science. It is sometimes regarded as a subdiscipline or offshoot of theoretical physics but others consider it an intermediate branch between theoretical and experimental physics a third way that supplements theory and experiment.  C20 IUPAP Commission on Computational Physics Applied Physical Society Division of Computational Physics Institute of Physics Computational Physics Group SciDAC Scientific Discovery through Advanced Computing Open Source Physics SCINET Scientific Software Framework"}, {"topic": "Concave lens", "content": "A lens is a transmissive optical device that affects the focus of a light beam through refraction. A simple lens consists of a single piece of material while a compound lens consists of several simple lenses elements usually along a common axis. Lenses are made from transparent materials such as glass ground and polished to a desired shape. A lens can focus light to form an image unlike a prism which refracts light without focusing. Devices that similarly refract radiation other than visible light are also called lenses such as microwave lenses or acoustic lenses.  Learning by Simulations Concave and Convex Lenses OpticalRayTracer Open source lens simulator downloadable java Video with a simulation of light while it passes a convex lens Video on YouTube Animations demonstrating lens by QED"}, {"topic": "Condensed matter physics", "content": "Condensed matter physics is a branch of physics that deals with the physical properties of condensed phases of matter. Condensed matter physicists seek to understand the behavior of these phases by using physical laws. In particular they include the laws of quantum mechanics electromagnetism and statistical mechanics. The most familiar condensed phases are solids and liquids while more exotic condensed phases include the superconducting phase exhibited by certain materials at low temperature the ferromagnetic and antiferromagnetic phases of spins on atomic lattices and the BoseEinstein condensate found in cold atomic systems. The study of condensed matter physics involves measuring various material properties via experimental probes along with using techniques of theoretical physics to develop mathematical models that help in understanding physical behavior. The diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics one third of all American physicists identify themselves as condensed matter physicists and the Division of Condensed Matter Physics is the largest division at the American Physical Society. The field overlaps with chemistry materials science and nanotechnology and relates closely to atomic physics and biophysics. Theoretical condensed matter physics shares important concepts and techniques with theoretical particle and nuclear physics. A variety of topics in physics such as crystallography metallurgy elasticity magnetism etc. were treated as distinct areas until the 1940s when they were grouped together as solid state physics. Around the 1960s the study of physical properties of liquids was added to this list forming the basis for the new related specialty of condensed matter physics. According to physicist Philip Warren Anderson the term was coined by him and Volker Heine when they changed the name of their group at the Cavendish Laboratories Cambridge from Solid state theory to Theory of Condensed Matter in 1967 as they felt it did not exclude their interests in the study of liquids nuclear matter and so on. Although Anderson and Heine helped popularize the name condensed matter it had been present in Europe for some years most prominently in the form of a journal published in English French and German by SpringerVerlag titled Physics of Condensed Matter which was launched in 1963. The funding environment and Cold War politics of the 1960s and 1970s were also factors that lead some physicists to prefer the name condensed matter physics which emphasized the commonality of scientific problems encountered by physicists working on solids liquids plasmas and other complex matter over solid state physics which was often associated with the industrial applications of metals and semiconductors. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics. References to condensed state can be traced to earlier sources. For example in the introduction to his 1947 book Kinetic Theory of Liquids Yakov Frenkel proposed that The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact it would be more correct to unify them under the title of condensed bodies.  Mudry Christopher 2014. Lecture Notes on Field Theory in Condensed Matter Physics. World Scientific. ISBN 9789814449106. Khan Abdul Qadeer 21 November 1998. Dimensional Anistrophy in Condensed Matter Physics PDF. Seven National Symposium on Frontiers in Physics. 7 7 7. Retrieved 21 October 2012. P. M. Chaikin and T. C. Lubensky 2000. Principles of Condensed Matter Physics Cambridge University Press 1st edition ISBN 0521794501 Alexander Altland and Ben Simons 2006. Condensed Matter Field Theory Cambridge University Press ISBN 0521845084 Michael P. Marder 2010. Condensed Matter Physics second edition John Wiley and Sons ISBN 0470617985 Lillian Hoddeson Ernest Braun Jrgen Teichmann and Spencer Weart eds. 1992. Out of the Crystal Maze Chapters from the History of Solid State Physics Oxford University Press ISBN 019505329X"}, {"topic": "Constructive interference", "content": "In physics interference is a phenomenon in which two waves superpose to form a resultant wave of greater lower or the same amplitude. Interference usually refers to the interaction of waves that are correlated or coherent with each other either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves for example light radio acoustic surface water waves or matter waves.  Easy JavaScript Simulation Model of One Dimensional Wave Interference Expressions of position and fringe spacing Java demonstration of interference Java simulation of interference of water waves 1 Java simulation of interference of water waves 2 Flash animations demonstrating interference Lissajous Curves Interactive simulation of graphical representations of musical intervals beats interference vibrating strings Animations demonstrating optical interference by QED"}, {"topic": "Continuous spectrum", "content": "In physics a continuous spectrum usually means a set of attainable values for some physical quantity such as energy or wavelength that is best described as an interval of real numbers. It is opposed to discrete spectrum a set of attainable values that is discrete in the mathematical sense where there is a positive gap between each value and the next one. The classical example of a continuous spectrum from which the name is derived is the part of the spectrum of the light emitted by excited atoms of hydrogen that is due to free electrons becoming bound to a hydrogen ion and emitting photons which are smoothly spread over a wide range of wavelengths in contrast to the discrete lines due to electrons falling from some bound quantum state to a state of lower energy. As in that classical example the term is most often used when the range of values of a physical quantity may have both a continuous and a discrete part whether at the same time or in different situations. In quantum systems continuous spectra as in bremsstrahlung and thermal radiation are usually associated with free particles such as atoms in a gas electrons in an electron beam or conduction band electrons in a metal. In particular the position and momentum of a free particle have a continuous spectrum but when the particle is confined to a limited space its spectrum becomes discrete. Often a continuous spectrum may be just a convenient model for a discrete spectrum whose values are too close to be distinguished as in the phonons in a crystal. The continuous and discrete spectra of physical systems can be modeled in functional analysis as different parts in the decomposition of the spectrum of a linear operator acting on a function space such as the Hamiltonian operator. "}, {"topic": "Convex lens", "content": "A lens is a transmissive optical device that affects the focus of a light beam through refraction. A simple lens consists of a single piece of material while a compound lens consists of several simple lenses elements usually along a common axis. Lenses are made from transparent materials such as glass ground and polished to a desired shape. A lens can focus light to form an image unlike a prism which refracts light without focusing. Devices that similarly refract radiation other than visible light are also called lenses such as microwave lenses or acoustic lenses.  Learning by Simulations Concave and Convex Lenses OpticalRayTracer Open source lens simulator downloadable java Video with a simulation of light while it passes a convex lens Video on YouTube Animations demonstrating lens by QED"}, {"topic": "Cosmic background radiation", "content": "Cosmic background radiation is electromagnetic radiation from the sky with no discernible source. The origin of this radiation depends on the region of the spectrum that is observed. One component is the cosmic microwave background radiation. This component is redshifted photons that have freely streamed from an epoch when the Universe became transparent for the first time to radiation. Its discovery and detailed observations of its properties are considered one of the major confirmations of the Big Bang. The discovery by chance in 1965 of the cosmic background radiation suggests that the early universe was dominated by a radiation field a field of extremely high temperature and pressure. The SunyaevZeldovich effect shows the phenomena of radiant cosmic background radiation interacting with electron clouds distorting the spectrum of the radiation. There is also background radiation in the infrared xrays etc. with different causes and they can sometimes be resolved into an individual source. See cosmic infrared background and Xray background. See also cosmic neutrino background and extragalactic background light.  The Diffuse Xray and Gammaray Background  Deep Fields"}, {"topic": "Cosmic rays", "content": "Cosmic rays are immensely highenergy radiation mainly originating outside the Solar System. They may produce showers of secondary particles that penetrate and impact the Earths atmosphere and sometimes even reach the surface. Composed primarily of highenergy protons and atomic nuclei they are of mysterious origin. Data from the Fermi space telescope 2013 have been interpreted as evidence that a significant fraction of primary cosmic rays originate from the supernovae of massive stars. However this is not thought to be their only source. Active galactic nuclei probably also produce cosmic rays.  Aspera European network portal Animation about cosmic rays on astroparticle.org Helmholtz Alliance for Astroparticle Physics Particle Data Group review of Cosmic Rays by C. Amsler et al. Physics Letters B667 1 2008. Introduction to Cosmic Ray Showers by Konrad Bernlhr. BBC news Cosmic rays find uranium 2003. BBC news Rays to nab nuclear smugglers 2005. BBC news Physicists probe ancient pyramid using cosmic rays 2004. Shielding Space Travelers by Eugene Parker. Anomalous cosmic ray hydrogen spectra from Voyager 1 and 2 Anomalous Cosmic Rays From NASAs Cosmicopia Review of Cosmic Rays Whos Afraid of a Solar Flare Solar activity can be surprisingly good for astronauts. 7 October 2005 at ScienceNASA video of Muon detector in use at Smithsonian Air and Space Museum Dr. Lothar Frey Cosmic rays and electronic devices YouTube Video SpaceUp Stuttgart 2012 ARMAS Realtime cosmicray radiation measurements at aviation altitudes. Padilla Antonio Tony. Where do Cosmic Rays come from. Sixty Symbols. Brady Haran for the University of Nottingham."}, {"topic": "Coulomb", "content": "The coulomb unit symbol C is the International System of Units SI unit of electric charge. It is the charge symbol Q or q transported by a constant current of one ampere in one second 1 C  1 A 1 s  It is equivalent to the charge of approximately 70186242000000000006.2421018 69951036000000000001.036105 mol protons and 1 C is equivalent to the charge of approximately 70186242000000000006.2421018 electrons. "}, {"topic": "Coulomb's law", "content": "Coulombs law or Coulombs inversesquare law is a law of physics that describes force interacting between static electrically charged particles. In its scalar form the law is F  k e q 1 q 2 r 2  Dividing 4 by 5 we get Measuring the angles 1 and 2 and the distance between the charges L1 and L2 is sufficient to verify that the equality is true taking into account the experimental error. In practice angles can be difficult to measure so if the length of the ropes is sufficiently great the angles will be small enough to make the following approximation Using this approximation the relationship 6 becomes the much simpler expression In this way the verification is limited to measuring the distance between the charges and check that the division approximates the theoretical value.  Coulombs Law on Project PHYSNET Electricity and the Atoma chapter from an online textbook A maze game for teaching Coulombs Lawa game created by the Molecular Workbench software Electric Charges Polarization Electric Force Coulombs Law Walter Lewin 8.02 Electricity and Magnetism Spring 2002 Lecture 1 video. MIT OpenCourseWare. License Creative Commons AttributionNoncommercialShare Alike."}, {"topic": "Covalent bond", "content": "A covalent bond also called a molecular bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are known as shared pairs or bonding pairs and the stable balance of attractive and repulsive forces between atoms when they share electrons is known as covalent bonding. For many molecules the sharing of electrons allows each atom to attain the equivalent of a full outer shell corresponding to a stable electronic configuration. Covalent bonding includes many kinds of interactions including bonding bonding metaltometal bonding agostic interactions bent bonds and threecenter twoelectron bonds. The term covalent bond dates from 1939. The prefix co means jointly associated in action partnered to a lesser degree etc. thus a covalent bond in essence means that the atoms share valence such as is discussed in valence bond theory. In the molecule H 2 the hydrogen atoms share the two electrons via covalent bonding. Covalency is greatest between atoms of similar electronegativities. Thus covalent bonding does not necessarily require that the two atoms be of the same elements only that they be of comparable electronegativity. Covalent bonding that entails sharing of electrons over more than two atoms is said to be delocalized.  Covalent Bonds and Molecular Structure Structure and Bonding in ChemistryCovalent Bonds"}, {"topic": "Covalent compound", "content": "A covalent bond also called a molecular bond is a chemical bond that involves the sharing of electron pairs between atoms. These electron pairs are known as shared pairs or bonding pairs and the stable balance of attractive and repulsive forces between atoms when they share electrons is known as covalent bonding. For many molecules the sharing of electrons allows each atom to attain the equivalent of a full outer shell corresponding to a stable electronic configuration. Covalent bonding includes many kinds of interactions including bonding bonding metaltometal bonding agostic interactions bent bonds and threecenter twoelectron bonds. The term covalent bond dates from 1939. The prefix co means jointly associated in action partnered to a lesser degree etc. thus a covalent bond in essence means that the atoms share valence such as is discussed in valence bond theory. In the molecule H 2 the hydrogen atoms share the two electrons via covalent bonding. Covalency is greatest between atoms of similar electronegativities. Thus covalent bonding does not necessarily require that the two atoms be of the same elements only that they be of comparable electronegativity. Covalent bonding that entails sharing of electrons over more than two atoms is said to be delocalized.  Covalent Bonds and Molecular Structure Structure and Bonding in ChemistryCovalent Bonds"}, {"topic": "Crest (physics)", "content": "A crest is the point on a wave with the maximum value or upward displacement within a cycle. A crest is a point on the wave where the displacement of the medium is at a maximum. A trough is the opposite of a crest so the minimum or lowest point in a cycle. A point on the wave is a trough if the displacement of the medium at that point is at a minimum  Kinsman Blair 1984 Wind Waves Their Generation and Propagation on the Ocean Surface Dover Publications ISBN 0486495116  704 pages."}, {"topic": "Crest factor", "content": "Crest factor is a measure of a waveform such as alternating current or sound showing the ratio of peak values to the effective value. In other words crest factor indicates how extreme the peaks are in a waveform. Crest factor 1 indicates no peaks such as direct current. Higher crest factors indicate peaks for example sound waves tend to have high crest factors. Crest factor is the peak amplitude of the waveform divided by the RMS value of the waveform C   x  p e a k x r m s .  When expressed in decibels crest factor and PAPR are equivalent due to the way decibels are calculated for power ratios vs amplitude ratios. Crest factor and PAPR are therefore dimensionless quantities. While the crest factor is defined as a positive real number in commercial products it is also commonly stated as the ratio of two whole numbers e.g. 21. The PAPR is most used in signal processing applications. As it is a power ratio it is normally expressed in decibels dB. The crest factor of the test signal is a fairly important issue in loudspeaker testing standards in this context it is usually expressed in dB. The minimum possible crest factor is 1 11 or 0 dB.  Definition of peaktoaverage ratio ATIS Alliance for Telecommunications Industry Solutions Telecom Glossary 2K Definition of crest factor ATIS Alliance for Telecommunications Industry Solutions Telecom Glossary 2K Peaktoaverage power ratio PAPR of OFDM systems  tutorial"}, {"topic": "Critical angle (optics)", "content": "Total internal reflection is a phenomenon which occurs when a propagating wave strikes a medium boundary at an angle larger than a particular critical angle with respect to the normal to the surface. If the refractive index is lower on the other side of the boundary and the incident angle is greater than the critical angle the wave cannot pass through and is entirely reflected. The critical angle is the angle of incidence above which the total internal reflection occurs. This is particularly common as an optical phenomenon where light waves are involved but it occurs with many types of waves such as electromagnetic waves in general or sound waves. When a wave reaches a boundary between different materials with different refractive indices the wave will in general be partially refracted at the boundary surface and partially reflected. However if the angle of incidence is greater i.e. the direction of propagation is closer to being parallel to the boundary than the critical angle the angle of incidence at which light is refracted such that it travels along the boundary then the wave will not cross the boundary but will instead be totally reflected back internally. This can only occur when the wave in a medium with a higher refractive index n1 reaches a boundary with a medium of lower refractive index n2. For example it will occur with light reaching air from glass but not when reaching glass from air.  FTIR Touch Sensing MultiTouch Interaction Research Georgia State University Total Internal Reflection by Michael Schreiber Wolfram Demonstrations Project Total Internal Reflection St. Marys Physics Online Notes Bowley Roger 2009. Total Internal Reflection. Sixty Symbols. Brady Haran for the University of Nottingham. TETM Reflection Coefficients interactive phase and magnitude plots"}, {"topic": "Current density", "content": "In electromagnetism current density is the electric current per unit area of cross section. It is defined as a vector whose magnitude is the electric current per crosssectional area at a given point in space i.e. it is a vector field. In SI units the electric current density is measured in amperes per square metre. "}, {"topic": "Curvilinear motion", "content": "The motion of an object moving in a curved path is called curvilinear motion. ExampleA stone thrown into the air at an angle. Curvilinear motion describes the motion of a moving particle that conforms to a known or fixed curve. The study of such motion involves the use of two such coordinate systems with the first being planar motion and the latter being cylindrical motion. "}, {"topic": "Cyclotron", "content": "A cyclotron is a type of particle accelerator invented by Ernest O. Lawrence in 1932 in which charged particles accelerate outwards from the centre along a spiral path. The particles are held to a spiral trajectory by a static magnetic field and accelerated by a rapidly varying radio frequency electric field. Lawrence was awarded the 1939 Nobel prize in physics for this invention. Cyclotrons were the most powerful particle accelerator technology until the 1950s when they were superseded by the synchrotron and are still used to produce particle beams in physics and nuclear medicine. The largest singlemagnet cyclotron was the 4.67 m 184 in synchrocyclotron built between 1940 and 1946 by Lawrence at the University of California at Berkeley which could accelerate protons to 730 MeV. The largest cyclotron is the 17.1 m 56 ft multimagnet TRIUMF accelerator at the University of British Columbia in Vancouver British Columbia which can produce 500 MeV protons. There are over 1200 cyclotrons used in nuclear medicine worldwide for the production of radionuclides.  The 88Inch Cyclotron at Lawrence Berkeley National Laboratory The first Cyclotron in Amsterdam Netherlands 1964 at the site of the Free University National Superconducting Cyclotron Laboratory of the Michigan State UniversityHome of coupled K500 and K1200 cyclotrons the K500 the first superconducting cyclotron and the K1200 formerly the most powerful in the world. Rutgers CyclotronStudents at Rutgers University built a 30 cm 12 in 1 MeV cyclotron as an undergraduate project which is now used for a seniorlevel undergraduate and a graduate lab course. RIKEN Nishina Center for Acceleratorbased ScienceHome of the most powerful cyclotron in the world."}, {"topic": "DC motor", "content": "A DC motor is any of a class of electrical machines that converts direct current electrical power into mechanical power. The most common types rely on the forces produced by magnetic fields. Nearly all types of DC motors have some internal mechanism either electromechanical or electronic to periodically change the direction of current flow in part of the motor. Most types produce rotary motion a linear motor directly produces force and motion in a straight line. DC motors were the first type widely used since they could be powered from existing directcurrent lighting power distribution systems. A DC motors speed can be controlled over a wide range using either a variable supply voltage or by changing the strength of current in its field windings. Small DC motors are used in tools toys and appliances. The universal motor can operate on direct current but is a lightweight motor used for portable power tools and appliances. Larger DC motors are used in propulsion of electric vehicles elevator and hoists or in drives for steel rolling mills. The advent of power electronics has made replacement of DC motors with AC motors possible in many applications. "}, {"topic": "Dalton's law", "content": "In chemistry and physics Daltons law also called Daltons law of partial pressures states that in a mixture of nonreacting gases the total pressure exerted is equal to the sum of the partial pressures of the individual gases. This empirical law was observed by John Dalton in 1801 and is related to the ideal gas laws. "}, {"topic": "Damped vibration", "content": "Vibration is a mechanical phenomenon whereby oscillations occur about an equilibrium point. The word comes from Latin vibrationem shaking brandishing. The oscillations may be periodic such as the motion of a pendulumor random such as the movement of a tire on a gravel road. Vibration can be desirable for example the motion of a tuning fork the reed in a woodwind instrument or harmonica a mobile phone or the cone of a loudspeaker. In many cases however vibration is undesirable wasting energy and creating unwanted sound. For example the vibrational motions of engines electric motors or any mechanical device in operation are typically unwanted. Such vibrations could be caused by imbalances in the rotating parts uneven friction or the meshing of gear teeth. Careful designs usually minimize unwanted vibrations. The studies of sound and vibration are closely related. Sound or pressure waves are generated by vibrating structures e.g. vocal cords these pressure waves can also induce the vibration of structures e.g. ear drum. Hence attempts to reduce noise are often related to issues of vibration.  Normal vibration modes of a circular membrane Free Excel sheets to estimate modal parameters Vibration Testing"}, {"topic": "Damping", "content": "Damping is an influence within or upon an oscillatory system that has the effect of reducing restricting or preventing its oscillations. In physical systems damping is produced by processes that dissipate the energy stored in the oscillation. Examples include viscous drag in mechanical systems resistance in electronic oscillators and absorption and scattering of light in optical oscillators. Damping not based on energy loss can be important in other oscillating systems such as those that occur in biological systems. The damping of a system can be described as being one of the following Overdamped The system returns exponentially decays to equilibrium without oscillating. Critically damped The system returns to equilibrium as quickly as possible without oscillating. Underdamped The system oscillates at reduced frequency compared to the undamped case with the amplitude gradually decreasing to zero. Undamped The system oscillates at its natural resonant frequency o without experiencing decay of its amplitude. For example consider a door that uses a spring to close the door once open. This can lead to any of the above types of damping depending on the strength of the damping. If the door is undamped it will swing back and forth forever at a particular resonant frequency. If it is underdamped it will swing back and forth with decreasing size of the swing until it comes to a stop. If it is critically damped then it will return to closed as quickly as possible without oscillating. Finally if it is overdamped it will return to closed without oscillating but more slowly depending on how overdamped it is. Different levels of damping are desired for different types of systems.  Calculation of the matching attenuationthe damping factor and the damping of bridging Damping Matlab scripts"}, {"topic": "Darcy-Weisbach equation", "content": "In fluid dynamics the DarcyWeisbach equation is a phenomenological equation which relates the head loss or pressure loss due to friction along a given length of pipe to the average velocity of the fluid flow for an incompressible fluid. The equation is named after Henry Darcy and Julius Weisbach. The DarcyWeisbach equation contains a dimensionless friction factor known as the Darcy friction factor. This is also variously called the DarcyWeisbach friction factor friction factor resistance coefficient or flow coefficient.  The History of the DarcyWeisbach Equation DarcyWeisbach equation calculator Pipe pressure drop calculator for single phase flows. Pipe pressure drop calculator for two phase flows. Open source pipe pressure drop calculator. Web application with pressure drop calculations for pipes and ducts"}, {"topic": "Dark energy", "content": "In physical cosmology and astronomy dark energy is an unknown form of energy which is hypothesized to permeate all of space tending to accelerate the expansion of the universe. Dark energy is the most accepted hypothesis to explain the observations since the 1990s indicating that the universe is expanding at an accelerating rate. Assuming that the standard model of cosmology is correct the best current measurements indicate that dark energy contributes 69 of the total energy in the presentday observable universe. The massenergy of dark matter and ordinary baryonic matter contribute 26 and 5 respectively and other components such as neutrinos and photons contribute a very small amount. Again on a massenergy equivalence basis the density of dark energy  7 1030 gcm3 is very low much less than the density of ordinary matter or dark matter within galaxies. However it comes to dominate the massenergy of the universe because it is uniform across space. Two proposed forms for dark energy are the cosmological constant a constant energy density filling space homogeneously and scalar fields such as quintessence or moduli dynamic quantities whose energy density can vary in time and space. Contributions from scalar fields that are constant in space are usually also included in the cosmological constant. The cosmological constant can be formulated to be equivalent to vacuum energy. Scalar fields that do change in space can be difficult to distinguish from a cosmological constant because the change may be extremely slow. Highprecision measurements of the expansion of the universe are required to understand how the expansion rate changes over time and space. In general relativity the evolution of the expansion rate is parameterized by the cosmological equation of state the relationship between temperature pressure and combined matter energy and vacuum energy density for any region of space. Measuring the equation of state for dark energy is one of the biggest efforts in observational cosmology today. Adding the cosmological constant to cosmologys standard FLRW metric leads to the LambdaCDM model which has been referred to as the standard model of cosmology because of its precise agreement with observations. Dark energy has been used as a crucial ingredient in a recent attempt to formulate a cyclic model for the universe.  Dark Energy on In Our Time at the BBC. listen now Dark energy Eric Linder Scholarpedia 324900. doi10.4249scholarpedia.4900 Dark energy how the paradigm shifted Physicsworld.com Dennis Overbye November 2006. 9 BillionYearOld Dark Energy Reported. The New York Times. Mysterious forces long presence BBC News online 2006 More evidence for dark energy being the cosmological constant Astronomy Picture of the Day one of the images of the Cosmic Microwave Background which confirmed the presence of dark energy and dark matter SuperNova Legacy Survey home page The CanadaFranceHawaii Telescope Legacy Survey Supernova Program aims primarily at measuring the equation of state of Dark Energy. It is designed to precisely measure several hundred highredshift supernovae. Report of the Dark Energy Task Force HubbleSite.org Dark Energy Website Multimedia presentation explores the science of dark energy and Hubbles role in its discovery. Surveying the dark side Dark energy and 3manifold topology Acta Physica Polonica 38 2007 p. 36333639 The Dark Energy Survey The Joint Dark Energy Mission Harvard Dark Energy Found Stifling Growth in Universe primary source April 2010 Smithsonian Magazine Article HETDEX Dark energy experiment Dark Energy FAQ The Hunt for Dark Energy George FR Ellis Peter Cameron and David Tong discuss the presence of dark energy in the Universe Euclid ESA Satellite a mission to map the geometry of the dark universe"}, {"topic": "Dark matter", "content": "Dark matter is an undescribed type of matter comprising approximately 27 of the mass and energy in the observable universe that is not accounted for by dark energy baryonic matter ordinary matter and neutrinos. The name refers to the fact that it does not emit or interact with electromagnetic radiation such as light and is thus invisible to the entire electromagnetic spectrum. Although dark matter has not been directly observed its existence and properties are inferred from its gravitational effects such as the motions of visible matter gravitational lensing its influence on the universes largescale structure and its effects in the cosmic microwave background. Dark matter is transparent to electromagnetic radiation andor is so dense and small that it fails to absorb or emit enough radiation to be detectable with current imaging technology. Estimates of masses for galaxies and larger structures via dynamical and general relativistic means are much greater than those based on the mass of the visible luminous matter. The standard model of cosmology indicates that the total massenergy of the universe contains 4.9 ordinary matter 26.8 dark matter and 68.3 dark energy. Thus dark matter constitutes 84.5 of total mass while dark energy plus dark matter constitute 95.1 of total massenergy content. The great majority of ordinary matter in the universe is also unseen since visible stars and gas inside galaxies and clusters account for less than 10 of the ordinary matter contribution to the massenergy density of the universe. The dark matter hypothesis plays a central role in current modeling of cosmic structure formation and galaxy formation and evolution and on explanations of the anisotropies observed in the cosmic microwave background CMB. All these lines of evidence suggest that galaxies clusters of galaxies and the universe as a whole contain far more matter than that which is observable via electromagnetic signals. The most widely accepted hypothesis on the form for dark matter is that it is composed of weakly interacting massive particles WIMPs that interact only through gravity and the weak force. Although the existence of dark matter is generally accepted by most of the astronomical community a minority of astronomers argue for various modifications of the standard laws of general relativity such as MOND TeVeS and Conformal gravity that attempt to account for the observations without invoking additional matter. Many experiments to detect proposed dark matter particles through nongravitational means are under way.  Dark matter at DMOZ Dark matter Astronomy at Encyclopdia Britannica What is dark matter at cosmosmagazine.com The Dark Matter Crisis 18 August 2010 by Pavel Kroupa posted in General The European astroparticle physics network Helmholtz Alliance for Astroparticle Physics NASA Finds Direct Proof of Dark Matter Press release. NASA. 21 August 2006. Tuttle Kelen 22 August 2006. Dark Matter Observed. SLAC Stanford Linear Accelerator Center Today. Astronomers claim first dark galaxy find. New Scientist. 23 February 2005. Sample Ian 17 December 2009. Dark Matter Detected. London Guardian. Retrieved 1 May 2010. Video lecture on dark matter by Scott Tremaine IAS professor Science Daily story Astronomers Doubts About the Dark Side ... Gray Meghan Merrifield Mike Copeland Ed 2010. Dark Matter. Sixty Symbols. Brady Haran for the University of Nottingham. CS1 maint Multiple names authors list link"}, {"topic": "Decibel", "content": "The decibel dB is a logarithmic unit used to express the ratio of two values of a physical quantity often power or intensity. One of these values is often a standard reference value in which case the decibel is used to express the level of the other value relative to this reference. The number of decibels is ten times the logarithm to base 10 of the ratio of two power quantities or of the ratio of the squares of two field amplitude quantities. One decibel is one tenth of one bel named in honor of Alexander Graham Bell however the bel is seldom used. The definition of the decibel is based on the measurement of power in telephony of the early 20th century in the Bell System in the United States. Today the unit is used for a wide variety of measurements in science and engineering most prominently in acoustics electronics and control theory. In electronics the gains of amplifiers attenuation of signals and signaltonoise ratios are often expressed in decibels. The decibel confers a number of advantages such as the ability to conveniently represent very large or small numbers and the ability to carry out multiplication of ratios by simple addition and subtraction. By contrast use of the decibel complicates operations of addition and subtraction. A change in power by a factor of 10 corresponds to a 10 dB change in level. At the half power point an audio circuit or an antenna exhibits an attenuation of approximately 3 dB. A change in voltage by a factor of 10 results in a change in power by a factor of 100 which corresponds to a 20 dB change in level. A change in voltage ratio by a factor of 2 equivalently factor of 4 in power change approximately corresponds to a 6 dB change in level. The decibel symbol is often qualified with a suffix that indicates the reference quantity that has been used or some other property of the quantity being measured. For example dBm indicates a reference power of one milliwatt while dBu is referenced to 0.775 volts RMS. In the International System of Quantities the decibel is defined as a unit of measurement for quantities of type level or level difference which are defined as the logarithm of the ratio of power or fieldtype quantities.  What is a decibel With sound files and animations Conversion of sound level units dBSPL or dBA to sound pressure p and sound intensity J OSHA Regulations on Occupational Noise Exposure"}, {"topic": "Definite integral", "content": "In mathematics an integral assigns numbers to functions in a way that can describe displacement area volume and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus with its inverse differentiation being the other. Given a function f of a real variable x and an interval a b of the real line the definite integral a b f  x  d x   which has no singularities at all.  Keisler H. Jerome Elementary Calculus An Approach Using Infinitesimals University of Wisconsin Stroyan K.D. A Brief Introduction to Infinitesimal Calculus University of Iowa Mauch Sean Seans Applied Math Book CIT an online textbook that includes a complete introduction to calculus Crowell Benjamin Calculus Fullerton College an online textbook Garrett Paul Notes on FirstYear Calculus Hussain Faraz Understanding Calculus an online textbook Johnson William Woolsey 1909 Elementary Treatise on Integral Calculus link from HathiTrust. Kowalk W.P. Integration Theory University of Oldenburg. A new concept to an old problem. Online textbook Sloughter Dan Difference Equations to Differential Equations an introduction to calculus Numerical Methods of Integration at Holistic Numerical Methods Institute P.S. Wang Evaluation of Definite Integrals by Symbolic Manipulation 1972 a cookbook of definite integral techniques"}, {"topic": "Deflection (engineering)", "content": "In engineering deflection is the degree to which a structural element is displaced under a load. It may refer to an angle or a distance. The deflection distance of a member under a load is directly related to the slope of the deflected shape of the member under that load and can be calculated by integrating the function that mathematically describes the slope of the member under that load. Deflection can be calculated by standard formula will only give the deflection of common beam configurations and load cases at discrete locations or by methods such as virtual work direct integration Castiglianos method Macaulays method or the direct stiffness method amongst others. The deflection of beam elements is usually calculated on the basis of the EulerBernoulli beam equation while that of a plate or shell element is calculated using plate or shell theory. An example of the use of deflection in this context is in building construction. Architects and engineers select materials for various applications. The beams used for frame work are selected on the basis of deflection amongst other factors.  Deflection  stress of beams Calculators Deflection of beams Online Calculator for Deflection and slope of beams Beam Deflections Beam Deflections Tabulated"}, {"topic": "Deformation (engineering)", "content": "In materials science deformation refers to any changes in the shape or size of an object due to an applied force the deformation energy in this case is transferred through work or a change in temperature the deformation energy in this case is transferred through heat. The first case can be a result of tensile pulling forces compressive pushing forces shear bending or torsion twisting. In the second case the most significant factor which is determined by the temperature is the mobility of the structural defects such as grain boundaries point vacancies line and screw dislocations stacking faults and twins in both crystalline and noncrystalline solids. The movement or displacement of such mobile defects is thermally activated and thus limited by the rate of atomic diffusion. Deformation is often described as strain. As deformation occurs internal intermolecular forces arise that oppose the applied force. If the applied force is not too great these forces may be sufficient to completely resist the applied force and allow the object to assume a new equilibrium state and to return to its original state when the load is removed. A larger applied force may lead to a permanent deformation of the object or even to its structural failure. In the figure it can be seen that the compressive loading indicated by the arrow has caused deformation in the cylinder so that the original shape dashed lines has changed deformed into one with bulging sides. The sides bulge because the material although strong enough to not crack or otherwise fail is not strong enough to support the load without change thus the material is forced out laterally. Internal forces in this case at right angles to the deformation resist the applied load. The concept of a rigid body can be applied if the deformation is negligible.  Table of deformation mechanisms and processes"}, {"topic": "Deformation (mechanics)", "content": "Deformation in continuum mechanics is the transformation of a body from a reference configuration to a current configuration. A configuration is a set containing the positions of all particles of the body. A deformation may be caused by external loads body forces such as gravity or electromagnetic forces or changes in temperature moisture content or chemical reactions etc. Strain is a description of deformation in terms of relative displacement of particles in the body that excludes rigidbody motions. Different equivalent choices may be made for the expression of a strain field depending on whether it is defined with respect to the initial or the final configuration of the body and on whether the metric tensor or its dual is considered. In a continuous body a deformation field results from a stress field induced by applied forces or is due to changes in the temperature field inside the body. The relation between stresses and induced strains is expressed by constitutive equations e.g. Hookes law for linear elastic materials. Deformations which are recovered after the stress field has been removed are called elastic deformations. In this case the continuum completely recovers its original configuration. On the other hand irreversible deformations remain even after stresses have been removed. One type of irreversible deformation is plastic deformation which occurs in material bodies after stresses have attained a certain threshold value known as the elastic limit or yield stress and are the result of slip or dislocation mechanisms at the atomic level. Another type of irreversible deformation is viscous deformation which is the irreversible part of viscoelastic deformation. In the case of elastic deformations the response function linking strain to the deforming stress is the compliance tensor of the material.  Bazant Zdenek P. Cedolin Luigi 2010. ThreeDimensional Continuum Instabilities and Effects of Finite Strain Tensor chapter 11 in Stability of Structures 3rd ed. Singapore New Jersey London World Scientific Publishing. ISBN 9814317039. Dill Ellis Harold 2006. Continuum Mechanics Elasticity Plasticity Viscoelasticity. Germany CRC Press. ISBN 0849397790. Hutter Kolumban Jhnk Klaus 2004. Continuum Methods of Physical Modeling. Germany Springer. ISBN 3540206191. Jirasek M Bazant Z.P. 2002. Inelastic Analysis of Structures. London and New York J. Wiley  Sons. ISBN 0471987166. Lubarda Vlado A. 2001. Elastoplasticity Theory. CRC Press. ISBN 0849311381. Macosko C. W. 1994. Rheology principles measurement and applications. VCH Publishers. ISBN 1560815795. Mase George E. 1970. Continuum Mechanics. McGrawHill Professional. ISBN 0070406634. Mase G. Thomas Mase George E. 1999. Continuum Mechanics for Engineers 2nd ed.. CRC Press. ISBN 0849318556. NematNasser Sia 2006. Plasticity A Treatise on Finite Deformation of Heterogeneous Inelastic Materials. Cambridge Cambridge University Press. ISBN 0521839793. Prager William 1961. Introduction to Mechanics of Continua. Boston Ginn and Co. ISBN 0486438090."}, {"topic": "Density", "content": "The density or more precisely the volumetric mass density of a substance is its mass per unit volume. The symbol most often used for density is the lower case Greek letter rho although the Latin letter D can also be used. Mathematically density is defined as mass divided by volume  m V    Density. Encyclopdia Britannica 8 11th ed.. 1911. Density. The New Students Reference Work. 1914. Video Density Experiment with Oil and Alcohol Video Density Experiment with Whiskey and Water Glass Density Calculation Calculation of the density of glass at room temperature and of glass melts at 1000 1400C List of Elements of the Periodic Table Sorted by Density Calculation of saturated liquid densities for some components Field density test Online calculator for densities and partial molar volumes of aqueous solutions of some common electrolytes and their mixtures at temperatures up to 323.15 K. Water Density and specific weight Temperature dependence of the density of water Conversions of density units A delicious density experiment Water density calculator Water density for a given salinity and temperature. Liquid density calculator Select a liquid from the list and calculate density as a function of temperature. Gas density calculator Calculate density of a gas for as a function of temperature and pressure. Densities of various materials. Determination of Density of Solid instructions for performing classroom experiment. density prediction density prediction"}, {"topic": "Derivative", "content": "The derivative of a function of a real variable measures the sensitivity to change of a quantity a function value or dependent variable which is determined by another quantity the independent variable. Derivatives are a fundamental tool of calculus. For example the derivative of the position of a moving object with respect to time is the objects velocity this measures how quickly the position of the object changes when time is advanced. The derivative of a function of a single variable at a chosen input value when it exists is the slope of the tangent line to the graph of the function at that point. The tangent line is the best linear approximation of the function near that input value. For this reason the derivative is often described as the instantaneous rate of change the ratio of the instantaneous change in the dependent variable to that of the independent variable. Derivatives may be generalized to functions of several real variables. In this generalization the derivative is reinterpreted as a linear transformation whose graph is after an appropriate translation the best linear approximation to the graph of the original function. The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. It can be calculated in terms of the partial derivatives with respect to the independent variables. For a realvalued function of several variables the Jacobian matrix reduces to the gradient vector. The process of finding a derivative is called differentiation. The reverse process is called antidifferentiation. The fundamental theorem of calculus states that antidifferentiation is the same as integration. Differentiation and integration constitute the two fundamental operations in singlevariable calculus.   Hazewinkel Michiel ed. 2001 Derivative Encyclopedia of Mathematics Springer ISBN 9781556080104 Khan Academy Newton Leibniz and Usain Bolt Weisstein Eric W. Derivative MathWorld. Online Derivative Calculator from Wolfram Alpha."}, {"topic": "Destructive interference", "content": "In physics interference is a phenomenon in which two waves superpose to form a resultant wave of greater lower or the same amplitude. Interference usually refers to the interaction of waves that are correlated or coherent with each other either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves for example light radio acoustic surface water waves or matter waves.  Easy JavaScript Simulation Model of One Dimensional Wave Interference Expressions of position and fringe spacing Java demonstration of interference Java simulation of interference of water waves 1 Java simulation of interference of water waves 2 Flash animations demonstrating interference Lissajous Curves Interactive simulation of graphical representations of musical intervals beats interference vibrating strings Animations demonstrating optical interference by QED"}, {"topic": "Dielectric", "content": "A dielectric material dielectric for short is an electrical insulator that can be polarized by an applied electric field. When a dielectric is placed in an electric field electric charges do not flow through the material as they do in a conductor but only slightly shift from their average equilibrium positions causing dielectric polarization. Because of dielectric polarization positive charges are displaced toward the field and negative charges shift in the opposite direction. This creates an internal electric field that reduces the overall field within the dielectric itself. If a dielectric is composed of weakly bonded molecules those molecules not only become polarized but also reorient so that their symmetry axes align to the field. The study of dielectric properties concerns storage and dissipation of electric and magnetic energy in materials. Dielectrics are important for explaining various phenomena in electronics optics and solidstate physics.  Electromagnetism A chapter from an online textbook Dielectric Sphere in an Electric Field DoITPoMS Teaching and Learning Package Dielectric Materials Texts on Wikisource Dielectric. Encyclopedia Americana. 1920. Dielectric. Encyclopdia Britannica 11th ed.. 1911."}, {"topic": "Differential calculus", "content": "In mathematics differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change. It is one of the two traditional divisions of calculus the other being integral calculus. The primary objects of study in differential calculus are the derivative of a function related notions such as the differential and their applications. The derivative of a function at a chosen input value describes the rate of change of the function near that input value. The process of finding a derivative is called differentiation. Geometrically the derivative at a point is the slope of the tangent line to the graph of the function at that point provided that the derivative exists and is defined at that point. For a realvalued function of a single real variable the derivative of a function at a point generally determines the best linear approximation to the function at that point. Differential calculus and integral calculus are connected by the fundamental theorem of calculus which states that differentiation is the reverse process to integration. Differentiation has applications to nearly all quantitative disciplines. For example in physics the derivative of the displacement of a moving body with respect to time is the velocity of the body and the derivative of velocity with respect to time is acceleration. The derivative of the momentum of a body equals the force applied to the body rearranging this derivative statement leads to the famous F  ma equation associated with Newtons second law of motion. The reaction rate of a chemical reaction is a derivative. In operations research derivatives determine the most efficient ways to transport materials and design factories. Derivatives are frequently used to find the maxima and minima of a function. Equations involving derivatives are called differential equations and are fundamental in describing natural phenomena. Derivatives and their generalizations appear in many fields of mathematics such as complex analysis functional analysis differential geometry measure theory and abstract algebra.  J. Edwards 1892. Differential Calculus. London MacMillan and Co."}, {"topic": "Diffraction", "content": "Diffraction refers to various phenomena which occur when a wave encounters an obstacle or a slit. It is defined as the bending of light around the corners of an obstacle or aperture into the region of geometrical shadow of the obstacle. In classical physics the diffraction phenomenon is described as the interference of waves according to the HuygensFresnel principle. These characteristic behaviors are exhibited when a wave encounters an obstacle or a slit that is comparable in size to its wavelength. Similar effects occur when a light wave travels through a medium with a varying refractive index or when a sound wave travels through a medium with varying acoustic impedance. Diffraction occurs with all waves including sound waves water waves and electromagnetic waves such as visible light Xrays and radio waves. Since physical objects have wavelike properties at the atomic level diffraction also occurs with matter and can be studied according to the principles of quantum mechanics. Italian scientist Francesco Maria Grimaldi coined the word diffraction and was the first to record accurate observations of the phenomenon in 1660. While diffraction occurs whenever propagating waves encounter such changes its effects are generally most pronounced for waves whose wavelength is roughly comparable to the dimensions of the diffracting object or slit. If the obstructing object provides multiple closely spaced openings a complex pattern of varying intensity can result. This is due to the addition or interference of different parts of a wave that travels to the observer by different paths where different path lengths result in different phases see diffraction grating and wave superposition. The formalism of diffraction can also describe the way in which waves of finite extent propagate in free space. For example the expanding profile of a laser beam the beam shape of a radar antenna and the field of view of an ultrasonic transducer can all be analyzed using diffraction equations.  Diffraction Ri Channel Video December 2011 Diffraction and Crystallography for beginners Diffraction and acoustics. Diffraction in photography. On Diffraction at MathPages. Diffraction pattern calculators at The Wolfram Demonstrations Project Wave Optics A chapter of an online textbook. 2D wave Java applet Displays diffraction patterns of various slit configurations. Diffraction Java applet Displays diffraction patterns of various 2D apertures. Diffraction approximations illustrated MIT site that illustrates the various approximations in diffraction and intuitively explains the Fraunhofer regime from the perspective of linear system theory. Gap Obstacle Corner Java simulation of diffraction of water wave. Google Maps Satellite image of Panama Canal entry ocean wave diffraction. Google Maps and Bing Maps Aerial photo of waves diffracting through sea barriers at Sea Palling in Norfolk UK. An Introduction to The Wigner Distribution in Geometric Optics DoITPoMS Teaching and Learning Package Diffraction and Imaging Animations demonstrating Diffraction by QED FDTD Animation of single slit diffraction on YouTube"}, {"topic": "Digital physics", "content": "In physics and cosmology digital physics also referred to as digital ontology or digital philosophy is a collection of theoretical perspectives based on the premise that the universe is at heart describable by information. Therefore according to this theory the universe can be conceived of as either the output of a deterministic or probabilistic computer program a vast digital computation device or mathematically isomorphic to such a device.  Discrete Physics Mountain Math Software. Luciano Floridi Against Digital Ontology Synthese 2009 168.1 2009 151178. Edward Fredkin Digital Philosophy Introduction to Digital Philosophy Gontigno Paulo Hypercomputation and the Physical ChurchTuring thesis Juergen Schmidhuber Home page 19962007 Computer Universes and Algorithmic Theory of Everything Zuses Thesis The Universe is a Computer Konrad Zuse PDF scan of Zuses paper. Konrad Zuse Reedition of Zuses paper in modern LaTeX. The Oxford Advanced Seminar on Informatic Structures Wired God is the Machine Gualtiero Piccinini. Computation in Physical Systems Discusses the metaphysical foundations of digital physics in section 3.4."}, {"topic": "Direct current", "content": "Direct current DC is the unidirectional flow of electric charge. Direct current is produced by sources such as batteries power supplies thermocouples solar cells or dynamos. Direct current may flow in a conductor such as a wire but can also flow through semiconductors insulators or even through a vacuum as in electron or ion beams. The electric current flows in a constant direction distinguishing it from alternating current AC. A term formerly used for this type of current was galvanic current. The abbreviations AC and DC are often used to mean simply alternating and direct as when they modify current or voltage. Direct current may be obtained from an alternating current supply by use of a rectifier which contains electronic elements usually or electromechanical elements historically that allow current to flow only in one direction. Direct current may be converted into alternating current with an inverter or a motorgenerator set. Direct current is used to charge batteries and as power supply for electronic systems. Very large quantities of directcurrent power are used in production of aluminum and other electrochemical processes. It is also used for some railways especially in urban areas. Highvoltage direct current is used to transmit large amounts of power from remote generation sites or to interconnect alternating current power grids.  ACDC Whats the Difference. DC And AC Supplies PDF. ITACA."}, {"topic": "Dispersion (optics)", "content": "In optics dispersion is the phenomenon in which the phase velocity of a wave depends on its frequency. Media having this common property may be termed dispersive media. Sometimes the term chromatic dispersion is used for specificity. Although the term is used in the field of optics to describe light and other electromagnetic waves dispersion in the same sense can apply to any sort of wave motion such as acoustic dispersion in the case of sound and seismic waves in gravity waves ocean waves and for telecommunication signals propagating along transmission lines such as coaxial cable or optical fiber. In optics one important and familiar consequence of dispersion is the change in the angle of refraction of different colors of light as seen in the spectrum produced by a dispersive prism and in chromatic aberration of lenses. Design of compound achromatic lenses in which chromatic aberration is largely cancelled uses a quantification of a glasss dispersion given by its Abbe number V where lower Abbe numbers correspond to greater dispersion over the visible spectrum. In some applications such as telecommunications the absolute phase of a wave is often not important but only the propagation of wave packets or pulses in that case one is interested only in variations of group velocity with frequency socalled groupvelocity dispersion GVD.  Dispersive Wiki discussing the mathematical aspects of dispersion. Dispersion Encyclopedia of Laser Physics and Technology Animations demonstrating optical dispersion by QED Interactive webdemo for chromatic dispersion Institute of Telecommunications University of Stuttgart"}, {"topic": "Displacement (fluid)", "content": "In fluid mechanics displacement occurs when an object is immersed in a fluid pushing it out of the way and taking its place. The volume of the fluid displaced can then be measured and from this the volume of the immersed object can be deduced the volume of the immersed object will be exactly equal to the volume of the displaced fluid. An object that sinks displaces an amount of fluid equal to the objects volume. Thus buoyancy is expressed through Archimedes principle which states that the weight of the object is reduced by its volume multiplied by the density of the fluid. If the weight of the object is less than this displaced quantity the object floats if more it sinks. The amount of fluid displaced is directly related via Archimedes Principle to its volume. In the case of an object that sinks is totally submerged the volume of the object is displaced. In the case of an object that floats the amount of fluid displaced will be equal in weight to the displacing object. Archimedes Principle physical law of buoyancy stating that any body completely or partially submerged in a fluid gas or liquid at rest is acted upon by an upward or buoyant force the magnitude of which is equal to the weight of the fluid displaced by the body. The volume of displaced fluid is equivalent to the volume of an object fully immersed in a fluid or to that fraction of the volume below the surface for an object partially submerged in a liquid. The weight of the displaced portion of the fluid is equivalent to the magnitude of the buoyant force. The buoyant force on a body floating in a liquid or gas is also equivalent in magnitude to the weight of the floating object and is opposite in direction the object neither rises nor sinks. If the weight of an object is less than that of the displaced fluid the object rises as in the case of a block of wood that is released beneath the surface of water or a heliumfilled balloon that is let loose in air. An object heavier than the amount of the fluid it displaces though it sinks when released has an apparent weight loss equal to the weight of the fluid displaced. In fact in some accurate weighing a correction must be made in order to compensate for the buoyancy effect of the surrounding air. The buoyant force which always opposes gravity is nevertheless caused by gravity. Fluid pressure increases with depth because of the gravitational weight of the fluid above. This increasing pressure applies a force on a submerged object that increases with depth. The result is buoyancy. "}, {"topic": "Displacement (vector)", "content": "A displacement is a vector that is the shortest distance from the initial to the final position of a point P. It quantifies both the distance and direction of an imaginary motion along a straight line from the initial position to the final position of the point. A displacement may be also described as a relative position the final position of a point Rf relative to its initial position Ri and a displacement vector can be mathematically defined as the difference between the final and initial position vectors s  R f R i  R  These common names correspond to terminology used in basic kinematics. By extension the higher order derivatives can be computed in a similar fashion. Study of these higher order derivatives can improve approximations of the original displacement function. Such higherorder terms are required in order to accurately represent the displacement function as a sum of an infinite series enabling several analytical techniques in engineering and physics. The fourth order derivative is called jounce the fifth crackle and the sixth pop. "}, {"topic": "Distance", "content": "Distance is a numerical description of how far apart objects are. In physics or everyday usage distance may refer to a physical length or an estimation based on other criteria e.g. two counties over. In mathematics a distance function or metric is a generalization of the concept of physical distance. A metric is a function that behaves according to a specific set of rules and is a concrete way of describing what it means for elements of some space to be close to or far away from each other. In most cases distance from A to B is interchangeable with distance between B and A.  Deza E. Deza M. 2006 Dictionary of Distances Elsevier ISBN 0444520872 ."}, {"topic": "Doppler effect", "content": "The Doppler effect or the Doppler shift is the change in frequency of a wave or other periodic event for an observer moving relative to its source. It is named after the Austrian physicist Christian Doppler who proposed it in 1842 in Prague. It is commonly heard when a vehicle sounding a siren or horn approaches passes and recedes from an observer. Compared to the emitted frequency the received frequency is higher during the approach identical at the instant of passing by and lower during the recession. When the source of the waves is moving towards the observer each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore each wave takes slightly less time to reach the observer than the previous wave. Hence the time between the arrival of successive wave crests at the observer is reduced causing an increase in the frequency. While they are travelling the distance between successive wave fronts is reduced so the waves bunch together. Conversely if the source of waves is moving away from the observer each wave is emitted from a position farther from the observer than the previous wave so the arrival time between successive waves is increased reducing the frequency. The distance between successive wave fronts is then increased so the waves spread out. For waves that propagate in a medium such as sound waves the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source motion of the observer or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium such as light or gravity in general relativity only the relative difference in velocity between the observer and the source needs to be considered.  Doppler Effect ScienceWorld Java simulation of Doppler effect Doppler Shift for Sound and Light at MathPages Flash simulation and game of Doppler effect of sound at Scratch programming language The Doppler Effect and Sonic Booms D.A. Russell Kettering University Video Mashup with Doppler Effect videos Wave Propagation from John de Pillis. An animation showing that the speed of a moving wave source does not affect the speed of the wave. EM Wave Animation from John de Pillis. How an electromagnetic wave propagates through a vacuum Doppler Shift Demo  Interactive flash simulation for demonstrating Doppler shift. Interactive applets at Physics 2000"}, {"topic": "Drag (physics)", "content": "In fluid dynamics drag sometimes called air resistance a type of friction or fluid resistance another type of friction or fluid friction is a force acting opposite to the relative motion of any object moving with respect to a surrounding fluid. This can exist between two fluid layers or surfaces or a fluid and a solid surface. Unlike other resistive forces such as dry friction which are nearly independent of velocity drag forces depend on velocity. Drag force is proportional to the velocity for a laminar flow and the squared velocity for a turbulent flow. Even though the ultimate cause of a drag is viscous friction the turbulent drag is independent of viscosity. Drag forces always decrease fluid velocity relative to the solid object in the fluids path.  Educational materials on air resistance Aerodynamic Drag and its effect on the acceleration and top speed of a vehicle. Vehicle Aerodynamic Drag calculator based on drag coefficient frontal area and speed. Smithsonian National Air and Space Museums How Things Fly website Effect of dimples on a golf ball and a car"}, {"topic": "Drift velocity", "content": "The drift velocity is the flow velocity that a particle such as an electron attains in a material due to an electric field. It can also be referred to as axial drift velocity. In general an electron will propagate randomly in a conductor at the Fermi velocity. An applied electric field will give this random motion a small net flow velocity in one direction. In a semiconductor the two main carrier scattering mechanisms are ionized impurity scattering and lattice scattering. Because current is proportional to drift velocity which in a resistive material is in turn proportional to the magnitude of an external electric field Ohms law can be explained in terms of drift velocity. The most elementary expression of Ohms law is u  E    Ohms Law Microscopic View at Hyperphysics"}, {"topic": "Ductility", "content": "In materials science ductility is a solid materials ability to deform under tensile stress this is often characterized by the materials ability to be stretched into a wire. Malleability a similar property is a materials ability to deform under compressive stress this is often characterized by the materials ability to form a thin sheet by hammering or rolling. Both of these mechanical properties are aspects of plasticity the extent to which a solid material can be plastically deformed without fracture. Also these material properties are dependent on temperature and pressure investigated by Percy Williams Bridgman as part of his Nobel Prizewinning work on high pressures. Ductility and malleability are not always coextensive for instance while gold has high ductility and malleability lead has low ductility but high malleability. The word ductility is sometimes used to encompass both types of plasticity.  Ductility definition at engineersedge.com DoITPoMS Teaching and Learning Package The DuctileBrittle Transition"}, {"topic": "Dynamics (mechanics)", "content": "Dynamics is a branch of applied mathematics specifically classical mechanics concerned with the study of forces and torques and their effect on motion as opposed to kinematics which studies the motion of objects without reference to its causes. Isaac Newton defined the fundamental physical laws which govern dynamics in physics especially his second law of motion.  Swagatam 25 March 2010. Calculating Engineering Dynamics Using Newtons Laws. Bright Hub. Archived from the original on April 12 2011. Retrieved 20100410. Wilson C. E. 2003. Kinematics and dynamics of machinery. Pearson Education. ISBN 9780201350999. Dresig H. Holzweiig F. 2010. Dynamics of Machinery. Theory and Applications. Springer ScienceBusiness Media Dordrecht London New York. ISBN 9783540899396. CS1 maint Multiple names authors list link"}, {"topic": "Dyne", "content": "The dyne symbol dyn from Greek  dynamis meaning power force is a unit of force specified in the centimetregramsecond system of units CGS a predecessor of the modern SI. One dyne is equal to 105 N or to 10 nsn nanosthenes in the old metretonnesecond system of units. Equivalently the dyne is defined as the force required to accelerate a mass of one gram at a rate of one centimetre per second squared 1 dyn  1 gcms2  105 kgms2  105 N 1 N  1 kgms2  105 gcms2  105 dyn The dyne per centimetre is the unit traditionally used to measure surface tension. For example the surface tension of distilled water is 72 dyncm at 25 C 77 F in SI units this is 699872000000000000072103 Nm or 699872000000000000072 mNm. "}, {"topic": "Econophysics", "content": "Econophysics is an interdisciplinary research field applying theories and methods originally developed by physicists in order to solve problems in economics usually those including uncertainty or stochastic processes and nonlinear dynamics. Some of its application to the study of financial markets has also been termed statistical finance referring to its roots in statistical physics.  Econophysics Ph.D. Program at University of Houston Houston TX. Finance Gets Physical  Yale Economic Review Econophysics Forum Conference to mark 25th anniversary of Farjoun and Machovers book Chair of International Economics University of Bamberg Germany Econophysics Colloquium"}, {"topic": "Elastic collision", "content": "An elastic collision is an encounter between two bodies in which the total kinetic energy of the two bodies after the encounter is equal to their total kinetic energy before the encounter. Elastic collisions occur only if there is no net conversion of kinetic energy into other forms such as heat or noise. During the collision of small objects kinetic energy is first converted to potential energy associated with a repulsive force between the particles when the particles move against this force i.e. the angle between the force and the relative velocity is obtuse then this potential energy is converted back to kinetic energy when the particles move with this force i.e. the angle between the force and the relative velocity is acute. The collisions of atoms are elastic collisions Rutherford backscattering is one example. The moleculesas distinct from atomsof a gas or liquid rarely experience perfectly elastic collisions because kinetic energy is exchanged between the molecules translational motion and their internal degrees of freedom with each collision. At any one instant half the collisions are to a varying extent inelastic collisions the pair possesses less kinetic energy in their translational motions after the collision than before and half could be described as superelastic possessing more kinetic energy after the collision than before. Averaged across the entire sample molecular collisions can be regarded as essentially elastic as long as Plancks law forbids blackbody photons to carry away energy from the system. In the case of macroscopic bodies perfectly elastic collisions are an ideal never fully realized but approximated by the interactions of objects such as billiard balls. When considering energies possible rotational energy before andor after a collision may also play a role.  Rigid Body Collision Resolution in three dimensions including a derivation using the conservation laws VNE Rigid Body Collision Simulation Small Open Source 3D engine with easytounderstand implementation of elastic collisions in C Visualize 2D Collision Free simulation of 2particle collision with useradjustable coefficient of restitution and particle velocities Requires Adobe Shockwave 2Dimensional Elastic Collisions without Trigonometry Explanation of how to calculate 2dimensional elastic collisions using vectors Bouncescope Free simulator of elastic collisions of dozens of userconfigurable objects Managing ball vs ball collision with Flash Flash script to manage elastic collisions among any number of spheres Elastic collision derivation Elastic collision formula derivation if one of balls velocity is 0"}, {"topic": "Elastic energy", "content": "Elastic energy is the potential mechanical energy stored in the configuration of a material or physical system as work is performed to distort its volume or shape. Elastic energy occurs when objects are compressed and stretched or generally deformed in any manner. Elasticity theory primarily develops formalisms for the mechanics of solid bodies and materials. Note however the work done by a stretched rubber band is not an example of elastic energy. It is an example of entropic elasticity. The elastic potential energy equation is used in calculations of positions of mechanical equilibrium. The energy is potential as it will be converted into another form of energy such as kinetic. Mathematically the equation can be stated as U  1 2 k x 2  in this case but merely a single component of a tensor. "}, {"topic": "Elastic instability", "content": "Elastic instability is a form of instability occurring in elastic systems such as buckling of beams and plates subject to large compressive loads.  Theory of elastic stability S. Timoshenko and J. Gere"}, {"topic": "Elastic modulus", "content": "The modulus of elasticity also known as the elastic modulus the tensile modulus or Youngs modulus is a number that measures an object or substances resistance to being deformed elastically i.e. nonpermanently when a force is applied to it. The elastic modulus of an object is defined as the slope of its stressstrain curve in the elastic deformation region A stiffer material will have a higher elastic modulus. An elastic modulus has the form.  def stress strain   describes an objects tendency to shear the deformation of shape at constant volume when acted upon by opposing forces it is defined as shear stress over shear strain. The shear modulus is part of the derivation of viscosity. The bulk modulus K describes volumetric elasticity or the tendency of an object to deform in all directions when uniformly loaded in all directions it is defined as volumetric stress over volumetric strain and is the inverse of compressibility. The bulk modulus is an extension of Youngs modulus to three dimensions. Three other elastic moduli are Axial Modulus Lams first parameter and Pwave modulus. Homogeneous and isotropic similar in all directions materials solids have their linear elastic properties fully described by two elastic moduli and one may choose any pair. Given a pair of elastic moduli all other elastic moduli can be calculated according to formulas in the table below at the end of page. Inviscid fluids are special in that they cannot support shear stress meaning that the shear modulus is always zero. This also implies that Youngs modulus is always zero. In some English texts the here described quantity is called elastic constant while the inverse quantity is referred to as elastic modulus.  Hartsuijker C. Welleman J. W. 2001. Engineering Mechanics. Volume 2. Springer. ISBN 9781402041235. De Jong Maarten Chen Wei 2015. Charting the complete elastic properties of inorganic crystalline compounds. Scientific Data 2 150009. doi10.1038sdata.2015.9."}, {"topic": "Elasticity (physics)", "content": "In physics elasticity from Greek ductible is the ability of a body to resist a distorting influence or stress and to return to its original size and shape when the stress is removed. Solid objects will deform when forces are applied on them. If the material is elastic the object will return to its initial shape and size when these forces are removed. The physical reasons for elastic behavior can be quite different for different materials. In metals the atomic lattice changes size and shape when forces are applied energy is added to the system. When forces are removed the lattice goes back to the original lower energy state. For rubbers and other polymers elasticity is caused by the stretching of polymer chains when forces are applied. Perfect elasticity is an approximation of the real world and few materials remain purely elastic even after very small deformations. In engineering the amount of elasticity of a material is determined by two types of material parameter. The first type of material parameter is called a modulus which measures the amount of force per unit area stress needed to achieve a given amount of deformation. The units of modulus are pascals Pa or pounds of force per square inch psi also lbfin2. A higher modulus typically indicates that the material is harder to deform. The second type of parameter measures the elastic limit. The limit can be a stress beyond which the material no longer behaves elastic and deformation of the material will take place. If the stress is released the material will elastically return to a permanent deformed shape instead of the original shape. When describing the relative elasticities of two materials both the modulus and the elastic limit have to be considered. Rubbers typically have a low modulus and tend to stretch a lot that is they have a high elastic limit and so appear more elastic than metals high modulus and low elastic limit in everyday experience. Of two rubber materials with the same elastic limit the one with a lower modulus will appear to be more elastic. "}, {"topic": "Electric charge", "content": "Electric charge is the physical property of matter that causes it to experience a force when placed in an electromagnetic field. There are two types of electric charges positive and negative. Like charges repel and unlike attract. An object is negatively charged if it has an excess of electrons and is otherwise positively charged or uncharged. The SI derived unit of electric charge is the coulomb C. In electrical engineering it is also common to use the amperehour Ah and in chemistry it is common to use the elementary charge e as a unit. The symbol Q often denotes charge. Early knowledge of how charged substances interact is now called classical electrodynamics and is still accurate for problems that dont require consideration of quantum effects. The electric charge is a fundamental conserved property of some subatomic particles which determines their electromagnetic interaction. Electrically charged matter is influenced by and produces electromagnetic fields. The interaction between a moving charge and an electromagnetic field is the source of the electromagnetic force which is one of the four fundamental forces See also magnetic field. Twentiethcentury experiments demonstrated that electric charge is quantized that is it comes in integer multiples of individual small units called the elementary charge e approximately equal to 69811602000000000001.6021019 coulombs except for particles called quarks which have charges that are integer multiples of e3. The proton has a charge of e and the electron has a charge of e. The study of charged particles and how their interactions are mediated by photons is called quantum electrodynamics.  How fast does a charge decay Science Aid Electrostatic charge Easytounderstand page on electrostatic charge. History of the electrical units."}, {"topic": "Electric circuit", "content": "An electrical network is an interconnection of electrical components e.g. batteries resistors inductors capacitors switches or a model of such an interconnection consisting of electrical elements e.g. voltage sources current sources resistances inductances capacitances. An electrical circuit is a network consisting of a closed loop giving a return path for the current. Linear electrical networks a special type consisting only of sources voltage or current linear lumped elements resistors capacitors inductors and linear distributed elements transmission lines have the property that signals are linearly superimposable. They are thus more easily analyzed using powerful frequency domain methods such as Laplace transforms to determine DC response AC response and transient response. A resistive circuit is a circuit containing only resistors and ideal current and voltage sources. Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors. If the sources are constant DC sources the result is a DC circuit. A network that contains active electronic components is known as an electronic circuit. Such networks are generally nonlinear and require more complex design and analysis tools. "}, {"topic": "Electric current", "content": "An electric current is a flow of electric charge. In electric circuits this charge is often carried by moving electrons in a wire. It can also be carried by ions in an electrolyte or by both ions and electrons such as in a plasma. The SI unit for measuring an electric current is the ampere which is the flow of electric charge across a surface at the rate of one coulomb per second. Electric current is measured using a device called an ammeter. Electric currents cause Joule heating which creates light in incandescent light bulbs. They also create magnetic fields which are used in motors inductors and generators. The particles that carry the charge in an electric current are called charge carriers. In metals one or more electrons from each atom are loosely bound to the atom and can move freely about within the metal. These conduction electrons are the charge carriers in metal conductors.  Allaboutcircuits.com a useful site introducing electricity and electronics"}, {"topic": "Electric displacement field", "content": "In physics the electric displacement field denoted by D is a vector field that appears in Maxwells equations. It accounts for the effects of free and bound charge within materials. D stands for displacement as in the related concept of displacement current in dielectrics. In free space the electric displacement field is equivalent to flux density a concept that lends understanding to Gausss law. It has the SI units of coulomb per squared metre C m2.  electric displacement field at PhysicsForums"}, {"topic": "Electric field", "content": "An electric field is a vector field that associates to each point in space the Coulomb force experienced by a unit electric charge. Electric fields converge and diverge at electric charges and they can be induced by timevarying magnetic fields. The electric field combines with the magnetic field to form the electromagnetic field.  Electric field in Electricity and Magnetism R Nave Hyperphysics Georgia State University Gausss Law Chapter 24 of Frank Wolfss lectures at University of Rochester The Electric Field Chapter 23 of Frank Wolfss lectures at University of Rochester 1 An applet that shows the electric field of a moving point charge. Fields a chapter from an online textbook Learning by Simulations Interactive simulation of an electric field of up to four point charges Java simulations of electrostatics in 2D and 3D Interactive Flash simulation picturing the electric field of userdefined or preselected sets of point charges by field vectors field lines or equipotential lines. Author David Chappell"}, {"topic": "Electric field gradient", "content": "In atomic molecular and solidstate physics the electric field gradient EFG measures the rate of change of the electric field at an atomic nucleus generated by the electronic charge distribution and the other nuclei. The EFG couples with the nuclear electric quadrupole moment of quadrupolar nuclei those with spin quantum number greater than onehalf to generate an effect which can be measured using several spectroscopic methods such as nuclear magnetic resonance NMR microwave spectroscopy electron paramagnetic resonance EPR ESR nuclear quadrupole resonance NQR Mssbauer spectroscopy or perturbed angular correlation PAC. The EFG is nonzero only if the charges surrounding the nucleus violate cubic symmetry and therefore generate an inhomogeneous electric field at the position of the nucleus. EFGs are highly sensitive to the electronic density in the immediate vicinity of a nucleus. This is because the EFG operator scales as r3 where r is the distance from a nucleus. This sensitivity has been used to study effects on charge distribution resulting from substitution weak interactions and charge transfer.  Kaufmann Elton N Reiner J. Vianden 1979. The electric field gradient in noncubic metals. Reviews of Modern Physics 51 1 161214. Bibcode1979RvMP...51..161K. doi10.1103RevModPhys.51.161."}, {"topic": "Electric generator", "content": "In electricity generation a generator is a device that converts mechanical energy to electrical energy for use in an external circuit. The source of mechanical energy may vary widely from a hand crank to an internal combustion engine. Generators provide nearly all of the power for electric power grids. The reverse conversion of electrical energy into mechanical energy is done by an electric motor and motors and generators have many similarities. Many motors can be mechanically driven to generate electricity and frequently make acceptable generators.  Simple generator Demonstration of an electrical generator Short video of a simple generator"}, {"topic": "Electric intensity", "content": "Electric field intensity is the strength of an electric field at any point. It is equal to the electric force per unit charge experienced by a test charge placed at that point. The unit of measurement is volts per meter or newtons per coulomb. This physical quantity has dimensions MLT3A1. It is a vector quantity and its direction is along the direction of force."}, {"topic": "Electric motor", "content": "An electric motor is an electrical machine that converts electrical energy into mechanical energy. The reverse of this would be the conversion of mechanical energy into electrical energy and is done by an electric generator. In normal motoring mode most electric motors operate through the interaction between an electric motors magnetic field and winding currents to generate force within the motor. In certain applications such as in the transportation industry with traction motors electric motors can operate in both motoring and generating or braking modes to also produce electrical energy from mechanical energy. Found in applications as diverse as industrial fans blowers and pumps machine tools household appliances power tools and disk drives electric motors can be powered by direct current DC sources such as from batteries motor vehicles or rectifiers or by alternating current AC sources such as from the power grid inverters or generators. Small motors may be found in electric watches. Generalpurpose motors with highly standardized dimensions and characteristics provide convenient mechanical power for industrial use. The largest of electric motors are used for ship propulsion pipeline compression and pumpedstorage applications with ratings reaching 100 megawatts. Electric motors may be classified by electric power source type internal construction application type of motion output and so on. Electric motors are used to produce linear or rotary force torque and should be distinguished from devices such as magnetic solenoids and loudspeakers that convert electricity into motion but do not generate usable mechanical powers which are respectively referred to as actuators and transducers.  SparkMuseum Early Electric Motors The Invention of the Electric Motor 1800 to 1893 hosted by Karlsrushe Institute of Technologys Martin Doppelbauer Electric Motors and Generators a U. of NSW Physclips multimedia resource IEA 4E  Efficient Electrical EndUse Equipment. iPES Rotating Magnetic Field animation"}, {"topic": "Electric potential", "content": "An electric potential also called the electric field potential or the electrostatic potential is the amount of electric potential energy that a unitary point electric charge would have if located at any point in space and is equal to the work done by an external agent in carrying a unit of positive charge from the arbitrarily chosen reference point usually infinity to that point without any acceleration. According to theoretical electromagnetics electric potential is a scalar quantity denoted by V equal to the electric potential energy of any charged particle at any location measured in joules divided by the charge of that particle measured in coulombs. By dividing out the charge on the particle a remainder is obtained that is a property of the electric field itself. This value can be calculated in either a static timeinvariant or a dynamic varying with time electric field at a specific time in units of joules per coulomb J C1 or volts V. The electric potential at infinity is assumed to be zero. A generalized electric scalar potential is also used in electrodynamics when timevarying electromagnetic fields are present but this can not be so simply calculated. The electric potential and the magnetic vector potential together form a four vector so that the two kinds of potential are mixed under Lorentz transformations. "}, {"topic": "Electric power", "content": "Electric power is the rate at which electrical energy is transferred by an electric circuit. The SI unit of power is the watt one joule per second. Electric power is usually produced by electric generators but can also be supplied by sources such as electric batteries. It is usually supplied to businesses and homes by the electric power industry through an electric power grid. Electric power is usually sold by the kilowatt hour 3.6 MJ which is the product of power in kilowatts multiplied by running time in hours. Electric utilities measure power using an electricity meter which keeps a running total of the electric energy delivered to a customer. Electrical power provides a low entropy form of energy and can be converted into motion or other forms of energy with high efficiency.  U.S. Department of Energy Electric Power GlobTek Inc. Glossary of Electric power Power Supply Terms"}, {"topic": "Electrical and electronics engineering", "content": "Electrical engineering is a field of engineering that generally deals with the study and application of electricity electronics and electromagnetism. This field first became an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph the telephone and electric power distribution and use. Subsequently broadcasting and recording media made electronics part of daily life. The invention of the transistor and later the integrated circuit brought down the cost of electronics to the point they can be used in almost any household object. Electrical engineering has now subdivided into a wide range of subfields including electronics digital computers power engineering telecommunications control systems radiofrequency engineering signal processing instrumentation and microelectronics. The subject of electronic engineering is often treated as its own subfield but it intersects with all the other subfields including the power electronics of power engineering. Electrical engineers typically hold a degree in electrical engineering or electronic engineering. Practicing engineers may have professional certification and be members of a professional body. Such bodies include the Institute of Electrical and Electronics Engineers IEEE and the Institution of Engineering and Technology professional society IET. Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from basic circuit theory to the management skills required of a project manager. The tools and equipment that an individual engineer may need are similarly variable ranging from a simple voltmeter to a top end analyzer to sophisticated design and manufacturing software.  International Electrotechnical Commission IEC MIT OpenCourseWare indepth look at Electrical Engineering  online courses with video lectures. IEEE Global History Network A wikibased site with many resources about the history of IEEE its members their professions and electrical and informational technologies and sciences."}, {"topic": "Electrical conductor", "content": "In physics and electrical engineering a conductor is an object or type of material that allows the flow of an electrical current in one or more directions. A metal wire is a common electrical conductor. In metals such as copper or aluminum the mobile charged particles are electrons. Positive charges may also be mobile such as the cationic electrolytes of a battery or the mobile protons of the proton conductor of a fuel cell. Insulators are nonconducting materials with few mobile charges that support only insignificant electric currents.  Stephen Gray scientist first to identify electrical conductors and insulators. Resistivity Charge transfer complex Bundle conductor Superconductivity Semiconductor"}, {"topic": "Electrical impedance", "content": "Electrical impedance is the measure of the opposition that a circuit presents to a current when a voltage is applied. In quantitative terms it is the complex ratio of the voltage to the current in an alternating current AC circuit. Impedance extends the concept of resistance to AC circuits and possesses both magnitude and phase unlike resistance which has only magnitude. When a circuit is driven with direct current DC there is no distinction between impedance and resistance the latter can be thought of as impedance with zero phase angle. It is necessary to introduce the concept of impedance in AC circuits because there are two additional impeding mechanisms to be taken into account besides the normal resistance of DC circuits the induction of voltages in conductors selfinduced by the magnetic fields of currents inductance and the electrostatic storage of charge induced by voltages between conductors capacitance. The impedance caused by these two effects is collectively referred to as reactance and forms the imaginary part of complex impedance whereas resistance forms the real part. The symbol for impedance is usually Z and it may be represented by writing its magnitude and phase in the form Z. However cartesian complex number representation is often more powerful for circuit analysis purposes. The term impedance was coined by Oliver Heaviside in July 1886. Arthur Kennelly was the first to represent impedance with complex numbers in 1893. Impedance is defined as the frequency domain ratio of the voltage to the current. In other words it is the voltagecurrent ratio for a single complex exponential at a particular frequency . In general impedance will be a complex number with the same units as resistance for which the SI unit is the ohm . For a sinusoidal current or voltage input the polar form of the complex impedance relates the amplitude and phase of the voltage and current. In particular The magnitude of the complex impedance is the ratio of the voltage amplitude to the current amplitude. The phase of the complex impedance is the phase shift by which the current lags the voltage. The reciprocal of impedance is admittance i.e. admittance is the currenttovoltage ratio and it conventionally carries units of siemens formerly called mhos.  Explaining Impedance Antenna Impedance ECE 209 Review of Circuits as LTI Systems Brief explanation of Laplacedomain circuit analysis includes a definition of impedance."}, {"topic": "Electrical insulator", "content": "An electrical insulator is a material whose internal electric charges do not flow freely and therefore make it nearly impossible to conduct an electric current under the influence of an electric field. This contrasts with other materials semiconductors and conductors which conduct electric current more easily. The property that distinguishes an insulator is its resistivity insulators have higher resistivity than semiconductors or conductors. A perfect insulator does not exist because even insulators contain small numbers of mobile charges charge carriers which can carry current. In addition all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass paper and Teflon which have high resistivity are very good electrical insulators. A much larger class of materials even though they may have lower bulk resistivity are still good enough to prevent significant current from flowing at normally used voltages and thus are employed as insulation for electrical wiring and cables. Examples include rubberlike polymers and most plastics. Insulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called insulation. The term insulator is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.  Sue Taylor May 2003. Bullers of Milton. ISBN 9781897949962. Function of Grading rings to Composite Insulator"}, {"topic": "Electrical network", "content": "An electrical network is an interconnection of electrical components e.g. batteries resistors inductors capacitors switches or a model of such an interconnection consisting of electrical elements e.g. voltage sources current sources resistances inductances capacitances. An electrical circuit is a network consisting of a closed loop giving a return path for the current. Linear electrical networks a special type consisting only of sources voltage or current linear lumped elements resistors capacitors inductors and linear distributed elements transmission lines have the property that signals are linearly superimposable. They are thus more easily analyzed using powerful frequency domain methods such as Laplace transforms to determine DC response AC response and transient response. A resistive circuit is a circuit containing only resistors and ideal current and voltage sources. Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors. If the sources are constant DC sources the result is a DC circuit. A network that contains active electronic components is known as an electronic circuit. Such networks are generally nonlinear and require more complex design and analysis tools. "}, {"topic": "Electrical potential energy", "content": "Electric potential energy or electrostatic potential energy is a potential energy measured in joules that results from conservative Coulomb forces and is associated with the configuration of a particular set of point charges within a defined system. An object may have electric potential energy by virtue of two key elements its own electric charge and its relative position to other electrically charged objects. The term electric potential energy is used to describe the potential energy in systems with timevariant electric fields while the term electrostatic potential energy is used to describe the potential energy in systems with timeinvariant electric fields. "}, {"topic": "Electrical reactance", "content": "In electrical and electronic systems reactance is the opposition of a circuit element to a change in current or voltage due to that elements inductance or capacitance. A builtup electric field resists the change of voltage on the element while a magnetic field resists the change of current. The notion of reactance is similar to electrical resistance but it differs in several respects. In phasor analysis reactance is used to compute amplitude and phase changes of sinusoidal alternating current going through a circuit element. It is denoted by the symbol X  phase difference with the sinusoidal current through the component. The component alternately absorbs energy from the circuit and then returns energy to the circuit thus a pure reactance does not dissipate power.  Interactive Java Tutorial on Inductive Reactance National High Magnetic Field Laboratory Reactance calculator"}, {"topic": "Electrical resistance", "content": "The electrical resistance of an electrical conductor is a measure of the difficulty to pass an electric current through that conductor. The inverse quantity is electrical conductance and is the ease with which an electric current passes. Electrical resistance shares some conceptual parallels with the notion of mechanical friction. The SI unit of electrical resistance is the ohm  while electrical conductance is measured in siemens S. An object of uniform cross section has a resistance proportional to its resistivity and length and inversely proportional to its crosssectional area. All materials show some resistance except for superconductors which have a resistance of zero. The resistance R of an object is defined as the ratio of voltage across it V to current through it I while the conductance G is the inverse R  V I  G  I V  1 R  is typically 3103 K1 to 6103 K1 for metals near room temperature. It is usually negative for semiconductors and insulators with highly variable magnitude.  The Notion of Electrical Resistance. Review of the equations that determine the value of electrical resistance. Clemson Vehicular Electronics Laboratory Resistance Calculator"}, {"topic": "Electricity", "content": "Electricity is the set of physical phenomena associated with the presence and flow of electric charge. Electricity gives a wide variety of wellknown effects such as lightning static electricity electromagnetic induction and electric current. In addition electricity permits the creation and reception of electromagnetic radiation such as radio waves. In electricity charges produce electromagnetic fields which act on other charges. Electricity occurs due to several types of physics electric charge a property of some subatomic particles which determines their electromagnetic interactions. Electrically charged matter is influenced by and produces electromagnetic fields electric charges can be positive or negative. electric field see electrostatics charges are surrounded by an electric field. The electric field produces a force on other charges. Changes in the electric field travel at the speed of light. electric potential the capacity of an electric field to do work on an electric charge typically measured in volts. electric current a movement or flow of electrically charged particles typically measured in amperes. electromagnets Moving charges produce a magnetic field. Electric currents generate magnetic fields and changing magnetic fields generate electric currents. In electrical engineering electricity is used for electric power where electric current is used to energise equipment electronics which deals with electrical circuits that involve active electrical components such as vacuum tubes transistors diodes and integrated circuits and associated passive interconnection technologies. Electrical phenomena have been studied since antiquity though progress in theoretical understanding remained slow until the seventeenth and eighteenth centuries. Even then practical applications for electricity were few and it would not be until the late nineteenth century that engineers were able to put it to industrial and residential use. The rapid expansion in electrical technology at this time transformed industry and society. Electricitys extraordinary versatility means it can be put to an almost limitless set of applications which include transport heating lighting communications and computation. Electrical power is now the backbone of modern industrial society.  Media related to Electricity at Wikimedia Commons OneHundred Years of Electricity May 1931 Popular Mechanics Illustrated view of how an American homes electrical system works Electricity around the world Electricity Misconceptions Electricity and Magnetism Understanding Electricity and Electronics in about 10 Minutes World Bank report on Water Electricity and Utility subsidies"}, {"topic": "Electrochemical cell", "content": "An electrochemical cell is a device capable of either generating electrical energy from chemical reactions or facilitating chemical reactions through the introduction of electrical energy. A common example of an electrochemical cell is a standard 1.5volt cell meant for consumer use. This type of device is known as a single Galvanic cell. A battery consists of two or more cells connected in either parallel or series pattern.  Activity chemistry Cell notation Electrochemical potential"}, {"topic": "Electrodynamics", "content": "Classical electromagnetism or classical electrodynamics is a branch of theoretical physics that studies the interactions between electric charges and currents using an extension of the classical Newtonian model. The theory provides an excellent description of electromagnetic phenomena whenever the relevant length scales and field strengths are large enough that quantum mechanical effects are negligible. For small distances and low field strengths such interactions are better described by quantum electrodynamics. Fundamental physical aspects of classical electrodynamics are presented in many texts such as those by Feynman Leighton and Sands Panofsky and Phillips and Jackson.  Electromagnetic Field Theory by Bo Thid"}, {"topic": "Electrolytic cell", "content": "An electrolytic cell is an electrochemical cell that undergoes a redox reaction when electrical energy is applied. It is most often used to decompose chemical compounds in a process called electrolysisthe Greek word lysis means to break up. When electrical energy is added to the system the chemical energy is increased. Similarly to a galvanic cell electrolytic cells usually consist of two half cells. Important examples of electrolysis are the decomposition of water into hydrogen and oxygen and bauxite into aluminium and other chemicals. Electroplating e.g. of copper silver nickel or chromium is performed using an electrolytic cell. An electrolytic cell has three component parts an electrolyte and two electrodes a cathode and an anode. The electrolyte is usually a solution of water or other solvents in which ions are dissolved. Molten salts such as sodium chloride are also electrolytes. When driven by an external voltage applied to the electrodes the ions in the electrolyte are attracted to an electrode with the opposite charge chargetransferring also called faradaic or redox reactions can take place. Only with an external electrical potential i.e. voltage of correct polarity and sufficient magnitude can an electrolytic cell decompose a normally stable or inert chemical compound in the solution. The electrical energy provided can produce a chemical reaction which would not occur spontaneously otherwise.  Concentration cell Electrochemical cell Galvanic cell"}, {"topic": "Electromagnet", "content": "An electromagnet is a type of magnet in which the magnetic field is produced by an electric current. The magnetic field disappears when the current is turned off. Electromagnets usually consist of a large number of closely spaced turns of wire that create the magnetic field. The wire turns are often wound around a magnetic core made from a ferromagnetic or ferrimagnetic material such as iron the magnetic core concentrates the magnetic flux and makes a more powerful magnet. The main advantage of an electromagnet over a permanent magnet is that the magnetic field can be quickly changed by controlling the amount of electric current in the winding. However unlike a permanent magnet that needs no power an electromagnet requires a continuous supply of current to maintain the magnetic field. Electromagnets are widely used as components of other electrical devices such as motors generators relays loudspeakers hard disks MRI machines scientific instruments and magnetic separation equipment. Electromagnets are also employed in industry for picking up and moving heavy iron objects such as scrap iron and steel.  Magnets from Mini to Mighty Primer on electromagnets and other magnets National High Magnetic Field Laboratory Magnetic Fields and Forces Cuyahoga Community College Fundamental Relationships School of Geology and Geophysics University of Oklahoma"}, {"topic": "Electromagnetic field", "content": "An electromagnetic field also EM field is a physical field produced by electrically charged objects. It affects the behavior of charged objects in the vicinity of the field. The electromagnetic field extends indefinitely throughout space and describes the electromagnetic interaction. It is one of the four fundamental forces of nature the others are gravitation weak interaction and strong interaction. The field can be viewed as the combination of an electric field and a magnetic field. The electric field is produced by stationary charges and the magnetic field by moving charges currents these two are often described as the sources of the field. The way in which charges and currents interact with the electromagnetic field is described by Maxwells equations and the Lorentz force law. From a classical perspective in the history of electromagnetism the electromagnetic field can be regarded as a smooth continuous field propagated in a wavelike manner whereas from the perspective of quantum field theory the field is seen as quantized being composed of individual particles.  On the Electrodynamics of Moving Bodies by Albert Einstein June 30 1905. On the Electrodynamics of Moving Bodies pdf NonIonizing Radiation Part 1 Static and Extremely LowFrequency ELF Electric and Magnetic Fields 2002 by the IARC. Zhang J Clement D Taunton J January 2000. The efficacy of Farabloc an electromagnetic shield in attenuating delayedonset muscle soreness. Clin J Sport Med 10 1 1521. PMID 10695845. National Institute for Occupational Safety and Health EMF Topic Page Biological Effects of Power Frequency Electric and Magnetic Fields May 1989 110 pages prepared for US Congress Office of Technology Assessment by Indira Nair M.Granger Morgan Keith Florig Department of Engineering and Public Policy Carnegie Mellon University"}, {"topic": "Electromagnetic induction", "content": "Electromagnetic or Magnetic induction is the production of an electromotive force or voltage across an electrical conductor due to its dynamic interaction with a magnetic field. Michael Faraday is generally credited with the discovery of induction in 1831 and mathematically described it as Faradays law of induction. Lenzs law describes the direction of the induced field. Faradays law was later generalized to the MaxwellFaraday equation which is one of the equations in James Clerk Maxwells theory of electromagnetism. Electromagnetic induction has found many applications in technology including electrical components such as inductors and transformers and devices such as electric motors and generators.  Magnet academy A simple interactive Java tutorial on electromagnetic induction National High Magnetic Field Laboratory R. Vega Induction Faradays law and Lenzs law  Highly animated lecture Faradays Law for EMC Engineers Tankersley and Mosca Introducing Faradays law A free java simulation on motional EMF"}, {"topic": "Electromagnetic radiation", "content": "Electromagnetic radiation EM radiation or EMR is the radiant energy released by certain electromagnetic processes. Visible light is an electromagnetic radiation. Other familiar electromagnetic radiations are invisible to the human eye such as radio waves infrared light and Xrays. Classically electromagnetic radiation consists of electromagnetic waves which are synchronized oscillations of electric and magnetic fields that propagate at the speed of light through a vacuum. The oscillations of the two fields are perpendicular to each other and perpendicular to the direction of energy and wave propagation forming a transverse wave. Electromagnetic waves can be characterized by either the frequency or wavelength of their oscillations to form the electromagnetic spectrum which includes in order of increasing frequency and decreasing wavelength radio waves microwaves infrared radiation visible light ultraviolet radiation Xrays and gamma rays. Electromagnetic waves are produced whenever charged particles are accelerated and these waves can subsequently interact with any charged particles. EM waves carry energy momentum and angular momentum away from their source particle and can impart those quantities to matter with which they interact. Quanta of EM waves are called photons which are massless but they are still affected by gravity. Electromagnetic radiation is associated with those EM waves that are free to propagate themselves radiate without the continuing influence of the moving charges that produced them because they have achieved sufficient distance from those charges. Thus EMR is sometimes referred to as the far field. In this language the near field refers to EM fields near the charges and current that directly produced them specifically electromagnetic induction and electrostatic induction phenomena. In the quantum theory of electromagnetism EMR consists of photons the elementary particles responsible for all electromagnetic interactions. Quantum effects provide additional sources of EMR such as the transition of electrons to lower energy levels in an atom and blackbody radiation. The energy of an individual photon is quantized and is greater for photons of higher frequency. This relationship is given by Plancks equation E  h where E is the energy per photon is the frequency of the photon and h is Plancks constant. A single gamma ray photon for example might carry 100000 times the energy of a single photon of visible light. The effects of EMR upon biological systems and also to many other chemical systems under standard conditions depend both upon the radiations power and its frequency. For EMR of visible frequencies or lower i.e. radio microwave infrared the damage done to cells and other materials is determined mainly by power and caused primarily by heating effects from the combined energy transfer of many photons. By contrast for ultraviolet and higher frequencies i.e. Xrays and gamma rays chemical materials and living cells can be further damaged beyond that done by simple heating since individual photons of such high frequency have enough energy to cause direct molecular damage.  Electromagnetism a chapter from an online textbook Electromagnetic Waves from Maxwells Equations on Project PHYSNET. Radiation of atoms em wave Polarisation ... An Introduction to The Wigner Distribution in Geometric Optics The windows of the electromagnetic spectrum on Astronoo Introduction to light and electromagnetic radiation course video from the Khan Academy Lectures on electromagnetic waves course video and notes from MIT Professor Walter Lewin Encyclopedia Britannica Electromagnetic Radiation Physics for the 21st Century Early Unification for Electromagnetism HarvardSmithsonian Center for Astrophysics"}, {"topic": "Electromagnetic spectrum", "content": "The electromagnetic spectrum is the collective term for all known frequencies and their linked wave lengths of the known photons  electromagnetic radiation . The electromagnetic spectrum of an object has a different meaning and is instead the characteristic distribution of electromagnetic radiation emitted or absorbed by that particular object. The electromagnetic spectrum extends from below the low frequencies used for modern radio communication to gamma radiation at the shortwavelength highfrequency end thereby covering wavelengths from thousands of kilometers down to a fraction of the size of an atom. Visible light lies toward the shorter end with wavelengths from 400 to 700 nanometres. The limit for long wavelengths is the size of the universe itself while it is thought that the short wavelength limit is in the vicinity of the Planck length. Until the middle of the 20th century it was believed by most physicists that this spectrum was infinite and continuous. Nearly all types of electromagnetic radiation can be used for spectroscopy to study and characterize matter. Other technological uses are described under electromagnetic radiation.  UnwantedEmissions.com U.S. radio spectrum allocations resource Australian Radiofrequency Spectrum Allocations Chart from Australian Communications and Media Authority Canadian Table of Frequency Allocations from Industry Canada U.S. Frequency Allocation Chart Covering the range 3 kHz to 300 GHz from Department of Commerce UK frequency allocation table from Ofcom which inherited the Radiocommunications Agencys duties pdf format Flash EM Spectrum Presentation  Tool Very complete and customizable. How to render the color spectrum  Code Only approximately right. Poster Electromagnetic Radiation Spectrum 992 kB Electromagnetic Spectrum presentation Electromagnetic Spectrum Strategy A Call to Action U.S. Department of Defense"}, {"topic": "Electromagnetic wave equation", "content": "The electromagnetic wave equation is a secondorder partial differential equation that describes the propagation of electromagnetic waves through a medium or in a vacuum. It is a threedimensional form of the wave equation. The homogeneous form of the equation written in terms of either the electric field E or the magnetic field B takes the form  c 2 2 2 t 2  E  0  c 2 2 2 t 2  B  0  These can be rewritten in terms of the spherical Bessel function. In cylindrical coordinates the solutions to the wave equation are the ordinary Bessel function of integer order.  P. C. Matthews Vector Calculus Springer 1998 ISBN 3540761802 H. M. Schey Div Grad Curl and all that An informal text on vector calculus 4th edition W. W. Norton  Company 2005 ISBN 0393925161."}, {"topic": "Electromagnetism", "content": "Electromagnetism is a branch of physics which involves the study of the electromagnetic force a type of physical interaction that occurs between electrically charged particles. The electromagnetic force usually exhibits electromagnetic fields such as electric fields magnetic fields and light. The electromagnetic force is one of the four fundamental interactions commonly called forces in nature. The other three fundamental interactions are the strong interaction the weak interaction and gravitation. The word electromagnetism is a compound form of two Greek terms  lektron amber and magntis lithos which means magnesian stone a type of iron ore. Electromagnetic phenomena is defined in terms of the electromagnetic force sometimes called the Lorentz force which includes both electricity and magnetism as different manifestations of the same phenomenon. The electromagnetic force plays a major role in determining the internal properties of most objects encountered in daily life. Ordinary matter takes its form as a result of intermolecular forces between individual atoms and molecules in matter and are a manifestation of the electromagnetic force. Electrons are bound by the electromagnetic force to atomic nuclei and their orbital shapes and their influence on nearby atoms with their electrons is described by quantum mechanics. The electromagnetic force governs the processes involved in chemistry which arise from interactions between the electrons of neighboring atoms. There are numerous mathematical descriptions of the electromagnetic field. In classical electrodynamics electric fields are described as electric potential and electric current. In Faradays law magnetic fields are associated with electromagnetic induction and magnetism and Maxwells equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. The theoretical implications of electromagnetism in particular the establishment of the speed of light based on properties of the medium of propagation permeability and permittivity led to the development of special relativity by Albert Einstein in 1905. Although electromagnetism is considered one of the four fundamental forces at high energy the weak force and electromagnetic force are unified as a single electroweak force. In the history of the universe during the quark epoch the unified force broke into the two separate forces as the universe cooled.  Oppelt Arnulf 20061102. magnetic field strength. Retrieved 20070604. magnetic field strength converter. Retrieved 20070604. Electromagnetic Force  from Eric Weissteins World of Physics Goudarzi Sara 20060815. Ties That Bind Atoms Weaker Than Thought. LiveScience.com. Retrieved 20131112. Quarked Electromagnetic force  A good introduction for kids The Deflection of a Magnetic Compass Needle by a Current in a Wire video on YouTube Electromagnetism abridged"}, {"topic": "Electromechanics", "content": "In engineering electromechanics combines electrical and mechanical processes and procedures drawn from electrical engineering and mechanical engineering. Electrical engineering in this context also encompasses electronics engineering. Devices which carry out electrical operations by using moving parts are known as electromechanical. Strictly speaking a manually operated switch is an electromechanical component but the term is usually understood to refer to devices which involve an electrical signal to create mechanical movement or mechanical movement to create an electric signal. Often involving electromagnetic principles such as in relays which allow a voltage or current to control other usually isolated circuit voltage or current by mechanically switching sets of contacts and solenoids by which a voltage can actuate a moving linkage as in solenoid valves. Piezoelectric devices are electromechanical but do not use electromagnetic principles. Piezoelectric devices can create sound or vibration from an electrical signal or create an electrical signal from sound or mechanical vibration. Before the development of modern electronics electromechanical devices were widely used in complicated systems subsystems including electric typewriters teleprinters very early television systems and the very early electromechanical digital computers.  General Davim J. Paulo editor 2011 Mechatronics John Wiley  Sons ISBN 9781848213081 . Furlani Edward P. August 15 2001. Permanent Magnet and Electromechanical Devices Materials Analysis and Applications. Academic Press Series in Electromagnetism. San Diego Academic Press. ISBN 0122699513. OCLC 47726317. Krause Paul C. Wasynczuk Oleg 1989. Electromechanical Motion Devices. McGrawHill Series in Electrical and Computer Engineering. New York McGrawHill. ISBN 0070354944. OCLC 18224514. Citations"}, {"topic": "Electron pair", "content": "In chemistry an electron pair or a Lewis pair consists of two electrons that occupy the same orbital but have opposite spins. The electron pair concept was introduced in a 1916 paper of Gilbert N. Lewis. Because electrons are fermions the Pauli exclusion principle forbids these particles from having exactly the same quantum numbers. Therefore the only way to occupy the same orbital i.e. have the same orbital quantum numbers is to differ in the spin quantum number. This limits the number of electrons in the same orbital to exactly two. The pairing of spins is often energetically favorable and electron pairs therefore play a very large role in chemistry. They can form a chemical bond between two atoms or they can occur as a lone pair of valence electrons. They also fill the core levels of an atom. Because the spins are paired the magnetic moment of the electrons cancels and the contribution of the pair to the magnetic properties will in general be a diamagnetic one. Although a strong tendency to pair off electrons can be observed in chemistry it is also possible that electrons occur as unpaired electrons. In the case of metallic bonding the magnetic moments also compensate to a large extent but the bonding is more communal so that individual pairs of electrons cannot be distinguished and it is better to consider the electrons as a collective ocean. A very special case of electron pair formation occurs in superconductivity the formation of Cooper pairs. "}, {"topic": "Electronegativity", "content": "Electronegativity symbol  is a chemical property that describes the tendency of an atom or a functional group to attract electrons or electron density towards itself. An atoms electronegativity is affected by both its atomic number and the distance at which its valence electrons reside from the charged nucleus. The higher the associated electronegativity number the more an element or compound attracts electrons towards it. The term electronegativity was introduced by Jns Jacob Berzelius in 1811 though the concept was known even before that and was studied by many chemists including Avogadro. In spite of its long history an accurate scale of electronegativity had to wait till 1932 when Linus Pauling proposed an electronegativity scale which depends on bond energies as a development of valence bond theory. It has been shown to correlate with a number of other chemical properties. Electronegativity cannot be directly measured and must be calculated from other atomic or molecular properties. Several methods of calculation have been proposed and although there may be small differences in the numerical values of the electronegativity all methods show the same periodic trends between elements. The most commonly used method of calculation is that originally proposed by Linus Pauling. This gives a dimensionless quantity commonly referred to as the Pauling scale on a relative scale running from around 0.7 to 3.98 hydrogen  2.20. When other methods of calculation are used it is conventional although not obligatory to quote the results on a scale that covers the same range of numerical values this is known as an electronegativity in Pauling units. As it is usually calculated electronegativity is not a property of an atom alone but rather a property of an atom in a molecule. Properties of a free atom include ionization energy and electron affinity. It is to be expected that the electronegativity of an element will vary with its chemical environment but it is usually considered to be a transferable property that is to say that similar values will be valid in a variety of situations. On the most basic level electronegativity is determined by factors like the nuclear charge the more protons an atom has the more pull it will have on electrons and the numberlocation of other electrons present in the atomic shells the more electrons an atom has the farther from the nucleus the valence electrons will be and as a result the less positive charge they will experienceboth because of their increased distance from the nucleus and because the other electrons in the lower energy core orbitals will act to shield the valence electrons from the positively charged nucleus. The opposite of electronegativity is electropositivity a measure of an elements ability to donate electrons. Caesium is the least electronegative element in the periodic table 0.79 while fluorine is most electronegative 3.98. Francium and caesium were originally both assigned 0.7 caesiums value was later refined to 0.79 but no experimental data allows a similar refinement for francium. However franciums ionization energy is known to be slightly higher than caesiums in accordance with the relativistic stabilization of the 7s orbital and this in turn implies that caesium is in fact more electronegative than francium.  WebElements lists values of electronegativities by a number of different methods of calculation Video explaining electronegativity"}, {"topic": "Electronics", "content": "Electronics is the science of how to control electric energy energy in which the electrons have a fundamental role. Electronics deals with electrical circuits that involve active electrical components such as vacuum tubes transistors diodes and integrated circuits and associated passive electrical components and interconnection technologies. Commonly electronic devices contain circuitry consisting primarily or exclusively of active semiconductors supplemented with passive elements such a circuit is described as an electronic circuit. The nonlinear behaviour of active components and their ability to control electron flows makes amplification of weak signals possible and electronics is widely used in information processing telecommunication and signal processing. The ability of electronic devices to act as switches makes digital information processing possible. Interconnection technologies such as circuit boards electronics packaging technology and other varied forms of communication infrastructure complete circuit functionality and transform the mixed components into a regular working system. Electronics is distinct from electrical and electromechanical science and technology which deal with the generation distribution switching storage and conversion of electrical energy to and from other energy forms using wires motors generators batteries switches relays transformers resistors and other passive components. This distinction started around 1906 with the invention by Lee De Forest of the triode which made electrical amplification of weak radio signals and audio signals possible with a nonmechanical device. Until 1950 this field was called radio technology because its principal application was the design and theory of radio transmitters receivers and vacuum tubes. Today most electronic devices use semiconductor components to perform electron control. The study of semiconductor devices and related technology is considered a branch of solidstate physics whereas the design and construction of electronic circuits to solve practical problems come under electronics engineering. This article focuses on engineering aspects of electronics.  Electronics at DMOZ Navy 1998 Navy Electricity and Electronics Training Series NEETS DOE 1998 Electrical Science Fundamentals Handbook 4 vols. Vol. 1 Basic Electrical Theory Basic DC Theory Vol. 2 DC Circuits Batteries Generators Motors Vol. 3 Basic AC Theory Basic AC Reactive Components Basic AC Power Basic AC Generators Vol. 4 AC Motors Transformers Test Instruments  Measuring Devices Electrical Distribution Systems"}, {"topic": "Electronvolt", "content": "In physics the electronvolt symbol eV also written electron volt is a unit of energy equal to approximately 160 zeptojoules 1021 joules symbol zJ or 69811600000000000001.61019 joules symbol J. By definition it is the amount of energy gained or lost by the charge of a single electron moving across an electric potential difference of one volt. Thus it is 1 volt 1 joule per coulomb 70001000000000000001 JC multiplied by the elementary charge e or 69811602176620799991.6021766208981019 C. Therefore one electronvolt is equal to 69811602176620799991.6021766208981019 J. Historically the electronvolt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences because a particle with charge q has an energy E  qV after passing through the potential V if q is quoted in integer units of the elementary charge and the terminal bias in volts one gets an energy in eV. The electronvolt is not an SI unit and its definition is empirical unlike the litre the light year and other such nonSI units thus its value in SI units must be obtained experimentally. Like the elementary charge on which it is based it is not an independent quantity but is equal to 1 JC2h0c0. It is a common unit of energy within physics widely used in solid state atomic nuclear and particle physics. It is commonly used with the metric prefixes milli kilo mega giga tera peta or exa meV keV MeV GeV TeV PeV and EeV respectively. Thus meV stands for millielectronvolt. In some older documents and in the name Bevatron the symbol BeV is used which stands for billion electronvolts it is equivalent to the GeV.  BIPMs definition of the electronvolt physical constants reference CODATA data"}, {"topic": "Elementary particle", "content": "In particle physics an elementary particle or fundamental particle is a particle whose substructure is unknown thus it is unknown whether it is composed of other particles. Known elementary particles include the fundamental fermions quarks leptons antiquarks and antileptons which generally are matter particles and antimatter particles as well as the fundamental bosons gauge bosons and the Higgs boson which generally are force particles that mediate interactions among fermions. A particle containing two or more elementary particles is a composite particle. Everyday matter is composed of atoms once presumed to be matters elementary particlesatom meaning unable to cut in Greekalthough the atoms existence remained controversial until about 1910 as some leading physicists regarded molecules as mathematical illusions and matter as ultimately composed of energy. Soon subatomic constituents of the atom were identified. As the 1930s opened the electron and the proton had been observed along with the photon the particle of electromagnetic radiation. At that time the recent advent of quantum mechanics was radically altering the conception of particles as a single particle could seemingly span a field as would a wave a paradox still eluding satisfactory explanation. Via quantum theory protons and neutrons were found to contain quarksup quarks and down quarksnow considered elementary particles. And within a molecule the electrons three degrees of freedom charge spin orbital can separate via wavefunction into three quasiparticles holon spinon orbiton. Yet a free electronwhich not orbiting an atomic nucleus lacks orbital motionappears unsplittable and remains regarded as an elementary particle. Around 1980 an elementary particles status as indeed elementaryan ultimate constituent of substancewas mostly discarded for a more practical outlook embodied in particle physics Standard Model sciences most experimentally successful theory. Many elaborations upon and theories beyond the Standard Model including the extremely popular supersymmetry double the number of elementary particles by hypothesizing that each known particle associates with a shadow partner far more massive although all such superpartners remain undiscovered. Meanwhile an elementary boson mediating gravitationthe gravitonremains hypothetical.  The most important address about the current experimental and theoretical knowledge about elementary particle physics is the Particle Data Group where different international institutions collect all experimental data and give short reviews over the contemporary theoretical understanding. Particle Data Group other pages are Greene Brian Elementary particles The Elegant Universe NOVA PBS particleadventure.org a wellmade introduction also for non physicists CERNCourier Season of Higgs and melodrama Pentaquark information page Interactions.org particle physics news Symmetry Magazine a joint FermilabSLAC publication Sized Matter perception of the extreme unseen Michigan University project for artistic visualisation of subatomic particles Elementary Particles made thinkable an interactive visualisation allowing physical properties to be compared"}, {"topic": "Emission spectrum", "content": "The emission spectrum of a chemical element or chemical compound is the spectrum of frequencies of electromagnetic radiation emitted due to an atom or molecule making a transition from a high energy state to a lower energy state. The photon energy of the emitted photon is equal to the energy difference between the two states. There are many possible electron transitions for each atom and each transition has a specific energy difference. This collection of different transitions leading to different radiated wavelengths make up an emission spectrum. Each elements emission spectrum is unique. Therefore spectroscopy can be used to identify the elements in matter of unknown composition. Similarly the emission spectra of molecules can be used in chemical analysis of substances.  Emission spectra of atmospheric gases NIST Physical Reference DataAtomic Spectroscopy Data Color Simulation of Element Emission Spectrum Based on NIST data Hydrogen emission spectrum Emissions Spectrum Java Applet Astrophysics lecture slides on the emission coefficient from University of Chicago."}, {"topic": "Endothermic", "content": "The term endothermic process describes a process or reaction in which the system absorbs energy from its surroundings usually but not always in the form of heat. The term was coined by Marcellin Berthelot from the Greek roots endo derived from the word endon  meaning within and the root therm  meaning hot. The intended sense is that of a reaction that depends on absorbing heat if it is to proceed. The opposite of an endothermic process is an exothermic process one that releases gives out energy in the form of usually but not always heat. Thus in each term endothermic  exothermic the prefix refers to where heat goes as the reaction occurs though in reality it only refers to where the energy goes without necessarily being in the form of heat. The concept is frequently applied in physical sciences to for example chemical reactions where thermal energy heat is converted to chemical bond energy. Endothermic and exothermic analysis only accounts for the enthalpy change H of a reaction. The full energy analysis of a reaction is the Gibbs free energy G which includes an entropy S and temperature term in addition to the enthalpy. A reaction will be a spontaneous process at a certain temperature if the products have a lower Gibbs free energy an exergonic reaction even if the enthalpy of the products is higher. Entropy and enthalpy are different terms so the change in entropic energy can overcome an opposite change in enthalpic energy and make an endothermic reaction favorable.  Endothermic Definition MSDS HyperGlossary"}, {"topic": "Energy", "content": "In physics energy is a property of objects which can be transferred to other objects or converted into different forms. The ability of a system to perform work is a common description but it is misleading because energy is not necessarily available to do work. For instance in SI units energy is measured in joules and one joule is defined mechanically being the energy transferred to an object by the mechanical work of moving it a distance of 1 metre against a force of 1 newton. However there are many other definitions of energy depending on the context such as thermal energy radiant energy electromagnetic nuclear etc. where definitions are derived that are the most convenient. Common energy forms include the kinetic energy of a moving object the potential energy stored by an objects position in a force field gravitational electric or magnetic the elastic energy stored by stretching solid objects the chemical energy released when a fuel burns the radiant energy carried by light and the thermal energy due to an objects temperature. All of the many forms of energy are convertible to other kinds of energy. In Newtonian physics there is a universal law of conservation of energy which says that energy can be neither created nor be destroyed however it can change from one form to another. For closed systems with no external source or sink of energy the first law of thermodynamics states that a systems energy is constant unless energy is transferred in or out by mechanical work or heat and that no energy is lost in transfer. This means that it is impossible to create or destroy energy. While heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system. Examples of energy transformation include generating electric energy from heat energy via a steam turbine or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy its total mass does not decrease due to that in itself since it still contains the same total energy even if in different forms but its mass does decrease when the energy escapes out to its surroundings largely as radiant energy. Mass and energy are closely related. According to the theory of massenergy equivalence any object that has mass when stationary in a frame of reference called rest mass also has an equivalent amount of energy whose form is called rest energy in that frame and any additional energy acquired by the object above that rest energy will increase an objects mass. For example with a sensitive enough scale one could measure an increase in mass after heating an object. Living organisms require available energy to stay alive such as the energy humans get from food. Civilisation gets the energy it needs from energy resources such as fossil fuels nuclear fuel or renewable energy. The processes of Earths climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.  Energy at DMOZ"}, {"topic": "Engine", "content": "An engine or motor is a machine designed to convert one form of energy into mechanical energy. Heat engines including internal combustion engines and external combustion engines such as steam engines burn a fuel to create heat which then creates a force. Electric motors convert electrical energy into mechanical motion pneumatic motors use compressed air and otherssuch as clockwork motors in windup toysuse elastic energy. In biological systems molecular motors like myosins in muscles use chemical energy to create forces and eventually motion.  U.S. Patent 194047 Detailed Engine Animations Video from inside a fourstroke engine cylinder. Working 4Stroke Engine  Animation Animated illustrations of various engines 5 Ways to Redesign the Internal Combustion Engine"}, {"topic": "Engineering physics", "content": "Engineering physics refers to the study of the combined disciplines of physics mathematics and combined with engineering studies in computer electrical materials or mechanical engineering. By focussing on the scientific method as a rigorous basis it seeks ways to apply and develop new solutions in engineering. Engineering physics or engineering science degrees are respected academic degrees awarded in many countries. It can be taught at the undergraduate level and is often designed as an honors program at some universities due to the rigorous nature of the academic curriculum which covers a wide spectrum of scientific disciplines.  US News Ranking of Undergraduate Programs in Engineering ScienceEngineering Physics"}, {"topic": "Equipartition", "content": "In classical statistical mechanics the equipartition theorem is a general formula that relates the temperature of a system with its average energies. The equipartition theorem is also known as the law of equipartition equipartition of energy or simply equipartition. The original idea of equipartition was that in thermal equilibrium energy is shared equally among all of its various forms for example the average kinetic energy per degree of freedom in the translational motion of a molecule should equal that of its rotational motions. The equipartition theorem makes quantitative predictions. Like the virial theorem it gives the total average kinetic and potential energies for a system at a given temperature from which the systems heat capacity can be computed. However equipartition also gives the average values of individual components of the energy such as the kinetic energy of a particular particle or the potential energy of a single spring. For example it predicts that every atom in a monatomic ideal gas has an average kinetic energy of 32kBT in thermal equilibrium where kB is the Boltzmann constant and T is the thermodynamic temperature. More generally it can be applied to any classical system in thermal equilibrium no matter how complicated. The equipartition theorem can be used to derive the ideal gas law and the DulongPetit law for the specific heat capacities of solids. It can also be used to predict the properties of stars even white dwarfs and neutron stars since it holds even when relativistic effects are considered. Although the equipartition theorem makes very accurate predictions in certain conditions it becomes inaccurate when quantum effects are significant such as at low temperatures. When the thermal energy kBT is smaller than the quantum energy spacing in a particular degree of freedom the average energy and heat capacity of this degree of freedom are less than the values predicted by equipartition. Such a degree of freedom is said to be frozen out when the thermal energy is much smaller than this spacing. For example the heat capacity of a solid decreases at low temperatures as various types of motion become frozen out rather than remaining constant as predicted by equipartition. Such decreases in heat capacity were among the first signs to physicists of the 19th century that classical physics was incorrect and that a new more subtle scientific model was required. Along with other evidence equipartitions failure to model blackbody radiationalso known as the ultraviolet catastropheled Max Planck to suggest that energy in the oscillators in an object which emit light were quantized a revolutionary hypothesis that spurred the development of quantum mechanics and quantum field theory.  Applet demonstrating equipartition in real time for a mixture of monatomic and diatomic gases The equipartition theorem in stellar physics written by Nir J. Shaviv an associate professor at the Racah Institute of Physics in the Hebrew University of Jerusalem."}, {"topic": "Escape velocity", "content": "In physics escape velocity is the minimum speed needed for an object to break free from the gravitational attraction of a massive body. The escape velocity from Earth is about 40270 kmh 25020 mph. More generally escape velocity is the speed at which the sum of an objects kinetic energy and its gravitational potential energy is equal to zero. Given escape velocity perpendicular to a massive body the object will move away from the body slowing forever and approaching but never reaching zero speed. Once escape velocity is achieved no further impulse need be applied for it to continue in its escape. In other words if given escape velocity the object will move away from the other body continually slowing and will asymptotically approach zero speed as the objects distance approaches infinity never to return. For a spherically symmetric massive body such as a star or planet the escape velocity for that body at a given distance is calculated by the formula v e  2 G M r    Escape velocity calculator Webbased numerical escape velocity calculator"}, {"topic": "Euclidean geometry", "content": "Euclidean geometry is a mathematical system attributed to the Alexandrian Greek mathematician Euclid which he described in his textbook on geometry the Elements. Euclids method consists in assuming a small set of intuitively appealing axioms and deducing many other propositions theorems from these. Although many of Euclids results had been stated by earlier mathematicians Euclid was the first to show how these propositions could fit into a comprehensive deductive and logical system. The Elements begins with plane geometry still taught in secondary school as the first axiomatic system and the first examples of formal proof. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory explained in geometrical language. For more than two thousand years the adjective Euclidean was unnecessary because no other sort of geometry had been conceived. Euclids axioms seemed so intuitively obvious with the possible exception of the parallel postulate that any theorem proved from them was deemed true in an absolute often metaphysical sense. Today however many other selfconsistent nonEuclidean geometries are known the first ones having been discovered in the early 19th century. An implication of Albert Einsteins theory of general relativity is that physical space itself is not Euclidean and Euclidean space is a good approximation for it only where the gravitational field is weak. Euclidean geometry is an example of synthetic geometry in that it proceeds logically from axioms to propositions without the use of coordinates. This is in contrast to analytic geometry which uses coordinates.  Hazewinkel Michiel ed. 2001 Euclidean geometry Encyclopedia of Mathematics Springer ISBN 9781556080104 Hazewinkel Michiel ed. 2001 Plane trigonometry Encyclopedia of Mathematics Springer ISBN 9781556080104 Kiran Kedlaya Geometry Unbound a treatment using analytic geometry PDF format GFDL licensed"}, {"topic": "Exothermic", "content": "In thermodynamics the term exothermic process exo  outside describes a process or reaction that releases energy from the system to its surroundings usually in the form of heat but also in a form of light e.g. a spark flame or flash electricity e.g. a battery or sound e.g. explosion heard when burning hydrogen. Its etymology stems from the Greek prefix ex which means outwards and the Greek word thermiks which means thermal. The term exothermic was first coined by Marcellin Berthelot. The opposite of an exothermic process is an endothermic process one that absorbs energy in the form of heat. The concept is frequently applied in the physical sciences to chemical reactions where as in chemical bond energy that will be converted to thermal energy heat. Exothermic and endothermic describe two types of chemical reactions or systems found in nature as follows. Simply stated after an exothermic reaction more energy has been released to the surroundings than was absorbed to initiate and maintain the reaction. An example would be the burning of a candle wherein the sum of calories produced by combustion found by looking at radiant heating of the surroundings and visible light produced including increase in temperature of the fuel wax itself which with oxygen have become hot CO2 and water vapor exceeds the number of calories absorbed initially in lighting the flame and in the flame maintaining itself. i.e. some energy produced by combustion is reabsorbed and used in melting then vaporizing the wax etc. but is far outstripped by the energy produced in breaking carbonhydrogen bonds and combination of oxygen with the resulting carbon and hydrogen. On the other hand in an endothermic reaction or system energy is taken from the surroundings in the course of the reaction. An example of an endothermic reaction is a first aid cold pack in which the reaction of two chemicals or dissolving of one in another requires calories from the surroundings and the reaction cools the pouch and surroundings by absorbing heat from them. An endothermic system is seen in the production of wood trees absorb radiant energy from the sun use it in endothermic reactions such as taking apart CO2 and H2O and combining the carbon and hydrogen generated to produce cellulose and other organic chemicals. These products in the form of wood say may later be burned in a fireplace exothermically producing CO2 and water and releasing energy in the form of heat and light to their surroundings e.g. to a homes interior and chimney gasses.  httpchemistry.about.comba184556.htm Observe exothermic reactions in a simple experiment"}, {"topic": "Experimental physics", "content": "Experimental physics is the category of disciplines and subdisciplines in the field of physics that are concerned with the observation of physical phenomena and experiments. Methods vary from discipline to discipline from simple experiments and observations such as the Cavendish experiment to more complicated ones such as the Large Hadron Collider.  Taylor John R. 1987. An Introduction to Error Analysis 2nd ed.. University Science Books. ISBN 093570275X."}, {"topic": "Falling bodies", "content": "A set of dynamical equations describe the resultant trajectories when objects move owing to a constant gravitational force under normal Earthbound conditions. For example Newtons law of universal gravitation simplifies to F  mg where m is the mass of the body. This assumption is reasonable for objects falling to earth over the relatively short vertical distances of our everyday experience but is very much untrue over larger distances such as spacecraft trajectories. Please note that in this article any resistance from air drag is neglected.  Falling body equations calculator"}, {"topic": "Farad", "content": "The farad symbol F is the SI derived unit of electrical capacitance the ability of a body to store an electrical charge. It is named after the English physicist Michael Faraday.  Farad unit conversion tool"}, {"topic": "Faraday (unit)", "content": "In physics and chemistry the Faraday constant denoted by the symbol F and named after Michael Faraday is the magnitude of electric charge per mole of electrons. It has the currently accepted value 700496485332890000096485.3328959 C mol1. The constant F has a simple relation to two other physical constants F  e N A  where e 69811602176600000001.60217661019 C NA 70236022141000000006.0221411023 mol1. NA is the Avogadro constant the ratio of the number of particles N which is unitless to the amount of substance n in units of moles and e is the elementary charge or the magnitude of the charge of an electron. This relation is true because the amount of charge of a mole of electrons is equal to the amount of charge in one electron multiplied by the number of electrons in a mole. One common use of the faraday constant is electrolysis. One can divide the amount of charge in coulombs by the Faraday constant in order to find the amount in moles of the element that has been oxidized. The value of F was first determined by weighing the amount of silver deposited in an electrochemical reaction in which a measured current was passed for a measured time and using Faradays law of electrolysis. Research is continuing into more accurate ways of determining the interrelated constants F NA and e. "}, {"topic": "Faraday constant", "content": "In physics and chemistry the Faraday constant denoted by the symbol F and named after Michael Faraday is the magnitude of electric charge per mole of electrons. It has the currently accepted value 700496485332890000096485.3328959 C mol1. The constant F has a simple relation to two other physical constants F  e N A  where e 69811602176600000001.60217661019 C NA 70236022141000000006.0221411023 mol1. NA is the Avogadro constant the ratio of the number of particles N which is unitless to the amount of substance n in units of moles and e is the elementary charge or the magnitude of the charge of an electron. This relation is true because the amount of charge of a mole of electrons is equal to the amount of charge in one electron multiplied by the number of electrons in a mole. One common use of the faraday constant is electrolysis. One can divide the amount of charge in coulombs by the Faraday constant in order to find the amount in moles of the element that has been oxidized. The value of F was first determined by weighing the amount of silver deposited in an electrochemical reaction in which a measured current was passed for a measured time and using Faradays law of electrolysis. Research is continuing into more accurate ways of determining the interrelated constants F NA and e. "}, {"topic": "Fermat's principle", "content": "In optics Fermats principle or the principle of least time is the principle that the path taken between two points by a ray of light is the path that can be traversed in the least time. This principle is sometimes taken as the definition of a ray of light. However this version of the principle is not general a more modern statement of the principle is that rays of light traverse the path of stationary optical length with respect to variations of the path. In other words a ray of light prefers the path such that there are other paths arbitrarily nearby on either side along which the ray would take almost exactly the same time to traverse. Fermats principle can be used to describe the properties of light rays reflected off mirrors refracted through different media or undergoing total internal reflection. It follows mathematically from Huygens principle at the limit of small wavelength. French mathematician Pierre de Fermats text Analyse des rfractions exploits the technique of adequality to derive Snells law of refraction and the law of reflection. Fermats principle has the same form as Hamiltons principle and it is the basis of Hamiltonian optics. "}, {"topic": "Fermions", "content": "In particle physics a fermion a name coined by Paul Dirac from the surname of Enrico Fermi is any particle characterized by FermiDirac statistics. These particles obey the Pauli exclusion principle. Fermions include all quarks and leptons as well as any composite particle made of an odd number of these such as all baryons and many atoms and nuclei. Fermions differ from bosons which obey BoseEinstein statistics. A fermion can be an elementary particle such as the electron or it can be a composite particle such as the proton. According to the spinstatistics theorem in any reasonable relativistic quantum field theory particles with integer spin are bosons while particles with halfinteger spin are fermions. Besides this spin characteristic fermions have another specific property they possess conserved baryon or lepton quantum numbers. Therefore what is usually referred as the spin statistics relation is in fact a spin statisticsquantum number relation. As a consequence of the Pauli exclusion principle only one fermion can occupy a particular quantum state at any given time. If multiple fermions have the same spatial probability distribution then at least one property of each fermion such as its spin must be different. Fermions are usually associated with matter whereas bosons are generally force carrier particles although in the current state of particle physics the distinction between the two concepts is unclear. Weakly interacting fermions can also display bosonic behavior under extreme conditions. At low temperature fermions show superfluidity for uncharged particles and superconductivity for charged particles. Composite fermions such as protons and neutrons are the key building blocks of everyday matter. "}, {"topic": "Ferrimagnetism", "content": "Not to be confused with ferromagnetism for an overview see magnetism In physics a ferrimagnetic material is one that has populations of atoms with opposing magnetic moments as in antiferromagnetism however in ferrimagnetic materials the opposing moments are unequal and a spontaneous magnetization remains. This happens when the populations consist of different materials or ions such as Fe2 and Fe3. Ferrimagnetism is exhibited by ferrites and magnetic garnets. The oldest known magnetic substance magnetite ironIIIII oxide Fe3O4 is a ferrimagnet it was originally classified as a ferromagnet before Nels discovery of ferrimagnetism and antiferromagnetism in 1948. Some ferrimagnetic materials are YIG yttrium iron garnet cubic ferrites composed of iron oxides and other elements such as aluminum cobalt nickel manganese and zinc hexagonal ferrites such as PbFe12O19 and BaFe12O19 and pyrrhotite Fe1xS. "}, {"topic": "Ferromagnetism", "content": "Not to be confused with Ferrimagnetism for an overview see Magnetism. Ferromagnetism is the basic mechanism by which certain materials such as iron form permanent magnets or are attracted to magnets. In physics several different types of magnetism are distinguished. Ferromagnetism including ferrimagnetism is the strongest type it is the only one that typically creates forces strong enough to be felt and is responsible for the common phenomena of magnetism in magnets encountered in everyday life. Substances respond weakly to magnetic fields with three other types of magnetism paramagnetism diamagnetism and antiferromagnetism but the forces are usually so weak that they can only be detected by sensitive instruments in a laboratory. An everyday example of ferromagnetism is a refrigerator magnet used to hold notes on a refrigerator door. The attraction between a magnet and ferromagnetic material is the quality of magnetism first apparent to the ancient world and to us today. Permanent magnets materials that can be magnetized by an external magnetic field and remain magnetized after the external field is removed are either ferromagnetic or ferrimagnetic as are the materials that are noticeably attracted to them. Only a few substances are ferromagnetic. The common ones are iron nickel cobalt and most of their alloys some compounds of rare earth metals and a few naturallyoccurring minerals such as lodestone. Ferromagnetism is very important in industry and modern technology and is the basis for many electrical and electromechanical devices such as electromagnets electric motors generators transformers and magnetic storage such as tape recorders and hard disks.  Electromagnetism a chapter from an online textbook Sandeman Karl January 2008. Ferromagnetic Materials. DoITPoMS. Dept. of Materials Sci. and Metallurgy Univ. of Cambridge. Retrieved 20080827. Detailed nonmathematical description of ferromagnetic materials with animated illustrations Magnetism Models and Mechanisms in E. Pavarini E. Koch and U. Schollwck Emergent Phenomena in Correlated Matter Jlich 2013 ISBN 9783893368846"}, {"topic": "Field (physics)", "content": "In physics a field is a physical quantity that has a value for each point in space and time. For example on a weather map the surface wind velocity is described by assigning a vector to each point on a map. Each vector represents the speed and direction of the movement of air at that point. As another example an electric field can be thought of as a condition in space emanating from an electric charge and extending throughout the whole of space. When a test electric charge is placed in this electric field the particle accelerates due to a force. Physicists have found the notion of a field to be of such practical utility for the analysis of forces that they have come to think of a force as due to a field. In the modern framework of the quantum theory of fields even without referring to a test particle a field occupies space contains energy and its presence eliminates a true vacuum. This led physicists to consider electromagnetic fields to be a physical entity making the field concept a supporting paradigm of the edifice of modern physics. The fact that the electromagnetic field can possess momentum and energy makes it very real... a particle makes a field and a field acts on another particle and the field has such familiar properties as energy content and momentum just as particles can have. In practice the strength of most fields has been found to diminish with distance to the point of being undetectable. For instance the strength of many relevant classical fields such as the gravitational field in Newtons theory of gravity or the electrostatic field in classical electromagnetism is inversely proportional to the square of the distance from the source i.e. they follow the Gausss law. One consequence is that the Earths gravitational field quickly becomes undetectable on cosmic scales. A field can be classified as a scalar field a vector field a spinor field or a tensor field according to whether the represented physical quantity is a scalar a vector a spinor or a tensor respectively. A field has a unique tensorial character in every point where it is defined i.e. a field cannot be a scalar field somewhere and a vector field somewhere else. For example the Newtonian gravitational field is a vector field specifying its value at a point in spacetime requires three numbers the components of the gravitational field vector at that point. Moreover within each category scalar vector tensor a field can be either a classical field or a quantum field depending on whether it is characterized by numbers or quantum operators respectively. In fact in this theory an equivalent representation of field is a field particle namely a boson.  Particle and Polymer Field Theories"}, {"topic": "Fluid", "content": "In physics a fluid is a substance that continually deforms flows under an applied shear stress. Fluids are a subset of the phases of matter and include liquids gases plasmas and to some extent plastic solids. Fluids are substances that have zero shear modulus or in simpler terms a fluid is a substance which cannot resist any shear force applied to it. Although the term fluid includes both the liquid and gas phases in common usage fluid is often used as a synonym for liquid with no implication that gas could also be present. For example brake fluid is hydraulic oil and will not perform its required incompressible function if there is gas in it. This colloquial usage of the term is also common in medicine and in nutrition take plenty of fluids. Liquids form a free surface that is a surface not created by the container while gases do not. The distinction between solids and fluid is not entirely obvious. The distinction is made by evaluating the viscosity of the substance. Silly Putty can be considered to behave like a solid or a fluid depending on the time period over which it is observed. It is best described as a viscoelastic fluid. There are many examples of substances proving difficult to classify. A particularly interesting one is pitch as demonstrated in the pitch drop experiment currently running at the University of Queensland.  Bird Byron Stewart Warren Lightfoot Edward 2007. Transport Phenomena. New York Wiley Second Edition. p. 912. ISBN 0471410772."}, {"topic": "Fluid mechanics", "content": "Fluid mechanics is the branch of physics that studies the mechanics of fluids liquids gases and plasmas and the forces on them. Fluid mechanics has a wide range of applications including for mechanical engineering chemical engineering geophysics astrophysics and biology. Fluid mechanics can be divided into fluid statics the study of fluids at rest and fluid dynamics the study of the effect of forces on fluid motion. It is a branch of continuum mechanics a subject which models matter without using the information that it is made out of atoms that is it models matter from a macroscopic viewpoint rather than from microscopic. Fluid mechanics especially fluid dynamics is an active field of research with many problems that are partly or wholly unsolved. Fluid mechanics can be mathematically complex and can best be solved by numerical methods typically using computers. A modern discipline called computational fluid dynamics CFD is devoted to this approach to solving fluid mechanics problems. Particle image velocimetry an experimental method for visualizing and analyzing fluid flow also takes advantage of the highly visual nature of fluid flow.  Free Fluid Mechanics books Annual Review of Fluid Mechanics CFDWiki the Computational Fluid Dynamics reference wiki. Educational Particle Image Velocimetry resources and demonstrations"}, {"topic": "Fluid physics", "content": "Fluid mechanics is the branch of physics that studies the mechanics of fluids liquids gases and plasmas and the forces on them. Fluid mechanics has a wide range of applications including for mechanical engineering chemical engineering geophysics astrophysics and biology. Fluid mechanics can be divided into fluid statics the study of fluids at rest and fluid dynamics the study of the effect of forces on fluid motion. It is a branch of continuum mechanics a subject which models matter without using the information that it is made out of atoms that is it models matter from a macroscopic viewpoint rather than from microscopic. Fluid mechanics especially fluid dynamics is an active field of research with many problems that are partly or wholly unsolved. Fluid mechanics can be mathematically complex and can best be solved by numerical methods typically using computers. A modern discipline called computational fluid dynamics CFD is devoted to this approach to solving fluid mechanics problems. Particle image velocimetry an experimental method for visualizing and analyzing fluid flow also takes advantage of the highly visual nature of fluid flow.  Free Fluid Mechanics books Annual Review of Fluid Mechanics CFDWiki the Computational Fluid Dynamics reference wiki. Educational Particle Image Velocimetry resources and demonstrations"}, {"topic": "Fluid statics", "content": "Fluid statics or hydrostatics is the branch of fluid mechanics that studies incompressible fluids at rest. It encompasses the study of the conditions under which fluids are at rest in stable equilibrium as opposed to fluid dynamics the study of fluids in motion. Hydrostatics are categorized as a part of the fluid statics which is the study of all fluids incompressible or not at rest. Hydrostatics is fundamental to hydraulics the engineering of equipment for storing transporting and using fluids. It is also relevant to geophysics and astrophysics for example in understanding plate tectonics and the anomalies of the Earths gravitational field to meteorology to medicine in the context of blood pressure and many other fields. Hydrostatics offers physical explanations for many phenomena of everyday life such as why atmospheric pressure changes with altitude why wood and oil float on water and why the surface of water is always flat and horizontal whatever the shape of its container.  J.B. Calvert 2003 Hydrostatics from University of Denver. Accessed on 20130522."}, {"topic": "Fluorescence", "content": "Fluorescence is the emission of light by a substance that has absorbed light or other electromagnetic radiation. It is a form of luminescence. In most cases the emitted light has a longer wavelength and therefore lower energy than the absorbed radiation. The most striking example of fluorescence occurs when the absorbed radiation is in the ultraviolet region of the spectrum and thus invisible to the human eye while the emitted light is in the visible region which gives the fluorescent substance a distinct color that can only be seen when exposed to UV light. However unlike phosphorescence where the substance would continue to glow and emit light for some time after the radiation source has been turned off fluorescent materials would cease to glow immediately upon removal of the excitation source. Hence it is not a persistent phenomenon. Fluorescence has many practical applications including mineralogy gemology chemical sensors fluorescence spectroscopy fluorescent labelling dyes biological detectors cosmicray detection and most commonly fluorescent lamps. Fluorescence also occurs frequently in nature in some minerals and in various biological states in many branches of the animal kingdom.  Fluorophores.org the database of fluorescent dyes FSU.edu Basic Concepts in Fluorescence A nanohistory of fluorescence lecture by David Jameson Excitation and emission spectra of various fluorescent dyes Database of fluorescent minerals with pictures activators and spectra fluomin.org Biofluorescent Night Dive DahabRed Sea Egypt Masbat BayMashraba Roman Rock. YouTube. 9 October 2012. Steffen O. Beyer. FluoPedia.org Publications. fluopedia.org. Steffen O. Beyer. FluoMedia.org Science. fluomedia.org."}, {"topic": "Flux", "content": "Flux is either of two separate simple and ubiquitous concepts throughout physics and applied mathematics. Within a discipline the term is generally used consistently but care must be taken when comparing phenomena from different disciplines. Both concepts have mathematical rigor enabling comparison of the underlying math when the terminology is unclear. For transport phenomena flux is a vector quantity describing the magnitude and direction of the flow of a substance or property. In electromagnetism flux is a scalar quantity defined as the surface integral of the component of a vector field perpendicular to the surface at each point. As will be made clear the easiest way to relate the two concepts is that the surface integral of a flux according to the first definition is a flux according to the second definition.  The dictionary definition of flux at Wiktionary"}, {"topic": "Focus (optics)", "content": "In geometrical optics a focus also called an image point is the point where light rays originating from a point on the object converge. Although the focus is conceptually a point physically the focus has a spatial extent called the blur circle. This nonideal focusing may be caused by aberrations of the imaging optics. In the absence of significant aberrations the smallest possible blur circle is the Airy disc which is caused by diffraction from the optical systems aperture. Aberrations tend to get worse as the aperture diameter increases while the Airy circle is smallest for large apertures. An image or image point or region is in focus if light from object points is converged almost as much as possible in the image and out of focus if light is not well converged. The border between these is sometimes defined using a circle of confusion criterion. A principal focus or focal point is a special focus For a lens or a spherical or parabolic mirror it is a point onto which collimated light parallel to the axis is focused. Since light can pass through a lens in either direction a lens has two focal pointsone on each side. The distance in air from the lens or mirrors principal plane to the focus is called the focal length. Elliptical mirrors have two focal points light that passes through one of these before striking the mirror is reflected such that it passes through the other. The focus of a hyperbolic mirror is either of two points which have the property that light from one is reflected as if it came from the other. Diverging negative lenses and convex mirrors do not focus a collimated beam to a point. Instead the focus is the point from which the light appears to be emanating after it travels through the lens or reflects from the mirror. A convex parabolic mirror will reflect a beam of collimated light to make it appear as if it were radiating from the focal point or conversely reflect rays directed toward the focus as a collimated beam. A convex elliptical mirror will reflect light directed towards one focus as if it were radiating from the other focus both of which are behind the mirror. A convex hyperbolic mirror will reflect rays emanating from the focal point in front of the mirror as if they were emanating from the focal point behind the mirror. Conversely it can focus rays directed at the focal point that is behind the mirror towards the focal point that is in front of the mirror as in a Cassegrain telescope. "}, {"topic": "For Inspiration and Recognition of Science and Technology", "content": "For Inspiration and Recognition of Science and Technology FIRST is an international youth organization that operates the FIRST Robotics Competition FIRST LEGO League FIRST LEGO League Jr. and FIRST Tech Challenge competitions. Founded by Dean Kamen and Woodie Flowers in 1989 its expressed goal is to develop ways to inspire students in engineering and technology fields. It is noted for its philosophy of cooperative competition which is expressed by the organization as coopertition and gracious professionalism. FIRST also operates FIRST Place a research facility at FIRST Headquarters in Manchester New Hampshire where it holds educational programs and day camps for students and teachers.  Official website"}, {"topic": "Force", "content": "In physics a force is any interaction that when unopposed will change the motion of an object. In other words a force can cause an object with mass to change its velocity which includes to begin moving from a state of rest i.e. to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F. The original form of Newtons second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant this law implies that the acceleration of an object is directly proportional to the net force acting on the object is in the direction of the net force and is inversely proportional to the mass of the object Related concepts to force include thrust which increases the velocity of an object drag which decreases the velocity of an object and torque which produces changes in rotational speed of an object. In an extended body each part usually applies forces on the adjacent parts the distribution of such forces through the body is the socalled mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials or flow in fluids.  Video lecture on Newtons three laws by Walter Lewin from MIT OpenCourseWare A Java simulation on vector addition of forces Force demonstrated as any influence on an object that changes the objects shape or motion video"}, {"topic": "Forces", "content": "In physics a force is any interaction that when unopposed will change the motion of an object. In other words a force can cause an object with mass to change its velocity which includes to begin moving from a state of rest i.e. to accelerate. Force can also be described by intuitive concepts such as a push or a pull. A force has both magnitude and direction making it a vector quantity. It is measured in the SI unit of newtons and represented by the symbol F. The original form of Newtons second law states that the net force acting upon an object is equal to the rate at which its momentum changes with time. If the mass of the object is constant this law implies that the acceleration of an object is directly proportional to the net force acting on the object is in the direction of the net force and is inversely proportional to the mass of the object Related concepts to force include thrust which increases the velocity of an object drag which decreases the velocity of an object and torque which produces changes in rotational speed of an object. In an extended body each part usually applies forces on the adjacent parts the distribution of such forces through the body is the socalled mechanical stress. Pressure is a simple type of stress. Stress usually causes deformation of solid materials or flow in fluids.  Video lecture on Newtons three laws by Walter Lewin from MIT OpenCourseWare A Java simulation on vector addition of forces Force demonstrated as any influence on an object that changes the objects shape or motion video"}, {"topic": "Fraunhofer lines", "content": "In physics and optics the Fraunhofer lines are a set of spectral lines named after the German physicist Joseph von Fraunhofer 17871826. The lines were originally observed as dark features absorption lines in the optical spectrum of the Sun.  Fraunhofer Lines at Science.Jrank"}, {"topic": "Free fall", "content": "In Newtonian physics free fall is any motion of a body where gravity is the only force acting upon it. In the context of general relativity where gravitation is reduced to a spacetime curvature a body in free fall has no force acting on it and moves along a geodesic. The present article only concerns itself with free fall in the Newtonian domain. An object in the technical sense of free fall may not necessarily be falling down in the usual sense of the term. An object moving upwards would not normally be considered to be falling but if it is subject to the force of gravity only it is said to be in free fall. The moon is thus in free fall. In a uniform gravitational field in the absence of any other forces gravitation acts on each part of the body equally and this is weightlessness a condition that also occurs when the gravitational field is zero such as when far away from any gravitating body. A body in free fall experiences 0 g. The term free fall is often used more loosely than in the strict sense defined above. Thus falling through an atmosphere without a deployed parachute or lifting device is also often referred to as free fall. The aerodynamic drag forces in such situations prevent them from producing full weightlessness and thus a skydivers free fall after reaching terminal velocity produces the sensation of the bodys weight being supported on a cushion of air.  Unplanned Freefall A slightly tongueincheek look at surviving freefall without a parachute. Free fall accidents mathematics of free fall  detailed research on the topic Freefall formula calculator Chuteless jump survivors Joseph W. Kittinger and the Highest Step in the World. Greg Kennedy. March 17 2010. detailed account of origins and development of EXCELSIOR project The Way Things Fall an educational website H.S. Free fall lesson with videos and interactive flash animations."}, {"topic": "Freezing point", "content": "The melting point or rarely liquefaction point of a solid is the temperature at which it changes state from solid to liquid at atmospheric pressure. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at standard pressure. When considered as the temperature of the reverse change from liquid to solid it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool the freezing point is not considered as a characteristic property of a substance. When the characteristic freezing point of a substance is determined in fact the actual methodology is almost always the principle of observing the disappearance rather than the formation of ice that is the melting point.  Melting and boiling point tables vol. 1 by Thomas Carnelley Harrison London 18851887 Melting and boiling point tables vol. 2 by Thomas Carnelley Harrison London 18851887 ONS melting point explorer Over 10000 Open Data melting points. Patent mined data Over 250000 freely downloadable melting point data. Also downloadable at figshare"}, {"topic": "Frequency modulation", "content": "In telecommunications and signal processing frequency modulation FM is the encoding of information in a carrier wave by varying the instantaneous frequency of the wave. This contrasts with amplitude modulation in which the amplitude of the carrier wave varies while the frequency remains constant. In analog frequency modulation such as FM radio broadcasting of an audio signal representing voice or music the instantaneous frequency deviation the difference between the frequency of the carrier and its center frequency is proportional to the modulating signal. Digital data can be encoded and transmitted via FM by shifting the carriers frequency among a predefined set of frequencies representing digits  for example one frequency can represent a binary 1 and a second can represent binary 0. This modulation technique is known as frequencyshift keying FSK. FSK is widely used in modems and fax modems and can also be used to send Morse code. Radioteletype also uses FSK. Frequency modulation is widely used for FM radio broadcasting. It is also used in telemetry radar seismic prospecting and monitoring newborns for seizures via EEG twoway radio systems music synthesis magnetic taperecording systems and some videotransmission systems. In radio transmission an advantage of frequency modulation is that it has a larger signaltonoise ratio and therefore rejects radio frequency interference better than an equal power amplitude modulation AM signal. For this reason most music is broadcast over FM radio. Frequency modulation has a close relationship with phase modulation phase modulation is often used as an intermediate step to achieve frequency modulation. Mathematically both of these are considered a special case of quadrature amplitude modulation QAM.  A. Bruce Carlson. Communication Systems 4th edition. McGrawHill ScienceEngineeringMath. 2001. ISBN 0070111278 ISBN 9780070111271. Gary L. Frost. Early FM Radio Incremental Technology in TwentiethCentury America. Baltimore Johns Hopkins University Press 2010. ISBN 0801894409 ISBN 9780801894404. Ken Seymour ATT Wireless Mobility. Frequency Modulation The Electronics Handbook pp 11881200 1st Edition 1996. 2nd Edition 2005 CRC Press Inc. ISBN 0849383455 1st Edition."}, {"topic": "Function (mathematics)", "content": "In mathematics a function is a relation between a set of inputs and a set of permissible outputs with the property that each input is related to exactly one output. An example is the function that relates each real number x to its square x2. The output of a function f corresponding to an input x is denoted by fx read f of x. In this example if the input is 3 then the output is 9 and we may write f3  9. Likewise if the input is 3 then the output is also 9 and we may write f3  9. The same output may be produced by more than one input but each input gives only one output. The input variables are sometimes referred to as the arguments of the function. Functions of various kinds are the central objects of investigation in most fields of modern mathematics. There are many ways to describe or represent a function. Some functions may be defined by a formula or algorithm that tells how to compute the output for a given input. Others are given by a picture called the graph of the function. In science functions are sometimes defined by a table that gives the outputs for selected inputs. A function could be described implicitly for example as the inverse to another function or as a solution of a differential equation. The input and output of a function can be expressed as an ordered pair ordered so that the first element is the input or tuple of inputs if the function takes more than one input and the second is the output. In the example above fx  x2 we have the ordered pair 3 9. If both input and output are real numbers this ordered pair can be viewed as the Cartesian coordinates of a point on the graph of the function. In modern mathematics a function is defined by its set of inputs called the domain a set containing the set of outputs and possibly additional elements as members called its codomain and the set of all inputoutput pairs called its graph. Sometimes the codomain is called the functions range but more commonly the word range is used to mean instead specifically the set of outputs this is also called the image of the function. For example we could define a function using the rule fx  x2 by saying that the domain and codomain are the real numbers and that the graph consists of all pairs of real numbers x x2. The image of this function is the set of nonnegative real numbers. Collections of functions with the same domain and the same codomain are called function spaces the properties of which are studied in such mathematical disciplines as real analysis complex analysis and functional analysis. In analogy with arithmetic it is possible to define addition subtraction multiplication and division of functions in those cases where the output is a number. Another important operation defined on functions is function composition where the output from one function becomes the input to another function.  Khan Academy Functions free online micro lectures Hazewinkel Michiel ed. 2001 Function Encyclopedia of Mathematics Springer ISBN 9781556080104 Weisstein Eric W. Function MathWorld. The Wolfram Functions Site gives formulae and visualizations of many mathematical functions. Shodor Function Flyer interactive Java applet for graphing and exploring functions. xFunctions a Java applet for exploring functions graphically. Draw Function Graphs online drawing program for mathematical functions. Functions from cuttheknot. Function at ProvenMath. Comprehensive webbased function graphing  evaluation tool. Abstractmath.org articles on functions"}, {"topic": "Fundamental frequency", "content": "The fundamental frequency often referred to simply as the fundamental is defined as the lowest frequency of a periodic waveform. In music the fundamental is the musical pitch of a note that is perceived as the lowest partial present. In terms of a superposition of sinusoids e.g. Fourier series the fundamental frequency is the lowest frequency sinusoidal in the sum. In some contexts the fundamental is usually abbreviated as f0 or FF indicating the lowest frequency counting from zero. In other contexts it is more common to abbreviate it as f1 the first harmonic. The second harmonic is then f2  2f1 etc. In this context the zeroth harmonic would be 0 Hz. "}, {"topic": "Fundamental theorem of calculus", "content": "The fundamental theorem of calculus is a theorem that links the concept of the derivative of a function with the concept of the functions integral. The first part of the theorem sometimes called the first fundamental theorem of calculus is that the indefinite integral of a function is related to its antiderivative and can be reversed by differentiation. This part of the theorem is also important because it guarantees the existence of antiderivatives for continuous functions. The second part of the theorem sometimes called the second fundamental theorem of calculus is that the definite integral of a function can be computed by using any one of its infinitelymany antiderivatives. This part of the theorem has key practical applications because it markedly simplifies the computation of definite integrals.  Hazewinkel Michiel ed. 2001 Fundamental theorem of calculus Encyclopedia of Mathematics Springer ISBN 9781556080104 James Gregorys Euclidean Proof of the Fundamental Theorem of Calculus at Convergence Isaac Barrows proof of the Fundamental Theorem of Calculus Fundamental Theorem of Calculus at imomath.com Alternative proof the to the fundamental theorem of calculus"}, {"topic": "Gamma-ray burst", "content": "Gammaray bursts GRBs are extremely energetic explosions that have been observed in distant galaxies. They are the brightest electromagnetic events known to occur in the universe. Bursts can last from ten milliseconds to several hours. After an initial flash of gamma rays a longerlived afterglow is usually emitted at longer wavelengths Xray ultraviolet optical infrared microwave and radio. The intense radiation of most observed GRBs is believed to be released during a supernova or hypernova as a rapidly rotating highmass star collapses to form a neutron star quark star or black hole. A subclass of GRBs the short bursts appear to originate from a different process the merger of binary neutron stars. The cause of the precursor burst observed in some of these short events may be due to the development of a resonance between the crust and core of such stars as a result of the massive tidal forces experienced in the seconds leading up to their collision causing the entire crust of the star to shatter. The sources of most GRBs are billions of light years away from Earth implying that the explosions are both extremely energetic a typical burst releases as much energy in a few seconds as the Sun will in its entire 10billionyear lifetime and extremely rare a few per galaxy per million years. All observed GRBs have originated from outside the Milky Way galaxy although a related class of phenomena soft gamma repeater flares are associated with magnetars within the Milky Way. It has been hypothesized that a gammaray burst in the Milky Way pointing directly towards the Earth could cause a mass extinction event. GRBs were first detected in 1967 by the Vela satellites a series of satellites designed to detect covert nuclear weapons tests. Hundreds of theoretical models were proposed to explain these bursts in the years following their discovery such as collisions between comets and neutron stars. Little information was available to verify these models until the 1997 detection of the first Xray and optical afterglows and direct measurement of their redshifts using optical spectroscopy and thus their distances and energy outputs. These discoveries and subsequent studies of the galaxies and supernovae associated with the bursts clarified the distance and luminosity of GRBs. These facts definitively placed them in distant galaxies and also connected long GRBs with the explosion of massive stars the only possible source for the energy outputs observed.  GRB Mission Sites Swift GammaRay Burst Mission Official NASA Swift Homepage UK Swift Science Data Centre Swift Mission Operations Center at Penn State HETE2 High Energy Transient Explorer Wiki entry INTEGRAL INTErnational GammaRay Astrophysics Laboratory Wiki entry BATSE Burst and Transient Source Explorer Fermi Gammaray Space Telescope Wiki entry AGILE Astrorivelatore Gamma a Immagini Leggero Wiki entry EXIST Energetic Xray Survey Telescope GRB Followup Programs The Gammaray bursts Coordinates Network GCN Wiki entry BOOTES Burst Observer and Optical Transient Exploring System Wiki entry GROND GammaRay Burst Optical Nearinfrared Detector Wiki entry KAIT The Katzman Automatic Imaging Telescope Wiki entry MASTER Mobile Astronomical System of the TelescopeRobots ROTSE Robotic Optical Transient Search Experiment Wiki entry"}, {"topic": "Gamma ray", "content": "Gamma ray also called gamma radiation denoted by the lowercase Greek letter gamma  or  where x is the thickness of the material from the incident surface  n is the absorption coefficient measured in cm1 n the number of atoms per cm3 of the material atomic density and the absorption cross section in cm2. As it passes through matter gamma radiation ionizes via three processes the photoelectric effect Compton scattering and pair production. Photoelectric effect This describes the case in which a gamma photon interacts with and transfers its energy to an atomic electron causing the ejection of that electron from the atom. The kinetic energy of the resulting photoelectron is equal to the energy of the incident gamma photon minus the energy that originally bound the electron to the atom binding energy. The photoelectric effect is the dominant energy transfer mechanism for Xray and gamma ray photons with energies below 50 keV thousand electron volts but it is much less important at higher energies. Compton scattering This is an interaction in which an incident gamma photon loses enough energy to an atomic electron to cause its ejection with the remainder of the original photons energy emitted as a new lower energy gamma photon whose emission direction is different from that of the incident gamma photon hence the term scattering. The probability of Compton scattering decreases with increasing photon energy. Compton scattering is thought to be the principal absorption mechanism for gamma rays in the intermediate energy range 100 keV to 10 MeV. Compton scattering is relatively independent of the atomic number of the absorbing material which is why very dense materials like lead are only modestly better shields on a per weight basis than are less dense materials. Pair production This becomes possible with gamma energies exceeding 1.02 MeV and becomes important as an absorption mechanism at energies over 5 MeV see illustration at right for lead. By interaction with the electric field of a nucleus the energy of the incident photon is converted into the mass of an electronpositron pair. Any gamma energy in excess of the equivalent rest mass of the two particles totaling at least 1.02 MeV appears as the kinetic energy of the pair and in the recoil of the emitting nucleus. At the end of the positrons range it combines with a free electron and the two annihilate and the entire mass of these two is then converted into two gamma photons of at least 0.51 MeV energy each or higher according to the kinetic energy of the annihilated particles. The secondary electrons andor positrons produced in any of these three processes frequently have enough energy to produce much ionization themselves. Additionally gamma rays particularly high energy ones can interact with atomic nuclei resulting in ejection of particles in photodisintegration or in some cases even nuclear fission photofission.  Basic reference on several types of radiation Radiation Q  A GCSE information Radiation information Gammaray bursts The LundLBNL Nuclear Data Search Contains information on gammaray energies from isotopes. Mapping soils with airborne detectors The LIVEChart of Nuclides IAEA with filter on gammaray energy Health Physics Society Public Education Website"}, {"topic": "General relativity", "content": "General relativity GR also known as the general theory of relativity or GTR is the geometric theory of gravitation published by Albert Einstein in 1915 and the current description of gravitation in modern physics. General relativity generalizes special relativity and Newtons law of universal gravitation providing a unified description of gravity as a geometric property of space and time or spacetime. In particular the curvature of spacetime is directly related to the energy and momentum of whatever matter and radiation are present. The relation is specified by the Einstein field equations a system of partial differential equations. Some predictions of general relativity differ significantly from those of classical physics especially concerning the passage of time the geometry of space the motion of bodies in free fall and the propagation of light. Examples of such differences include gravitational time dilation gravitational lensing the gravitational redshift of light and the gravitational time delay. The predictions of general relativity have been confirmed in all observations and experiments to date. Although general relativity is not the only relativistic theory of gravity it is the simplest theory that is consistent with experimental data. However unanswered questions remain the most fundamental being how general relativity can be reconciled with the laws of quantum physics to produce a complete and selfconsistent theory of quantum gravity. Einsteins theory has important astrophysical implications. For example it implies the existence of black holesregions of space in which space and time are distorted in such a way that nothing not even light can escapeas an endstate for massive stars. There is ample evidence that the intense radiation emitted by certain kinds of astronomical objects is due to black holes for example microquasars and active galactic nuclei result from the presence of stellar black holes and black holes of a much more massive type respectively. The bending of light by gravity can lead to the phenomenon of gravitational lensing in which multiple images of the same distant astronomical object are visible in the sky. General relativity also predicts the existence of gravitational waves which have since been observed directly by physics collaboration LIGO. In addition general relativity is the basis of current cosmological models of a consistently expanding universe.  Einstein Online Articles on a variety of aspects of relativistic physics for a general audience hosted by the Max Planck Institute for Gravitational Physics NCSA Spacetime Wrinkles produced by the numerical relativity group at the NCSA with an elementary introduction to general relativity Einsteins General Theory of Relativity on YouTube lecture by Leonard Susskind recorded September 22 2008 at Stanford University. Series of lectures on General Relativity given in 2006 at the Institut Henri Poincar introductoryadvanced. General Relativity Tutorials by John Baez. Brown Kevin. Reflections on relativity. Mathpages.com. Retrieved May 29 2005. Carroll Sean M. Lecture Notes on General Relativity. Retrieved January 5 2014. Moor Rafi. Understanding General Relativity. Retrieved July 11 2006. Waner Stefan. Introduction to Differential Geometry and General Relativity PDF. Retrieved 20150405."}, {"topic": "Geometrical optics", "content": "Geometrical optics or ray optics describes light propagation in terms of rays. The ray in geometric optics is an abstraction or instrument useful in approximating the paths along which light propagates in certain classes of circumstances. The simplifying assumptions of geometrical optics include that light rays propagate in rectilinear paths as they travel in a homogeneous medium bend and in particular circumstances may split in two at the interface between two dissimilar media follow curved paths in a medium in which the refractive index changes may be absorbed or reflected. Geometrical optics does not account for certain optical effects such as diffraction and interference. This simplification is useful in practice it is an excellent approximation when the wavelength is small compared to the size of structures with which the light interacts. The techniques are particularly useful in describing geometrical aspects of imaging including optical aberrations.  Fundamentals of Photonics  Module on Basic Geometrical Optics"}, {"topic": "Geophysics", "content": "Geophysics diofzks is a subject of natural science concerned with the physical processes and physical properties of the Earth and its surrounding space environment and the use of quantitative methods for their analysis. The term geophysics sometimes refers only to the geological applications Earths shape its gravitational and magnetic fields its internal structure and composition its dynamics and their surface expression in plate tectonics the generation of magmas volcanism and rock formation. However modern geophysics organizations use a broader definition that includes the water cycle including snow and ice fluid dynamics of the oceans and the atmosphere electricity and magnetism in the ionosphere and magnetosphere and solarterrestrial relations and analogous problems associated with the Moon and other planets. Although geophysics was only recognized as a separate discipline in the 19th century its origins date back to ancient times. The first magnetic compasses were made from lodestones while more modern magnetic compasses played an important role in the history of navigation. The first seismic instrument was built in 132 BC. Isaac Newton applied his theory of mechanics to the tides and the precession of the equinox and instruments were developed to measure the Earths shape density and gravity field as well as the components of the water cycle. In the 20th century geophysical methods were developed for remote exploration of the solid Earth and the ocean and geophysics played an essential role in the development of the theory of plate tectonics. Geophysics is applied to societal needs such as mineral resources mitigation of natural hazards and environmental protection. Geophysical survey data are used to analyze potential petroleum reservoirs and mineral deposits locate groundwater find archaeological relics determine the thickness of glaciers and soils and assess sites for environmental remediation.  A reference manual for nearsurface geophysics techniques and applications Commission on Geophysical Risk and Sustainability GeoRisk International Union of Geodesy and Geophysics IUGG Study of the Earths Deep Interior a Committee of IUGG Union Commissions IUGG USGS Geomagnetism Program Career crate Seismic processor Society of Exploration Geophysicists"}, {"topic": "Glossary of areas of mathematics", "content": "This is a glossary of terms that are or have been considered areas of study in mathematics.  Glossary of astronomy Glossary of biology Glossary of chemistry Glossary of engineering Glossary of physics Glossary of probability and statistics"}, {"topic": "Glossary of astronomy", "content": "This page is a glossary of astronomy. This scientific study is concerned with celestial objects and phenomena that originate outside the atmosphere of Earth. The field of astronomy has an extensive vocabulary and a significant amount of jargon.  Astronomical Glossary A Knowledgebase for Extragalactic Astronomy and Cosmology NASAIPAC January 10 2006 retrieved 20120219 ESO Astronomical Glossary Public Outreach European Southern Observatory retrieved 20120221 Glossary of comet and astronomical terms International Comet Quarterly Harvard University retrieved 20120220"}, {"topic": "Glossary of biology", "content": "This glossary of biology terms is a list of definitions about the fundamentals of biology its subdisciplines and related fields. "}, {"topic": "Glossary of chemistry terms", "content": "This page is a glossary of chemistry terms. Chemistry has an extensive vocabulary and a significant amount of jargon. This is a list of chemical terms including laboratory tools glassware and equipment. Chemistry itself is a physical science concerned with the composition structure and properties of matter as well as the changes it undergoes during chemical reactions. Note All periodic table references refer to the IUPAC Style of the Periodic Table  IUPAC Compendium of Chemical Terminology"}, {"topic": "Glossary of engineering", "content": "This glossary of engineering terms is a broad overview of the major concepts of engineering. Please see the bottom of the page for glossaries of specific fields of engineering. "}, {"topic": "Glossary of probability and statistics", "content": "The following is a glossary of terms used in the mathematical sciences statistics and probability.  A Glossary of DOE Terminology NISTSEMATECH eHandbook of Statistical Methods NIST retrieved 28 February 2009 Statistical glossary statistics.com retrieved 28 February 2009 Probability and Statistics on the Earliest Uses Pages Univ. of Southampton"}, {"topic": "Gluon", "content": "Gluons lunz are elementary particles that act as the exchange particles or gauge bosons for the strong force between quarks analogous to the exchange of photons in the electromagnetic force between two charged particles. In layman terms they glue quarks together forming protons and neutrons. In technical terms gluons are vector gauge bosons that mediate strong interactions of quarks in quantum chromodynamics QCD. Gluons themselves carry the color charge of the strong interaction. This is unlike the photon which mediates the electromagnetic interaction but lacks an electric charge. Gluons therefore participate in the strong interaction in addition to mediating it making QCD significantly harder to analyze than QED quantum electrodynamics.  A. Ali and G. Kramer 2011. JETS and QCD A historical review of the discovery of the quark and gluon jets and its impact on QCD. European Physical Journal H 36 2 245326. arXiv1012.2288. Bibcode2011EPJH...36..245A. doi10.1140epjhe2011100471."}, {"topic": "Graham's law of diffusion", "content": "Grahams law also known as Grahams law of effusion was formulated by Scottish physical chemist Thomas Graham in 1848. Graham found experimentally that the rate of effusion of a gas is inversely proportional to the square root of the mass of its particles. This formula can be written as Rate 1 Rate 2  M 2 M 1  Grahams law was the basis for separating 235U from 238U found in natural uraninite uranium ore during the Manhattan Project to build the first atomic bomb. The United States government built a gaseous diffusion plant in Clinton Tennessee at the cost of 100 million 7.7 billion in 2014 dollars. In this plant uranium from uranium ore was first converted to uranium hexafluoride and then forced repeatedly to diffuse through porous barriers each time becoming a little more enriched in the slightly lighter 235U isotope. "}, {"topic": "Gravitation", "content": "Gravity or gravitation is a natural phenomenon by which all things with energy are brought toward or gravitate toward one another including stars planets galaxies and even light and subatomic particles. Gravity is responsible for many of the structures in the Universe by creating spheres of hydrogen where hydrogen fuses under pressure to form stars and grouping them into galaxies. On Earth gravity gives weight to physical objects and causes the tides. Gravity has an infinite range although its effects become increasingly weaker on farther objects. Gravity is most accurately described by the general theory of relativity proposed by Albert Einstein in 1915 which describes gravity not as a force but as a consequence of the curvature of spacetime caused by the uneven distribution of massenergy and resulting in gravitational time dilation where time lapses more slowly in lower stronger gravitational potential. However for most applications gravity is well approximated by Newtons law of universal gravitation which postulates that gravity causes a force where two bodies of mass are directly drawn or attracted to each other according to a mathematical relationship where the attractive force is proportional to the product of their masses and inversely proportional to the square of the distance between them. This is considered to occur over an infinite range such that all bodies with mass in the universe are drawn to each other no matter how far they are apart. Gravity is the weakest of the four fundamental interactions of nature. The gravitational attraction is approximately 1038 times the strength of the strong force i.e. gravity is 38 orders of magnitude weaker 1036 times the strength of the electromagnetic force and 1029 times the strength of the weak force. As a consequence gravity has a negligible influence on the behavior of subatomic particles and plays no role in determining the internal properties of everyday matter but see quantum gravity. On the other hand gravity is the dominant interaction at the macroscopic scale and is the cause of the formation shape and trajectory orbit of astronomical bodies. It is responsible for various phenomena observed on Earth and throughout the universe for example it causes the Earth and the other planets to orbit the Sun the Moon to orbit the Earth the formation of tides and the formation and evolution of galaxies stars and the Solar System. In pursuit of a theory of everything the merging of general relativity and quantum mechanics or quantum field theory into a more general theory of quantum gravity has become an area of research.  Hazewinkel Michiel ed. 2001 Gravitation Encyclopedia of Mathematics Springer ISBN 9781556080104 Hazewinkel Michiel ed. 2001 Gravitation theory of Encyclopedia of Mathematics Springer ISBN 9781556080104"}, {"topic": "Gravitational acceleration", "content": "In physics gravitational acceleration is the acceleration on an object caused by force of gravitation. Neglecting friction such as air resistance all small bodies accelerate in a gravitational field at the same rate relative to the center of mass. This equality is true regardless of the masses or compositions of the bodies. At different points on Earth objects fall with an acceleration between 9.78 and 9.83 ms2 depending on altitude and latitude with a conventional standard value of exactly 9.80665 ms2 approximately 32.174 fts2. Objects with low densities do not accelerate as rapidly due to buoyancy and air resistance. "}, {"topic": "Gravitational constant", "content": "The gravitational constant also known as universal gravitational constant or as Newtons constant denoted by the letter G is an empirical physical constant involved in the calculation of gravitational effects in Sir Isaac Newtons law of universal gravitation and in Albert Einsteins general theory of relativity. Its value is approximately 69896674000000000006.6741011 Nm2kg2.  Newtonian constant of gravitation G at the National Institute of Standards and Technology References on Constants Units and Uncertainty The Controversy over Newtons Gravitational Constant additional commentary on measurement problems"}, {"topic": "Gravitational energy", "content": "Gravitational energy is potential energy associated with the gravitational field. This phrase is found frequently in scientific writings about quasars quasistellar objects and other active galaxies. Quasars generate and emit their energy from a very small region. The emission of large amounts of power from a small region requires a power source far more efficient than the nuclear fusion that powers stars. The release of gravitational energy by matter falling towards a massive black hole is the only process known that can produce such high power continuously. Stellar explosions supernovas and gammaray bursts can do so but only for a few weeks. "}, {"topic": "Gravitational field", "content": "In physics a gravitational field is a model used to explain the influence that a massive body extends into the space around itself producing a force on another massive body. Thus a gravitational field is used to explain gravitational phenomena and is measured in newtons per kilogram Nkg. In its original concept gravity was a force between point masses. Following Newton Laplace attempted to model gravity as some kind of radiation field or fluid and since the 19th century explanations for gravity have usually been taught in terms of a field model rather than a point attraction. In a field model rather than two particles attracting each other the particles distort spacetime via their mass and this distortion is what is perceived and measured as a force. In such a model one states that matter moves in certain ways in response to the curvature of spacetime and that there is either no gravitational force or that gravity is a fictitious force. "}, {"topic": "Gravitational potential", "content": "In classical mechanics the gravitational potential at a location is equal to the work energy transferred per unit mass that would be done by the force of gravity if an object were moved from its location in space to a fixed reference location. It is analogous to the electric potential with mass playing the role of charge. The reference location where the potential is zero is by convention infinitely far away from any mass resulting in a negative potential at any finite distance. In mathematics the gravitational potential is also known as the Newtonian potential and is fundamental in the study of potential theory. "}, {"topic": "Gravitational waves", "content": "Gravitational waves are ripples in the curvature of spacetime that propagate as waves generated in certain gravitational interactions and travelling outward from their source. The possibility of gravitational waves was discussed in 1893 by Heaviside using the analogy between the inversesquare law in gravitation and electricity. In 1905 Henri Poincare first predicted gravitational waves ondes gravifiques emanating from a body and propagating at the speed of light as being required by the formalism of spacetime. Predicted in 1916 by Albert Einstein on the basis of his theory of general relativity gravitational waves transport energy as gravitational radiation a form of radiant energy similar to electromagnetic radiation. Gravitational waves cannot exist in the Newtonian theory of gravitation since Newtonian theory postulates that physical interactions propagate at infinite speed. Gravitationalwave astronomy is an emerging branch of observational astronomy which aims to use gravitational waves to collect observational data about objects such as neutron stars and black holes events such as supernovae and processes including those of the early universe shortly after the Big Bang. Various gravitationalwave observatories detectors are under construction or in operation such as Advanced LIGO which began observations in September 2015. Potential sources of detectable gravitational waves include binary star systems composed of white dwarfs neutron stars and black holes. On February 11 2016 the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made the first observation of gravitational waves originating from a pair of merging black holes using the Advanced LIGO detectors. On June 15 2016 a second detection of gravitational waves from coalescing black holes was announced.  Gravitational waves at Encyclopdia Britannica Laser Interferometer Gravitational Wave Observatory. LIGO Laboratory operated by the California Institute of Technology and the Massachusetts Institute of Technology Caltech Relativity Tutorial A basic introduction to gravitational waves Gravitational Waves Collected articles at Nature Journal Gravitational Waves Collected articles Scientific American Video 0436 Detecting a gravitational wave Dennis Overbye NYT 11 February 2016. Video 7129 Press Conference announcing discovery LIGO detects gravitational waves National Science Foundation 11 February 2016. Video 9434 Scientific Talk on Discovery Barry Barish CERN 11 February 2016"}, {"topic": "Gravity", "content": "Gravity or gravitation is a natural phenomenon by which all things with energy are brought toward or gravitate toward one another including stars planets galaxies and even light and subatomic particles. Gravity is responsible for many of the structures in the Universe by creating spheres of hydrogen where hydrogen fuses under pressure to form stars and grouping them into galaxies. On Earth gravity gives weight to physical objects and causes the tides. Gravity has an infinite range although its effects become increasingly weaker on farther objects. Gravity is most accurately described by the general theory of relativity proposed by Albert Einstein in 1915 which describes gravity not as a force but as a consequence of the curvature of spacetime caused by the uneven distribution of massenergy and resulting in gravitational time dilation where time lapses more slowly in lower stronger gravitational potential. However for most applications gravity is well approximated by Newtons law of universal gravitation which postulates that gravity causes a force where two bodies of mass are directly drawn or attracted to each other according to a mathematical relationship where the attractive force is proportional to the product of their masses and inversely proportional to the square of the distance between them. This is considered to occur over an infinite range such that all bodies with mass in the universe are drawn to each other no matter how far they are apart. Gravity is the weakest of the four fundamental interactions of nature. The gravitational attraction is approximately 1038 times the strength of the strong force i.e. gravity is 38 orders of magnitude weaker 1036 times the strength of the electromagnetic force and 1029 times the strength of the weak force. As a consequence gravity has a negligible influence on the behavior of subatomic particles and plays no role in determining the internal properties of everyday matter but see quantum gravity. On the other hand gravity is the dominant interaction at the macroscopic scale and is the cause of the formation shape and trajectory orbit of astronomical bodies. It is responsible for various phenomena observed on Earth and throughout the universe for example it causes the Earth and the other planets to orbit the Sun the Moon to orbit the Earth the formation of tides and the formation and evolution of galaxies stars and the Solar System. In pursuit of a theory of everything the merging of general relativity and quantum mechanics or quantum field theory into a more general theory of quantum gravity has become an area of research.  Hazewinkel Michiel ed. 2001 Gravitation Encyclopedia of Mathematics Springer ISBN 9781556080104 Hazewinkel Michiel ed. 2001 Gravitation theory of Encyclopedia of Mathematics Springer ISBN 9781556080104"}, {"topic": "Ground (electricity)", "content": "In electrical engineering ground or earth is the reference point in an electrical circuit from which voltages are measured a common return path for electric current or a direct physical connection to the Earth. In electrical power distribution systems a protective ground conductor is an essential part of the safety Earthing system. Electrical circuits may be connected to ground earth for several reasons. In mains powered equipment exposed metal parts are connected to ground to prevent user contact with dangerous voltage when electrical insulation fails. Connection to ground also limits the buildup of static electricity when handling flammable products or electrostaticsensitive devices. In some telegraph and power transmission circuits the earth itself can be used as one conductor of the circuit saving the cost of installing a separate return conductor see singlewire earth return. For measurement purposes the Earth serves as a reasonably constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate currentcarrying capability to serve as an adequate zerovoltage reference level. In electronic circuit theory a ground is usually idealized as an infinite source or sink for charge which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur which may create noise in signals or if large enough will produce an electric shock hazard. The use of the term ground or earth is so common in electrical and electronics applications that circuits in portable electronic devices such as cell phones and media players as well as circuits in vehicles may be spoken of as having a ground connection without any actual connection to the Earth despite common being a more appropriate term for such a connection. This is usually a large conductor attached to one side of the power supply such as the ground plane on a printed circuit board which serves as the common return path for current from many different components in the circuit.  Circuit Grounds and Grounding Practices The Electromagnetic Telegraph by J. B. Calvert Grounding for Low and High Frequency Circuits PDF Analog Devices Application Note An IC Amplifier Users Guide to Decoupling Grounding and Making Things Go Right for a Change PDF Analog Devices Application Note"}, {"topic": "Ground state", "content": "The ground state of a quantum mechanical system is its lowestenergy state the energy of the ground state is known as the zeropoint energy of the system. An excited state is any state with energy greater than the ground state. The ground state of a quantum field theory is usually called the vacuum state or the vacuum. If more than one ground state exists they are said to be degenerate. Many systems have degenerate ground states. Degeneracy occurs whenever there exists a unitary operator which acts nontrivially on a ground state and commutes with the Hamiltonian of the system. According to the third law of thermodynamics a system at absolute zero temperature exists in its ground state thus its entropy is determined by the degeneracy of the ground state. Many systems such as a perfect crystal lattice have a unique ground state and therefore have zero entropy at absolute zero. It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature.  Feynman Richard Leighton Robert Sands Matthew 1965. see section 25 for energy levels 19 for the hydrogen atom. The Feynman Lectures on Physics 3."}, {"topic": "Group velocity", "content": "The group velocity of a wave is the velocity with which the overall shape of the waves amplitudesknown as the modulation or envelope of the wavepropagates through space. For example if a stone is thrown into the middle of a very still pond a circular pattern of waves with a quiescent center appears in the water. The expanding ring of waves is the wave group within which one can discern individual wavelets of differing wavelengths traveling at different speeds. The longer waves travel faster than the group as a whole but their amplitudes diminish as they approach the leading edge. The shorter waves travel more slowly and their amplitudes diminish as they emerge from the trailing boundary of the group.  Greg Egan has an excellent Java applet on his web site that illustrates the apparent difference in group velocity from phase velocity. Maarten Ambaum has a webpage with movie demonstrating the importance of group velocity to downstream development of weather systems. Phase vs. Group Velocity Various Phase and Groupvelocity relations animation"}, {"topic": "Hadron", "content": "In particle physics a hadron hdrn Greek  hadrs stout thick is a composite particle made of quarks held together by the strong force in a similar way as molecules are held together by the electromagnetic force. Hadrons are categorized into two families baryons made of three quarks and mesons made of one quark and one antiquark. Protons and neutrons are examples of baryons pions are an example of a meson. Hadrons containing more than three valence quarks exotic hadrons have been discovered in recent years. A tetraquark state an exotic meson named the Z4430 was discovered in 2007 by the Belle Collaboration and confirmed as a resonance in 2014 by the LHCb collaboration. Two pentaquark states exotic baryons named P c4380 and P c4450 were discovered in 2015 by the LHCb collaboration. There are several more exotic hadron candidates and other coloursinglet quark combinations may also exist. Of the hadrons protons are stable and neutrons bound within atomic nuclei are stable. Other hadrons are unstable under ordinary conditions free neutrons decay with a halflife of about 611 seconds. Experimentally hadron physics is studied by colliding protons or nuclei of heavy elements such as lead and detecting the debris in the produced particle showers. "}, {"topic": "Half-life", "content": "Halflife abbreviated t12 is the time required for a quantity to reduce to half its initial value. The term is commonly used in nuclear physics to describe how quickly unstable atoms undergo or how long stable atoms survive radioactive decay. The term is also used more generally to characterize any type of exponential or nonexponential decay. For example the medical sciences refer to the biological halflife of drugs and other chemicals in the body. The converse of halflife is doubling time. The original term halflife period dating to Ernest Rutherfords discovery of the principle in 1907 was shortened to halflife in the early 1950s. Rutherford applied the principle of a radioactive elements halflife to studies of age determination of rocks by measuring the decay period of radium to lead206. Halflife is constant over the lifetime of an exponentially decaying quantity and it is a characteristic unit for the exponential decay equation. The accompanying table shows the reduction of a quantity as a function of the number of halflives elapsed.  Nucleonica.net Nuclear Science Portal Nucleonica.net wiki Decay Engine Bucknell.edu System Dynamics Time Constants Subotex.com HalfLife elimination of drugs in blood plasma Simple Charting Tool"}, {"topic": "Hamilton's principle", "content": "In physics Hamiltons principle is William Rowan Hamiltons formulation of the principle of stationary action see that article for historical formulations. It states that the dynamics of a physical system is determined by a variational problem for a functional based on a single function the Lagrangian which contains all physical information concerning the system and the forces acting on it. The variational problem is equivalent to and allows for the derivation of the differential equations of motion of the physical system. Although formulated originally for classical mechanics Hamiltons principle also applies to classical fields such as the electromagnetic and gravitational fields and has even been extended to quantum mechanics quantum field theory and criticality theories.  W.R. Hamilton On a General Method in Dynamics. Philosophical Transaction of the Royal Society Part II 1834 pp. 247308 Part I 1835 pp. 95144. From the collection Sir William Rowan Hamilton 18051865 Mathematical Papers edited by David R. Wilkins School of Mathematics Trinity College Dublin 2 Ireland. 2000 also reviewed as On a General Method in Dynamics Goldstein H. 1980 Classical Mechanics 2nd ed. Addison Wesley pp. 3569. Landau LD and Lifshitz EM 1976 Mechanics 3rd. ed. Pergamon Press. ISBN 0080210228 hardcover and ISBN 0080291414 softcover pp. 24. Arnold VI. 1989 Mathematical Methods of Classical Mechanics 2nd ed. Springer Verlag pp. 5961. Cassel Kevin W. Variational Methods with Applications in Science and Engineering Cambridge University Press 2013."}, {"topic": "Hamiltonian mechanics", "content": "Hamiltonian mechanics is a theory developed as a reformulation of classical mechanics and predicts the same outcomes as nonHamiltonian classical mechanics. It uses a different mathematical formalism providing a more abstract understanding of the theory. Historically it was an important reformulation of classical mechanics which later contributed to the formulation of statistical mechanics and quantum mechanics. Hamiltonian mechanics was first formulated by William Rowan Hamilton in 1833 starting from Lagrangian mechanics a previous reformulation of classical mechanics introduced by Joseph Louis Lagrange in 1788.  Binney James J. Classical Mechanics lecture notes PDF University of Oxford retrieved 27 October 2010 Tong David Classical Dynamics Cambridge lecture notes University of Cambridge retrieved 27 October 2010 Hamilton William Rowan On a General Method in Dynamics Trinity College Dublin"}, {"topic": "Harmonic mean", "content": "In mathematics the harmonic mean sometimes called the subcontrary mean is one of several kinds of average and in particular one of the Pythagorean means. Typically it is appropriate for situations when the average of rates is desired. The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals. As a simple example the harmonic mean of 1 2 and 4 is 3 1 1  1 2  1 4  1 1 3  1 1  1 2  1 4   12 7 .  The unweighted harmonic mean can be regarded as the special case where all of the weights are equal to 1 or equivalently where all weights are equal.  Weisstein Eric W. Harmonic Mean MathWorld. Averages Arithmetic and Harmonic Means at cuttheknot"}, {"topic": "Health physics", "content": "Health physics or The Physics of Radiation Protection is the science concerned with the recognition evaluation and control of health hazards to permit the safe use and application of ionizing radiation. Health physics professionals promote excellence in the science and practice of radiation protection and safety. Health physicists principally work at facilities where radionuclides or ionizing radiation are used or produced such as medical institutions government laboratories academic and research institutions nuclear power plants regulatory agencies and manufacturing plants.  The Health Physics Society a scientific and professional organization whose members specialize in occupational and environmental radiation safety. 3  The confusing world of radiation dosimetry  M.A. Boyd 2009 U.S. Environmental Protection Agency. An account of chronological differences between USA and ICRP dosimetry systems. QA Health effects of radiation exposure BBC News 21 July 2011."}, {"topic": "Heat", "content": "In physics heat is energy that spontaneously passes between a system and its surroundings in some way other than through work or the transfer of matter. When a suitable physical pathway exists heat flows spontaneously from a hotter to a colder body. The transfer can be by contact between the source and the destination body as in conduction or by radiation between remote bodies or by conduction and radiation through a thick solid wall or by way of an intermediate fluid body as in convective circulation or by a combination of these. Because heat refers to a quantity of energy transferred between two bodies it is not a state function of either of the bodies in contrast to temperature and internal energy. Instead according to the first law of thermodynamics heat exchanged during some process contributes to the change in the internal energy and the amount of heat can be quantified by the equivalent amount of work that would bring about the same change. While heat flows spontaneously from hot to cold it is possible to construct a heat pump or refrigeration system that does work to increase the difference in temperature between two systems. Conversely a heat engine reduces an existing temperature difference to do work on another system. Historically many energy units for measurement of heat have been used. The standardsbased unit in the International System of Units SI is the joule J. Heat is measured by its effect on the states of interacting bodies for example by the amount of ice melted or a change in temperature. The quantification of heat via the temperature change of a body is called calorimetry and is widely used in practice. In calorimetry sensible heat is defined with respect to a specific chosen state variable of the system such as pressure or volume. Sensible heat causes a change of the temperature of the system while leaving the chosen state variable unchanged. Heat transfer that occurs at a constant system temperature but changes the state variable is called latent heat with respect to the variable. For infinitesimal changes the total incremental heat transfer is then the sum of the latent and sensible heat.  Heat on In Our Time at the BBC. listen now Plasma heat at 2 gigakelvins Article about extremely high temperature generated by scientists Foxnews.com Correlations for Convective Heat Transfer ChE Online Resources"}, {"topic": "Heat transfer", "content": "Heat transfer is the exchange of thermal energy between physical systems. The rate of heat transfer is dependent on the temperatures of the systems and the properties of the intervening medium through which the heat is transferred. The three fundamental modes of heat transfer are conduction convection and radiation. Heat transfer the flow of energy in the form of heat is a process by which a system changes its internal energy hence is of vital use in applications of the First Law of Thermodynamics. Conduction is also known as diffusion not to be confused with diffusion related to the mixing of constituents of a fluid. The direction of heat transfer is from a region of high temperature to another region of lower temperature and is governed by the Second Law of Thermodynamics. Heat transfer changes the internal energy of the systems from which and to which the energy is transferred. Heat transfer will occur in a direction that increases the entropy of the collection of systems. Thermal equilibrium is reached when all involved bodies and the surroundings reach the same temperature. Thermal expansion is the tendency of matter to change in volume in response to a change in temperature.  ThermalFluidsPedia  An online thermal fluids encyclopedia. Hyperphysics Article on Heat Transfer  Overview Interseasonal Heat Transfer  a practical example of how heat transfer is used to heat buildings without burning fossil fuels. Aspects of Heat Transfer Cambridge University ThermalFluids Central Energy2D Interactive Heat Transfer Simulations for Everyone"}, {"topic": "Heliophysics", "content": "The term heliophysics means physics of the Sun the prefix helio Attic Greek helios means Sun and appears to have been used only in that sense until quite recently. For example the Debrecen Heliophysical Observatory is an astronomical observatory owned and operated by Konkoly Thege Mikls Astronomical Institute of Hungarian Academy of Sciences and provides solar data and images of the solar surface obtained in Hungary since the middle of the nineteenth century. In the early times heliophysics was concerned principally with the superficial layers of the star and was synonymous with what is now more commonly called solar physics. Usage was extended explicitly 1981 to its literal meaning denoting the physics of the entire Sun from center to corona. As such it was a direct translation from the French hliophysique which had been introduced to provide a distinction from physique solaire solar physics. It thus became a subdiscipline of heliology. Early in the 21st century the meaning of the term was extended by Dr George Siscoe of Boston University to include the physics of the heliosphere the space around the Sun beyond the corona in principle out to the shock where the solar wind encounters the interstellar medium but excluding the planets and other condensed bodies although Siscoes view of the discipline appears not to contain most of the true realm of endeavour. The term was adopted in Siscoes restricted sense by the NASA Science Mission Directorate to denote the study of the heliosphere and the objects that interact with itmost notably planetary atmospheres and magnetospheres the solar corona and the interstellar medium. Heliophysics combines several other disciplines including solar physics and stellar physics in general and also several branches of nuclear physics plasma physics and space physics. The recent extension of heliophysics is closely tied to the study of space weather and the phenomena that affect it and consequently to climatology. To quote Siscoe from a recent conference presentation Heliophysics encompasses environmental science a unique hybrid between meteorology and astrophysics comprising a body of data and a set of paradigms general lawsperhaps mostly still undiscovered specific to magnetized plasmas and neutrals in the heliosphere interacting with themselves and with gravitating bodies and their atmospheres. Heliophysics is now the name of one of four divisions within NASAs Science Mission Directorate Earth Science Planetary Science Heliophysics and Astrophysics. The title was used to simplify the name of the SunSolarSystem Connections Division and before that the SunEarth Connections Division. NASAs restricted use of the term heliophysics has also been adopted in naming the International Heliophysical Year in 20072008.  NASA Science Heliophysics National Space Science Data Center"}, {"topic": "Helmholtz free energy", "content": "In thermodynamics the Helmholtz free energy is a thermodynamic potential that measures the useful work obtainable from a closed thermodynamic system at a constant temperature. The negative of the difference in the Helmholtz energy is equal to the maximum amount of work that the system can perform in a thermodynamic process in which volume is held constant. If the volume is not held constant part of this work will be performed as boundary work. The Helmholtz energy is commonly used for systems held at constant volume. Since in this case no work is performed on the environment the drop in the Helmholtz energy is equal to the maximum amount of useful work that can be extracted from the system. For a system at constant temperature and volume the Helmholtz energy is minimized at equilibrium. The Helmholtz free energy was developed by Hermann von Helmholtz a German physicist and is usually denoted by the letter A from the German Arbeit or work or the letter F . The IUPAC recommends the letter A as well as the use of name Helmholtz energy. In physics the letter F can also be used to denote the Helmholtz energy as Helmholtz energy is sometimes referred to as the Helmholtz function Helmholtz free energy or simply free energy not to be confused with Gibbs free energy. While Gibbs free energy is most commonly used as a measure of thermodynamic potential especially in the field of chemistry it is inconvenient for some applications that do not occur at constant pressure. For example in explosives research Helmholtz free energy is often used since explosive reactions by their nature induce pressure changes. It is also frequently used to define fundamental equations of state of pure substances.  Atkins Physical Chemistry 7th edition by Peter Atkins and Julio de Paula Oxford University Press HyperPhysics Helmholtz Free Energy 1"}, {"topic": "Henderson\u2013Hasselbalch equation", "content": "In chemistry the HendersonHasselbalch equation describes the derivation of pH as a measure of acidity using pKa the negative log of the acid dissociation constant in biological and chemical systems. The equation is also useful for estimating the pH of a buffer solution and finding the equilibrium pH in acidbase reactions it is widely used to calculate the isoelectric point of proteins. The equation is given by p H  p K a  log 10   A   H A    where pH is the acidity in the blood HCO3 is the concentration of bicarbonate in the blood pCO2 is the partial pressure of carbon dioxide in the arterial blood  Variable buffer system HendersonHasselbalch Calculator Applications and Example Problems Using HendersonHasselbalch Equation HendersonHasselbalch Calculator Online HendersonHasselbalch Calculator Derivation and detailed discussion of HendersonHasselbalch equation True example of using HendersonHasselbalch equation for calculation net charge of proteins"}, {"topic": "Henry's law", "content": "In chemistry Henrys law is one of the gas laws formulated by the English chemist William Henry who studied the topic in the early 19th century. In his publication about the quantity of gases absorbed by water he described the results of his experiments ...water takes up of gas condensed by one two or more additional atmospheres a quantity which ordinarily compressed would be equal to twice thrice c. the volume absorbed under the common pressure of the atmosphere. In other words the amount of dissolved gas is proportional to its partial pressure in the gas phase. The proportionality factor is called the Henrys law constant. An example where Henrys law is at play is in the depthdependent dissolution of oxygen and nitrogen in the blood of underwater divers that changes during decompression leading to decompression sickness. An everyday example is given by ones experience with carbonated soft drinks which contain dissolved carbon dioxide. Before opening the gas above the drink in its container is almost pure carbon dioxide at a pressure higher than atmospheric pressure. After the bottle is opened this gas escapes moving the partial pressure of carbon dioxide above the liquid to be much lower resulting in degassing as the dissolved carbon dioxide comes out of solution. "}, {"topic": "Hertz", "content": "The hertz symbol Hz is the unit of frequency in the International System of Units SI and is defined as one cycle per second. It is named for Heinrich Rudolf Hertz the first person to provide conclusive proof of the existence of electromagnetic waves. Some of the units most common uses are in the description of sine waves and musical tones particularly those used in radio and audiorelated applications. It is also used to describe the speeds at which computers and other electronics are driven.  BIPM Cesium ion fCs definition National Research Council of Canada Cesium fountain clock National Physical Laboratory Trapped ion optical frequency standards National Research Council of Canada Optical frequency standard based on a single trapped ion National Research Council of Canada Optical frequency comb Online Tone Generator"}, {"topic": "Higgs boson", "content": "The Higgs boson is an elementary particle in the Standard Model of particle physics. It is the quantum excitation of the Higgs fielda fundamental field of crucial importance to particle physics theory first suspected to exist in the 1960s. Unlike other known fields such as the electromagnetic field it takes a nonzero constant value almost everywhere. The question of the Higgs fields existence has been the last unverified part of the Standard Model of particle physics and according to some the central problem in particle physics. The presence of this field now believed to be confirmed explains why some fundamental particles have mass when based on the symmetries controlling their interactions they should be massless. The existence of the Higgs field would also resolve several other longstanding puzzles such as the reason for the weak forces extremely short range. Although it is hypothesized that the Higgs field permeates the entire Universe evidence for its existence has been very difficult to obtain. In principle the Higgs field can be detected through its excitations manifest as Higgs particles but these are extremely difficult to produce and detect. The importance of this fundamental question led to a 40 year search and the construction of one of the worlds most expensive and complex experimental facilities to date CERNs Large Hadron Collider in an attempt to create Higgs bosons and other particles for observation and study. On 4 July 2012 the discovery of a new particle with a mass between 125 and 7002127000000000000127 GeVc2 was announced physicists suspected that it was the Higgs boson. Since then the particle has been shown to behave interact and decay in many of the ways predicted by the Standard Model. It was also tentatively confirmed to have even parity and zero spin two fundamental attributes of a Higgs boson. This appears to be the first elementary scalar particle discovered in nature. More studies are needed to verify that the discovered particle has properties matching those predicted for the Higgs boson by the Standard Model or whether as predicted by some theories multiple Higgs bosons exist. The Higgs boson is named after Peter Higgs one of six physicists who in 1964 proposed the mechanism that suggested the existence of such a particle. On December 10 2013 two of them Peter Higgs and Franois Englert were awarded the Nobel Prize in Physics for their work and prediction Englerts coresearcher Robert Brout had died in 2011 and the Nobel Prize is not ordinarily given posthumously. Although Higgss name has come to be associated with this theory several researchers between about 1960 and 1972 independently developed different parts of it. In mainstream media the Higgs boson has often been called the God particle from a 1993 book on the topic the nickname is strongly disliked by many physicists including Higgs who regard it as sensationalistic. In the Standard Model the Higgs particle is a boson with no spin electric charge or colour charge. It is also very unstable decaying into other particles almost immediately. It is a quantum excitation of one of the four components of the Higgs field. The latter constitutes a scalar field with two neutral and two electrically charged components that form a complex doublet of the weak isospin SU2 symmetry. The Higgs field is tachyonic this does not refer to fasterthanlight speeds it means that symmetrybreaking through condensation of a particle must occur under certain conditions and has a Mexican hat shaped potential with nonzero strength everywhere including otherwise empty space which in its vacuum state breaks the weak isospin symmetry of the electroweak interaction. When this happens three components of the Higgs field are absorbed by the SU2 and U1 gauge bosons the Higgs mechanism to become the longitudinal components of the nowmassive W and Z bosons of the weak force. The remaining electrically neutral component separately couples to other particles known as fermions via Yukawa couplings causing these to acquire mass as well. Some versions of the theory predict more than one kind of Higgs fields and bosons. Alternative Higgsless models may have been considered if the Higgs boson was not discovered. On 15 December 2015 two teams of physicists working independently at CERN reported preliminary hints of a possible new subatomic particle more specifically the ATLAS and CMS experiments using 13 TeV proton collision data showed a moderate excess around 750 GeV in the twophoton spectrum if real one possibility is that the particle could be a heavier version of a Higgs boson.  Spontaneous symmetry breaking gauge theories the Higgs mechanism and all that Bernstein Reviews of Modern Physics Jan 1974  an introduction of 47 pages covering the development history and mathematics of Higgs theories from around 1950 to 1974."}, {"topic": "Horsepower", "content": "Horsepower hp is a unit of measurement of power the rate at which work is done. There are many different standards and types of horsepower. The term was adopted in the late 18th century by Scottish engineer James Watt to compare the output of steam engines with the power of draft horses. It was later expanded to include the output power of other types of piston engines as well as turbines electric motors and other machinery. The definition of the unit varied between geographical regions. Most countries now use the SI unit watt for measurement of power. With the implementation of the EU Directive 80181EEC on January 1 2010 the use of horsepower in the EU is permitted only as a supplementary unit.  How Stuff Works  Horsepower"}, {"topic": "Hubble Deep Field", "content": "The Hubble Deep Field HDF is an image of a small region in the constellation Ursa Major constructed from a series of observations by the Hubble Space Telescope. It covers an area about 2.6 arcminutes on a side about one 24millionth of the whole sky which is equivalent in angular size to a tennis ball at a distance of 100 metres. The image was assembled from 342 separate exposures taken with the Space Telescopes Wide Field and Planetary Camera 2 over ten consecutive days between December 18 and December 28 1995. The field is so small that only a few foreground stars in the Milky Way lie within it thus almost all of the 3000 objects in the image are galaxies some of which are among the youngest and most distant known. By revealing such large numbers of very young galaxies the HDF has become a landmark image in the study of the early universe with the associated scientific paper having received over 900 citations by the end of 2014. Three years after the HDF observations were taken a region in the south celestial hemisphere was imaged in a similar way and named the Hubble Deep Field South. The similarities between the two regions strengthened the belief that the universe is uniform over large scales and that the Earth occupies a typical region in the Universe the cosmological principle. A wider but shallower survey was also made as part of the Great Observatories Origins Deep Survey. In 2004 a deeper image known as the Hubble UltraDeep Field HUDF was constructed from a few months of light exposure. The HUDF image was at the time the most sensitive astronomical image ever made at visible wavelengths and it remained so until the Hubble eXtreme Deep Field XDF was released in 2012.  Media related to Hubble Deep Field at Wikimedia Commons The Hubble Deep Field. STScI. Main Hubble Deep Field website. Hubbles Deepest View of the Universe Unveils Bewildering Galaxies across Billions of Years. January 15 1996. NASAs original press release. Opus Cartoon. salon. Opus Cartoon."}, {"topic": "Huygens\u2013Fresnel principle", "content": "The HuygensFresnel principle named after Dutch physicist Christiaan Huygens and French physicist AugustinJean Fresnel is a method of analysis applied to problems of wave propagation both in the farfield limit and in nearfield diffraction.  Stratton Julius Adams Electromagnetic Theory McGrawHill 1941. Reissued by Wiley IEEE Press ISBN 9780470131534."}, {"topic": "Hydrostatics", "content": "Fluid statics or hydrostatics is the branch of fluid mechanics that studies incompressible fluids at rest. It encompasses the study of the conditions under which fluids are at rest in stable equilibrium as opposed to fluid dynamics the study of fluids in motion. Hydrostatics are categorized as a part of the fluid statics which is the study of all fluids incompressible or not at rest. Hydrostatics is fundamental to hydraulics the engineering of equipment for storing transporting and using fluids. It is also relevant to geophysics and astrophysics for example in understanding plate tectonics and the anomalies of the Earths gravitational field to meteorology to medicine in the context of blood pressure and many other fields. Hydrostatics offers physical explanations for many phenomena of everyday life such as why atmospheric pressure changes with altitude why wood and oil float on water and why the surface of water is always flat and horizontal whatever the shape of its container.  J.B. Calvert 2003 Hydrostatics from University of Denver. Accessed on 20130522."}, {"topic": "Ice point", "content": "Melting also known as fusion is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases typically by the application of heat or pressure which increases the substances temperature to the melting point. At the melting point the ordering of ions or molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. An object that has melted completely is molten although this word is typically used for substances that melt only at a high temperature such as molten iron or molten lava. Substances in the molten state generally have reduced viscosity as the temperature increases. An exception to this principle is the element sulfur whose viscosity increases to a point due to polymerization and then decreases with higher temperatures in its molten state. Some organic compounds melt through mesophases states of partial order between solid and liquid. "}, {"topic": "Indefinite integral", "content": "In calculus an antiderivative primitive function primitive integral or indefinite integral of a function f is a differentiable function F whose derivative is equal to the original function f. This can be stated symbolically as F  f. The process of solving for antiderivatives is called antidifferentiation or indefinite integration and its opposite operation is called differentiation which is the process of finding a derivative. Antiderivatives are related to definite integrals through the fundamental theorem of calculus the definite integral of a function over an interval is equal to the difference between the values of an antiderivative evaluated at the endpoints of the interval. The discrete equivalent of the notion of antiderivative is antidifference.  Wolfram Integrator Free online symbolic integration with Mathematica Mathematical Assistant on Web symbolic computations online. Allows to integrate in small steps with hints for next step integration by parts substitution partial fractions application of formulas and others powered by Maxima Function Calculator from WIMS Integral Antiderivatives and indefinite integrals  at the Khan Academy Free online integral calculator with step by step solution  1 2 3 4"}, {"topic": "Index of physics articles", "content": "Physics Greek physis meaning nature is the natural science which examines basic concepts such as mass charge matter and its motion and all that derives from these such as energy force and spacetime. More broadly it is the general analysis of nature conducted in order to understand how the world and universe behave. The index of physics articles is split into multiple pages due to its size. To navigate by individual letter use the table of contents below. "}, {"topic": "Inductance", "content": "In electromagnetism and electronics inductance is the property of an electrical conductor by which a change in current through it induces an electromotive force in both the conductor itself and in any nearby conductors by mutual inductance. These effects are derived from two fundamental observations of physics a steady current creates a steady magnetic field described by Oersteds law and a timevarying magnetic field induces an electromotive force EMF in nearby conductors which is described by Faradays law of induction. According to Lenzs law a changing electric current through a circuit that contains inductance induces a proportional voltage which opposes the change in current selfinductance. The varying field in this circuit may also induce an EMF in neighbouring circuits mutual inductance. The term inductance was coined by Oliver Heaviside in 1886. It is customary to use the symbol L for inductance in honour of the physicist Heinrich Lenz. In the SI system the measurement unit for inductance is the henry with the unit symbol H named in honor of Joseph Henry who discovered inductance independently of but not before Faraday.  Clemson Vehicular Electronics Laboratory Inductance Calculator"}, {"topic": "Inertia", "content": "Inertia is the resistance of any physical object to any change in its state of motion this includes changes to its speed direction or state of rest. It is the tendency of objects to keep moving in a straight line at constant velocity. The principle of inertia is one of the fundamental principles of classical physics that are used to describe the motion of objects and how they are affected by applied forces. Inertia comes from the Latin word iners meaning idle sluggish. Inertia is one of the primary manifestations of mass which is a quantitative property of physical systems. Isaac Newton defined inertia as his first law in his Philosophi Naturalis Principia Mathematica which states The vis insita or innate force of matter is a power of resisting by which every body as much as in it lies endeavours to preserve its present state whether it be of rest or of moving uniformly forward in a straight line. In common usage the term inertia may refer to an objects amount of resistance to change in velocity which is quantified by its mass or sometimes to its momentum depending on the context. The term inertia is more properly understood as shorthand for the principle of inertia as described by Newton in his First Law of Motion an object not subject to any net external force moves at a constant velocity. Thus an object will continue moving at its current velocity until some force causes its speed or direction to change. On the surface of the Earth inertia is often masked by the effects of friction and air resistance both of which tend to decrease the speed of moving objects commonly to the point of rest and gravity. This misled the philosopher Aristotle to believe that objects would move only as long as force was applied to them ...it body stops when the force which is pushing the travelling object has no longer power to push it along...  Jean Buridan Stanford Encyclopaedia of Philosophy Inertia Formula Why Does the Earth Spin YouTube Connectivity and the Origin of Inertia"}, {"topic": "Infrasound", "content": "Infrasound sometimes referred to as lowfrequency sound is sound that is lower in frequency than 20 Hz hertz or cycles per second the normal limit of human hearing. Hearing becomes gradually less sensitive as frequency decreases so for humans to perceive infrasound the sound pressure must be sufficiently high. The ear is the primary organ for sensing infrasound but at higher intensities it is possible to feel infrasound vibrations in various parts of the body. The study of such sound waves is referred to sometimes as infrasonics covering sounds beneath 20 Hz down to 0.001 Hz. This frequency range is utilized for monitoring earthquakes charting rock and petroleum formations below the earth and also in ballistocardiography and seismocardiography to study the mechanics of the heart. Infrasound is characterized by an ability to cover long distances and get around obstacles with little dissipation. In music low frequency sounds including near infrasound can be produced using acoustic waveguide methods such as a large pipe organ or for reproduction exotic loudspeaker designs such as transmission line rotary woofer or traditional subwoofer designs. Subwoofers designed to produce infrasound are capable of sound reproduction an octave or more below that of most commercially available subwoofers and are often about 10 times the size.  Inframatics an international infrasound monitoring organization NOAA Infrasonics Program US Army Space and Missile Defense Command Monitoring Research Program Los Alamos Infrasound Monitoring Laboratory Infrasonic and AcousticGravity Waves Generated by the Mount Pinatubo Eruption of 15 June 1991 Makoto Tahira Masahiro Nomura Yosihiro Sawada and Kosuke Kamo Subsurface windscreen for the measurement of outdoor infrasound Qamar A. Shams Cecil G. Burkett and Toby Comeaux NASA Langley Research Center Allan J. Zuckerwar Analytical Services and Material and George R. Weistroffer Virginia Commonwealth University"}, {"topic": "Integral", "content": "In mathematics an integral assigns numbers to functions in a way that can describe displacement area volume and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus with its inverse differentiation being the other. Given a function f of a real variable x and an interval a b of the real line the definite integral a b f  x  d x   which has no singularities at all.  Keisler H. Jerome Elementary Calculus An Approach Using Infinitesimals University of Wisconsin Stroyan K.D. A Brief Introduction to Infinitesimal Calculus University of Iowa Mauch Sean Seans Applied Math Book CIT an online textbook that includes a complete introduction to calculus Crowell Benjamin Calculus Fullerton College an online textbook Garrett Paul Notes on FirstYear Calculus Hussain Faraz Understanding Calculus an online textbook Johnson William Woolsey 1909 Elementary Treatise on Integral Calculus link from HathiTrust. Kowalk W.P. Integration Theory University of Oldenburg. A new concept to an old problem. Online textbook Sloughter Dan Difference Equations to Differential Equations an introduction to calculus Numerical Methods of Integration at Holistic Numerical Methods Institute P.S. Wang Evaluation of Definite Integrals by Symbolic Manipulation 1972 a cookbook of definite integral techniques"}, {"topic": "Integral calculus", "content": "In mathematics an integral assigns numbers to functions in a way that can describe displacement area volume and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus with its inverse differentiation being the other. Given a function f of a real variable x and an interval a b of the real line the definite integral a b f  x  d x   which has no singularities at all.  Keisler H. Jerome Elementary Calculus An Approach Using Infinitesimals University of Wisconsin Stroyan K.D. A Brief Introduction to Infinitesimal Calculus University of Iowa Mauch Sean Seans Applied Math Book CIT an online textbook that includes a complete introduction to calculus Crowell Benjamin Calculus Fullerton College an online textbook Garrett Paul Notes on FirstYear Calculus Hussain Faraz Understanding Calculus an online textbook Johnson William Woolsey 1909 Elementary Treatise on Integral Calculus link from HathiTrust. Kowalk W.P. Integration Theory University of Oldenburg. A new concept to an old problem. Online textbook Sloughter Dan Difference Equations to Differential Equations an introduction to calculus Numerical Methods of Integration at Holistic Numerical Methods Institute P.S. Wang Evaluation of Definite Integrals by Symbolic Manipulation 1972 a cookbook of definite integral techniques"}, {"topic": "Integral transform", "content": "In mathematics an integral transform is any transform T of the following form  T f   u   t 1 t 2 K  t  u  f  t  d t   one obtains n n matrices as integration kernels convolution corresponds to circulant matrices.  A. D. Polyanin and A. V. Manzhirov Handbook of Integral Equations CRC Press Boca Raton 1998. ISBN 0849328764 R. K. M. Thambynayagam The Diffusion Handbook Applied Solutions for Engineers McGrawHill New York 2011. ISBN 9780071751841 Hazewinkel Michiel ed. 2001 Integral transform Encyclopedia of Mathematics Springer ISBN 9781556080104 Tables of Integral Transforms at EqWorld The World of Mathematical Equations."}, {"topic": "International System of Units", "content": "The International System of Units French Systme international dunits SI is the modern form of the metric system and is the most widely used system of measurement. It comprises a coherent system of units of measurement built on seven base units. It defines twentytwo named units and includes many more unnamed coherent derived units. The system also establishes a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units. The system was published in 1960 as the result of an initiative that began in 1948. It is based on the metrekilogramsecond system of units MKS rather than any variant of the centimetregramsecond system CGS. SI is intended to be an evolving system so prefixes and units are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The 24th and 25th General Conferences on Weights and Measures CGPM in 2011 and 2014 for example discussed a proposal to change the definition of the kilogram linking it to an invariant of nature rather than to the mass of a material artefact thereby ensuring longterm stability. The motivation for the development of the SI was the diversity of units that had sprung up within the CGS systems and the lack of coordination between the various disciplines that used them. The CGPM which was established by the Metre Convention of 1875 brought together many international organisations to not only agree on the definitions and standards of the new system but also agree on the rules for writing and presenting measurements in a standardised manner around the world. The International System of Units has been adopted by most developed countries however the adoption has not been universal in all Englishspeaking countries. While metrication in the United States is consistent in science medicine government and various fields of technology and engineering common measurements are mostly performed in United States customary units although these have officially been defined in terms of SI units. The United Kingdom has officially adopted a policy of partial metrication. Canada has adopted the SI for most governmental medical and scientific purposes and for such varied uses as grocery weights weather reports traffic signs and gasoline sales but imperial units are still legally permitted and remain in common use throughout many sectors of Canadian society particularly in the building trade and the railway sector.  Official BIPM Bureau International des Poids et Mesures SI maintenance agency home page BIPM brochure SI reference ISO 8000012009 Quantities and units Part 1 General NIST Official Publications NIST Special Publication 330 2008 Edition The International System of Units SI NIST Special Publication 811 2008 Edition Guide for the Use of the International System of Units NIST Special Pub 814 Interpretation of the SI for the United States and Federal Government Metric Conversion Policy Rules for SAE Use of SI Metric Units International System of Units at DMOZ EngNet Metric Conversion Chart Online Categorised Metric Conversion Calculator U.S. Metric Association. 2008. A Practical Guide to the International System of Units History LaTeX SIunits package manual gives a historical background to the SI system. Research The metrological triangle Recommendation of ICWM 1 CI2005 Prometric advocacy groups The UK Metric Association The US Metric Association Procustomary measures pressure groups Procustomary measures groups at DMOZ"}, {"topic": "Invariant mass", "content": "The invariant mass rest mass intrinsic mass proper mass or in the case of bound systems simply mass is a characteristic of the total energy and momentum of an object or a system of objects that is the same in all frames of reference related by Lorentz transformations. If a center of momentum frame exists for the system then the invariant mass of a system is simply the total energy divided by the speed of light squared. In other reference frames the energy of the system increases but system momentum is subtracted from this so that the invariant mass remains unchanged. Systems whose fourmomentum is a null vector for example a single photon or many photons moving in exactly the same direction have zero invariant mass and are referred to as massless. A physical object or particle moving faster than the speed of light would have spacelike fourmomenta such as the hypothesized tachyon and these do not appear to exist. Any timelike fourmomentum possesses a reference frame where the momentum 3dimensional is zero which is a center of momentum frame. In this case invariant mass is positive and is referred to as the rest mass. If objects within a system are in relative motion then the invariant mass of the whole system will differ from the sum of the objects rest masses. This is also equal to the total energy of the system divided by c2. See massenergy equivalence for a discussion of definitions of mass. Since the mass of systems must be measured with a weight or mass scale in a center of momentum frame in which the entire system has zero momentum such a scale always measures the systems invariant mass. For example a scale would measure the kinetic energy of the molecules in a bottle of gas to be part of invariant mass of the bottle and thus also its rest mass. The same is true for massless particles in such system which add invariant mass and also rest mass to systems according to their energy. For an isolated massive system the center of mass of the system moves in a straight line with a steady subluminal velocity with a velocity depending on the reference frame used to view it. Thus an observer can always be placed to move along with it. In this frame which is the center of momentum frame the total momentum is zero and the system as a whole may be thought of as being at rest if it is a bound system like a bottle of gas. In this frame which exists under these assumptions the invariant mass of the system is equal to the total system energy in the zeromomentum frame divided by c2. This total energy in the center of momentum frame is the minimum energy which the system may be observed to have when seen by various observers from various inertial frames. Note that for reasons above such a rest frame does not exist for single photons or rays of light moving in one direction. When two or more photons move in different directions however a center of mass frame or rest frame if the system is bound exists. Thus the mass of a system of several photons moving in different directions is positive which means that an invariant mass exists for this system even though it does not exist for each photon. "}, {"topic": "Ion", "content": "An ion an n is an atom or a molecule in which the total number of electrons is not equal to the total number of protons giving the atom or molecule a net positive or negative electrical charge. Ions can be created by either chemical or physical means via ionization. In chemical terms if a neutral atom loses one or more electrons it has a net positive charge and is known as a cation. If an atom gains electrons it has a net negative charge and is known as an anion. Ions consisting of only a single atom are atomic or monatomic ions if they consist of two or more atoms they are molecular or polyatomic ions. Because of their electric charges cations and anions attract each other and readily form ionic compounds such as salts. In the case of physical ionization of a medium such as a gas what are known as ion pairs are created by ion impact and each pair consists of a free electron and a positive ion. "}, {"topic": "Ionic bond", "content": "Ionic bonding is a type of chemical bond that involves the electrostatic attraction between oppositely charged ions and is the primary interaction occurring in ionic compounds. The ions are atoms that have gained one or more electrons known as anions which are negatively charged and atoms that have lost one or more electrons known as cations which are positively charged. This transfer of electrons is known as electrovalence in contrast to covalence. In the simplest case the cation is a metal atom and the anion is a nonmetal atom but these ions can be of a more complex nature e.g. molecular ions like NH4 or SO42. In simpler words an ionic bond is the transfer of electrons from a metal to a nonmetal in order for both atoms to obtain a full valence shell. It is important to recognize that clean ionic bonding in which one atom or molecule completely share an electron from another cannot exist all ionic compounds have some degree of covalent bonding or electron sharing. Thus the term ionic bonding is given when the ionic character is greater than the covalent characterthat is a bond in which a large electronegativity difference exists between the two atoms causing the bonding to be more polar ionic than in covalent bonding where electrons are shared more equally. Bonds with partially ionic and partially covalent character are called polar covalent bonds. Ionic compounds conduct electricity when molten or in solution typically as a solid. Ionic compounds generally have a high melting point depending on the charge of the ions they consist of. The higher the charges the stronger the cohesive forces and the higher the melting point. They also tend to be soluble in water. Here the opposite trend roughly holds The weaker the cohesive forces the greater the solubility.  Ionic bonding tutorial Video on ionic bonding"}, {"topic": "Ionization", "content": "Ionization is the process by which an atom or a molecule acquires a negative or positive charge by gaining or losing electrons to form ions often in conjunction with other chemical changes. Ionization can result from the loss of an electron after collisions with subatomic particles collisions with other atoms molecules and ions or through the interaction with light. Heterolytic bond cleavage and heterolytic substitution reactions can result in the formation of ion pairs. Ionization can occur through radioactive decay by the internal conversion process in which an excited nucleus transfers its energy to one of the innershell electrons causing it to be ejected.  The dictionary definition of ionization at Wiktionary"}, {"topic": "Isotope", "content": "Isotopes are variants of a particular chemical element which differ in neutron number although all isotopes of a given element have the same number of protons in each atom. The term isotope is formed from the Greek roots isos  equal and topos  place meaning the same place thus the meaning behind the name is that different isotopes of a single element occupy the same position on the periodic table. The number of protons within the atoms nucleus is called atomic number and is equal to the number of electrons in the neutral nonionized atom. Each atomic number identifies a specific element but not the isotope an atom of a given element may have a wide range in its number of neutrons. The number of nucleons both protons and neutrons in the nucleus is the atoms mass number and each isotope of a given element has a different mass number. For example carbon12 carbon13 and carbon14 are three isotopes of the element carbon with mass numbers 12 13 and 14 respectively. The atomic number of carbon is 6 which means that every carbon atom has 6 protons so that the neutron numbers of these isotopes are 6 7 and 8 respectively.  The Nuclear Science web portal Nucleonica The Karlsruhe Nuclide Chart National Nuclear Data Center Portal to large repository of free data and analysis programs from NNDC National Isotope Development Center Coordination and management of the production availability and distribution of isotopes and reference information for the isotope community Isotope Development  Production for Research and Applications IDPRA U.S. Department of Energy program for isotope production and production research and development International Atomic Energy Agency Homepage of International Atomic Energy Agency IAEA an Agency of the United Nations UN Atomic Weights and Isotopic Compositions for All Elements Static table from NIST National Institute of Standards and Technology Atomgewichte Zerfallsenergien und Halbwertszeiten aller Isotope Exploring the Table of the Isotopes at the LBNL Current isotope research and information isotope.info Emergency Preparedness and Response Radioactive Isotopes by the CDC Centers for Disease Control and Prevention Chart of Nuclides Interactive Chart of Nuclides National Nuclear Data Center Interactive Chart of the nuclides isotopes and Periodic Table The LIVEChart of Nuclides IAEA with isotope data. Annotated bibliography for isotopes from the Alsos Digital Library for Nuclear Issues The Valley of Stability video a virtual flight through 3D representation of the nuclide chart by CEA France"}, {"topic": "Joule", "content": "The joule dul symbol J is a derived unit of energy in the International System of Units. It is equal to the energy transferred or work done to an object when a force of one newton acts on that object in the direction of its motion through a distance of one metre 1 newton metre or Nm. It is also the energy dissipated as heat when an electric current of one ampere passes through a resistance of one ohm for one second. It is named after the English physicist James Prescott Joule 18181889. In terms firstly of base SI units and then in terms of other SI units J  k g m 2 s 2  N m  P a m 3  W s  C V  where E is the energy is the torque and is the angle moved in radians. Since radians are dimensionless it follows that torque and energy have the same dimensions. The use of newtonmetres for torque and joules for energy is useful in helping avoid misunderstandings and miscommunications. An additional solution is to realize that joules are scalars they are the dot product of a vector force and a vector displacement whereas torque is a vector. Torque is the cross product of a distance vector and a force vector. Drawing a traditional vector arrow over newtonmetre in a torque resolves the ambiguity. "}, {"topic": "Kelvin", "content": "The kelvin is a unit of measure for temperature based upon an absolute scale. It is one of the seven base units in the International System of Units SI and is assigned the unit symbol K. The Kelvin scale is an absolute thermodynamic temperature scale using as its null point absolute zero the temperature at which all thermal motion ceases in the classical description of thermodynamics. The kelvin is defined as the fraction 1273.16 of the thermodynamic temperature of the triple point of water exactly 0.01 C or 32.018 F. In other words it is defined such that the triple point of water is exactly 273.16 K. The Kelvin scale is named after the Belfastborn Glasgow University engineer and physicist William Lord Kelvin 18241907 who wrote of the need for an absolute thermometric scale. Unlike the degree Fahrenheit and degree Celsius the kelvin is not referred to or typeset as a degree. The kelvin is the primary unit of temperature measurement in the physical sciences but is often used in conjunction with the Celsius degree which has the same magnitude. The definition implies that absolute zero 0 K is equivalent to 273.15 C 459.67 F.  Bureau International des Poids et Mesures 2006. The International System of Units SI Brochure PDF. 8th Edition. International Committee for Weights and Measures. Retrieved 20080206. Kelvin to Celsius converter"}, {"topic": "Kinematics", "content": "Kinematics is the branch of classical mechanics which describes the motion of points alternatively particles bodies objects and systems of bodies without consideration of the masses of those objects nor the forces that may have caused the motion. Kinematics as a field of study is often referred to as the geometry of motion and as such may be seen as a branch of mathematics. Kinematics begins with a description of the geometry of the system and the initial conditions of known values of the position velocity and or acceleration of various points that are a part of the system then from geometrical arguments it can determine the position the velocity and the acceleration of any part of the system. The study of the influence of forces acting on masses falls within the purview of kinetics. For further details see analytical dynamics. Kinematics is used in astrophysics to describe the motion of celestial bodies and collections of such bodies. In mechanical engineering robotics and biomechanics kinematics is used to describe the motion of systems composed of joined parts multilink systems such as an engine a robotic arm or the skeleton of the human body. The use of geometric transformations also called rigid transformations to describe the movement of components of a mechanical system simplifies the derivation of its equations of motion and is central to dynamic analysis. Kinematic analysis is the process of measuring the kinematic quantities used to describe motion. In engineering for instance kinematic analysis may be used to find the range of movement for a given mechanism and working in reverse using kinematic synthesis used to design a mechanism for a desired range of motion. In addition kinematics applies algebraic geometry to the study of the mechanical advantage of a mechanical system or mechanism.  Java applet of 1D kinematics Physclips Mechanics with animations and video clips from the University of New South Wales. Kinematic Models for Design Digital Library KMODDL featuring movies and photos of hundreds of working models of mechanical systems at Cornell University and an ebook library of classic texts on mechanical design and engineering. MicroInch Positioning with Kinematic Components"}, {"topic": "Kirchhoff's circuit laws", "content": "Kirchhoffs circuit laws are two equalities that deal with the current and potential difference commonly known as voltage in the lumped element model of electrical circuits. They were first described in 1845 by German physicist Gustav Kirchhoff. This generalized the work of Georg Ohm and preceded the work of Maxwell. Widely used in electrical engineering they are also called Kirchhoffs rules or simply Kirchhoffs laws. Both of Kirchhoffs laws can be understood as corollaries of the Maxwell equations in the lowfrequency limit. They are accurate for DC circuits and for AC circuits at frequencies where the wavelengths of electromagnetic radiation are very large compared to the circuits. "}, {"topic": "Kirchhoff's equations", "content": "In fluid dynamics the Kirchhoff equations named after Gustav Kirchhoff describe the motion of a rigid body in an ideal fluid. d d t T  T  T v v  Q h  Q  d d t T v  T v  F h  F  T  1 2  T I   m v 2  Q h  p x n  d  F h  p n  d  . Further integration produces explicit expressions for position and velocities.  Kirchhoff G. R. Vorlesungen ueber Mathematische Physik Mechanik. Lecture 19. Leipzig Teubner. 1877. Lamb H. Hydrodynamics. Sixth Edition Cambridge UK Cambridge University Press. 1932."}, {"topic": "LC circuit", "content": "An LC circuit also called a resonant circuit tank circuit or tuned circuit is an electric circuit consisting of an inductor represented by the letter L and a capacitor represented by the letter C connected together. The circuit can act as an electrical resonator an electrical analogue of a tuning fork storing energy oscillating at the circuits resonant frequency. LC circuits are used either for generating signals at a particular frequency or picking out a signal at a particular frequency from a more complex signal. They are key components in many electronic devices particularly radio equipment used in circuits such as oscillators filters tuners and frequency mixers. An LC circuit is an idealized model since it assumes there is no dissipation of energy due to resistance. Any practical implementation of an LC circuit will always include loss resulting from small but nonzero resistance within the components and connecting wires. The purpose of an LC circuit is usually to oscillate with minimal damping so the resistance is made as low as possible. While no practical circuit is without losses it is nonetheless instructive to study this ideal form of the circuit to gain understanding and physical intuition. For a circuit model incorporating resistance see RLC circuit.  An electric pendulum by Tony Kuphaldt is a classical story about the operation of LC tank How the parallelLC circuit stores energy is another excellent LC resource."}, {"topic": "Lagrangian mechanics", "content": "Lagrangian mechanics is a reformulation of classical mechanics introduced by the ItalianFrench mathematician and astronomer JosephLouis Lagrange in 1788. In Lagrangian mechanics the trajectory of a system of particles is derived by solving the Lagrange equations in one of two forms either the Lagrange equations of the first kind which treat constraints explicitly as extra equations often using Lagrange multipliers or the Lagrange equations of the second kind which incorporate the constraints directly by judicious choice of generalized coordinates. In each case a mathematical function called the Lagrangian is a function of the generalized coordinates their time derivatives and time and contains the information about the dynamics of the system. No new physics is introduced in Lagrangian mechanics compared to Newtonian mechanics. Newtons laws can include nonconservative forces like friction however they must include constraint forces explicitly and are best suited to Cartesian coordinates. Lagrangian mechanics is ideal for systems with conservative forces and for bypassing constraint forces in any coordinate system. Dissipative and driven forces can be accounted for by splitting the external forces into a sum of potential and nonpotential forces leading to a set of modified EulerLagrange EL equations. Generalized coordinates can be chosen by convenience to exploit symmetries in the system or the geometry of the constraints which may simplify solving for the motion of the system. Lagrangian mechanics also reveals conserved quantities and their symmetries in a direct way as a special case of Noethers theorem. Lagrangian mechanics is important not just for its broad applications but also for its role in advancing deep understanding of physics. Although Lagrange only sought to describe classical mechanics in his treatise Mcanique analytique Hamiltons principle that can be used to derive the Lagrange equation was later recognized to be applicable to much of fundamental theoretical physics as well particularly quantum mechanics and the theory of relativity. Lagrangian mechanics is widely used to solve mechanical problems in physics and engineering when Newtons formulation of classical mechanics is not convenient. Lagrangian mechanics applies to the dynamics of particles fields are described using a Lagrangian density. Lagranges equations are also used in optimisation problems of dynamic systems. In mechanics Lagranges equations of the second kind are used much more than those of the first kind.  Tong David Classical Dynamics Cambridge lecture notes Principle of least action interactive Excellent interactive explanationwebpage Joseph Louis de Lagrange  uvres compltes GallicaMath"}, {"topic": "Laminar flow", "content": "In fluid dynamics laminar flow or streamline flow occurs when a fluid flows in parallel layers with no disruption between the layers. At low velocities the fluid tends to flow without lateral mixing and adjacent layers slide past one another like playing cards. There are no crosscurrents perpendicular to the direction of flow nor eddies or swirls of fluids. In laminar flow the motion of the particles of the fluid is very orderly with all particles moving in straight lines parallel to the pipe walls. Laminar flow is a flow regime characterized by high momentum diffusion and low momentum convection. When a fluid is flowing through a closed channel such as a pipe or between two flat plates either of two types of flow may occur depending on the velocity and viscosity of the fluid laminar flow or turbulent flow. Laminar flow tends to occur at lower velocities below a threshold at which it becomes turbulent. Turbulent flow is a less orderly flow regime that is characterised by eddies or small packets of fluid particles which result in lateral mixing. In nonscientific terms laminar flow is smooth while turbulent flow is rough.  Laminar Flow on YouTube Laminar flow in a pipe on YouTube"}, {"topic": "Laplace transform", "content": "In mathematics the Laplace transform is an integral transform named after its discoverer PierreSimon Laplace lpls. It takes a function of a positive real variable t often time to a function of a complex variable s frequency. The Laplace transform is very similar to the Fourier transform. While the Fourier transform of a function is a complex function of a real variable frequency the Laplace transform of a function is a complex function of a complex variable. Laplace transforms are usually restricted to functions of t with t  0. A consequence of this restriction is that the Laplace transform of a function is a holomorphic function of the variable s. Unlike the Fourier transform the Laplace transform of a distribution is generally a wellbehaved function. Also techniques of complex variables can be used directly to study Laplace transforms. As a holomorphic function the Laplace transform has a power series representation. This power series expresses a function as a linear superposition of moments of the function. This perspective has applications in probability theory. The Laplace transform is invertible on a large class of functions. The inverse Laplace transform takes a function of a complex variable s often frequency and yields a function of a real variable t time. Given a simple mathematical or functional description of an input or output to a system the Laplace transform provides an alternative functional description that often simplifies the process of analyzing the behavior of the system or in synthesizing a new system based on a set of specifications. So for example Laplace transformation from the time domain to the frequency domain transforms differential equations into algebraic equations and convolution into multiplication. It has many applications in the sciences and technology.  Hazewinkel Michiel ed. 2001 Laplace transform Encyclopedia of Mathematics Springer ISBN 9781556080104 Online Computation of the transform or inverse transform wims.unice.fr Tables of Integral Transforms at EqWorld The World of Mathematical Equations. Weisstein Eric W. Laplace Transform MathWorld. Laplace Transform Module by John H. Mathews Good explanations of the initial and final value theorems Laplace Transforms at MathPages Computational Knowledge Engine allows to easily calculate Laplace Transforms and its inverse Transform."}, {"topic": "Laplace-Runge-Lenz vector", "content": "In classical mechanics the LaplaceRungeLenz vector or simply the LRL vector is a vector used chiefly to describe the shape and orientation of the orbit of one astronomical body around another such as a planet revolving around a star. For two bodies interacting by Newtonian gravity the LRL vector is a constant of motion meaning that it is the same no matter where it is calculated on the orbit equivalently the LRL vector is said to be conserved. More generally the LRL vector is conserved in all problems in which two bodies interact by a central force that varies as the inverse square of the distance between them such problems are called Kepler problems. The hydrogen atom is a Kepler problem since it comprises two charged particles interacting by Coulombs law of electrostatics another inverse square central force. The LRL vector was essential in the first quantum mechanical derivation of the spectrum of the hydrogen atom before the development of the Schrdinger equation. However this approach is rarely used today. In classical and quantum mechanics conserved quantities generally correspond to a symmetry of the system. The conservation of the LRL vector corresponds to an unusual symmetry the Kepler problem is mathematically equivalent to a particle moving freely on the surface of a fourdimensional hypersphere so that the whole problem is symmetric under certain rotations of the fourdimensional space. This higher symmetry results from two properties of the Kepler problem the velocity vector always moves in a perfect circle and for a given total energy all such velocity circles intersect each other in the same two points. The LaplaceRungeLenz vector is named after PierreSimon de Laplace Carl Runge and Wilhelm Lenz. It is also known as the Laplace vector the RungeLenz vector and the Lenz vector. Ironically none of those scientists discovered it. The LRL vector has been rediscovered several times and is also equivalent to the dimensionless eccentricity vector of celestial mechanics. Various generalizations of the LRL vector have been defined which incorporate the effects of special relativity electromagnetic fields and even different types of central forces.  Baez John. Mysteries of the gravitational 2body problem. DEliseo MM 2007. The firstorder orbital equation. American Journal of Physics 75 352355. Bibcode2007AmJPh..75..352D. doi10.11191.2432126. Leach P.G.L. G.P. Flessas 2003. Generalisations of the LaplaceRungeLenz vector. J. Nonlinear Math. Phys. 10 340423. arXivmathph0403028. Bibcode2003JNMP...10..340L. doi10.2991jnmp.2003.10.3.6."}, {"topic": "Laser", "content": "A laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The term laser originated as an acronym for light amplification by stimulated emission of radiation. The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow. A laser differs from other sources of light in that it emits light coherently. Spatial coherence allows a laser to be focused to a tight spot enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances collimation enabling applications such as laser pointers. Lasers can also have high temporal coherence which allows them to emit light with a very narrow spectrum i.e. they can emit a single color of light. Temporal coherence can be used to produce pulses of light as short as a femtosecond. Among their many applications lasers are used in optical disk drives laser printers and barcode scanners DNA sequencing instruments fiberoptic and freespace optical communication laser surgery and skin treatments cutting and welding materials military and law enforcement devices for marking targets and measuring range and speed and laser lighting displays in entertainment.  Encyclopedia of laser physics and technology by Dr. Rdiger Paschotta A Practical Guide to Lasers for Experimenters and Hobbyists by Samuel M. Goldwasser Homebuilt Lasers Page by Professor Mark Csele Powerful laser is brightest light in the universe The worlds most powerful laser as of 2008 might create supernovalike shock waves and possibly even antimatter New Scientist April 9 2008 Laser Fundamentals an online course by Prof. F. Balembois and Dr. S. Forget. Instrumentation for Optics 2008 accessed January 17 2014 Northrop Grummans Press Release on the Firestrike 15kw tactical laser product. Website on Lasers 50th anniversary by APS OSA SPIE Advancing the Laser anniversary site by SPIE Video interviews openaccess articles posters DVDs Bright Idea The First Lasers history of the invention with audio interview clips. Free software for Simulation of random laser dynamics Video Demonstrations in Lasers and Optics Produced by the Massachusetts Institute of Technology MIT. Realtime effects are demonstrated in a way that would be difficult to see in a classroom setting. Virtual Museum of Laser History from the touring exhibit by SPIE website with animations applications and research about laser and other quantum based phenomena Universite Paris Sud"}, {"topic": "Laser medicine", "content": "Laser medicine consists in the use of lasers in medical diagnosis treatments or therapies such as laser photodynamic therapy. "}, {"topic": "Lepton", "content": "A lepton is an elementary halfinteger spin spin 12 particle that does not undergo strong interactions. Two main classes of leptons exist charged leptons also known as the electronlike leptons and neutral leptons better known as neutrinos. Charged leptons can combine with other particles to form various composite particles such as atoms and positronium while neutrinos rarely interact with anything and are consequently rarely observed. The best known of all leptons is the electron. There are six types of leptons known as flavours forming three generations. The first generation is the electronic leptons comprising the electron e and electron neutrino  e the second is the muonic leptons comprising the muon  and muon neutrino   and the third is the tauonic leptons comprising the tau  and the tau neutrino  . Electrons have the least mass of all the charged leptons. The heavier muons and taus will rapidly change into electrons and neutrinos through a process of particle decay the transformation from a higher mass state to a lower mass state. Thus electrons are stable and the most common charged lepton in the universe whereas muons and taus can only be produced in high energy collisions such as those involving cosmic rays and those carried out in particle accelerators. Leptons have various intrinsic properties including electric charge spin and mass. Unlike quarks however leptons are not subject to the strong interaction but they are subject to the other three fundamental interactions gravitation electromagnetism excluding neutrinos which are electrically neutral and the weak interaction. For every lepton flavor there is a corresponding type of antiparticle known as an antilepton that differs from the lepton only in that some of its properties have equal magnitude but opposite sign. However according to certain theories neutrinos may be their own antiparticle but it is not currently known whether this is the case or not. The first charged lepton the electron was theorized in the mid19th century by several scientists and was discovered in 1897 by J. J. Thomson. The next lepton to be observed was the muon discovered by Carl D. Anderson in 1936 which was classified as a meson at the time. After investigation it was realized that the muon did not have the expected properties of a meson but rather behaved like an electron only with higher mass. It took until 1947 for the concept of leptons as a family of particle to be proposed. The first neutrino the electron neutrino was proposed by Wolfgang Pauli in 1930 to explain certain characteristics of beta decay. It was first observed in the CowanReines neutrino experiment conducted by Clyde Cowan and Frederick Reines in 1956. The muon neutrino was discovered in 1962 by Leon M. Lederman Melvin Schwartz and Jack Steinberger and the tau discovered between 1974 and 1977 by Martin Lewis Perl and his colleagues from the Stanford Linear Accelerator Center and Lawrence Berkeley National Laboratory. The tau neutrino remained elusive until July 2000 when the DONUT collaboration from Fermilab announced its discovery. Leptons are an important part of the Standard Model. Electrons are one of the components of atoms alongside protons and neutrons. Exotic atoms with muons and taus instead of electrons can also be synthesized as well as leptonantilepton particles such as positronium.  Particle Data Group homepage. The PDG compiles authoritative information on particle properties. Leptons a summary of leptons from Hyperphysics."}, {"topic": "Lever", "content": "A lever livr or US lvr is a machine consisting of a beam or rigid rod pivoted at a fixed hinge or fulcrum. A lever is a rigid body capable of rotating on a point on itself. On the basis of the location of fulcrum load and effort the lever is divided into three types. It is one of the six simple machines identified by Renaissance scientists. A lever amplifies an input force to provide a greater output force which is said to provide leverage. The ratio of the output force to the input force is the mechanical advantage of the lever.  Lever at Diracdelta science and engineering encyclopedia A Simple Lever by Stephen Wolfram Wolfram Demonstrations Project. Levers Simple Machines at EnchantedLearning.com"}, {"topic": "Light", "content": "Light is electromagnetic radiation within a certain portion of the electromagnetic spectrum. The word usually refers to visible light which is visible to the human eye and is responsible for the sense of sight. Visible light is usually defined as having wavelengths in the range of 400700 nanometres nm or 4.00 107 to 7.00 107 m between the infrared with longer wavelengths and the ultraviolet with shorter wavelengths. This wavelength means a frequency range of roughly 430750 terahertz THz. The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically another important source of light for humans has been fire from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems electric lighting has effectively replaced firelight. Some species of animals generate their own light a process called bioluminescence. For example fireflies use light to locate mates and vampire squids use it to hide themselves from prey. The primary properties of visible light are intensity propagation direction frequency or wavelength spectrum and polarization while its speed in a vacuum 299792458 metres per second is one of the fundamental constants of nature. Visible light as with all types of electromagnetic radiation EMR is experimentally found to always move at this speed in a vacuum. In physics the term light sometimes refers to electromagnetic radiation of any wavelength whether visible or not. In this sense gamma rays Xrays microwaves and radio waves are also light. Like all types of light visible light is emitted and absorbed in tiny packets called photons and exhibits properties of both waves and particles. This property is referred to as the waveparticle duality. The study of light known as optics is an important research area in modern physics.  Media related to Light at Wikimedia Commons The dictionary definition of light at Wiktionary Quotations related to Light at Wikiquote"}, {"topic": "Linear", "content": "The Lincoln NearEarth Asteroid Research LINEAR project is a collaboration of the United States Air Force NASA and the MITs Lincoln Laboratory for the systematic detection and tracking of nearEarth objects. LINEAR was responsible for the majority of asteroid discoveries from 1998 until it was overtaken by the Catalina Sky Survey in 2005. As of 15 September 2011 LINEAR had detected 231082 new small Solar System bodies of which at least 2423 were nearEarth asteroids and 279 were comets. The instruments used by the LINEAR program are located at the Lincoln Laboratorys site on the White Sands Missile Range WSMR near Socorro New Mexico.  Official website NEO discovery statistics from JPL. Shows the number of asteroids of various types potentially hazardous size 1 km etc. that different programs have discovered by year. Sky Database for Objects in TimeDomain provides corrected LINEAR data."}, {"topic": "Linear actuator", "content": "A linear actuator is an actuator that creates motion in a straight line in contrast to the circular motion of a conventional electric motor. Linear actuators are used in machine tools and industrial machinery in computer peripherals such as disk drives and printers in valves and dampers and in many other places where linear motion is required. Hydraulic or pneumatic cylinders inherently produce linear motion. Many other mechanisms are used to generate linear motion from a rotating motor.  Leo Dorsts Lego linear actuator"}, {"topic": "Linear algebra", "content": "Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines planes and subspaces but is also concerned with properties common to all vector spaces. The set of points with coordinates that satisfy a linear equation forms a hyperplane in an ndimensional space. The conditions under which a set of n hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors. Linear algebra is central to both pure and applied mathematics. For instance abstract algebra arises by relaxing the axioms of a vector space leading to a number of generalizations. Functional analysis studies the infinitedimensional version of the theory of vector spaces. Combined with calculus linear algebra facilitates the solution of linear systems of differential equations. Techniques from linear algebra are also used in analytic geometry engineering physics natural sciences computer science computer animation and the social sciences particularly in economics. Because linear algebra is such a welldeveloped theory nonlinear mathematical models are sometimes approximated by linear models.  Beezer Rob A First Course in Linear Algebra Connell Edwin H. Elements of Abstract and Linear Algebra Hefferon Jim Linear Algebra Matthews Keith Elementary Linear Algebra Sharipov Ruslan Course of linear algebra and multidimensional geometry Treil Sergei Linear Algebra Done Wrong"}, {"topic": "Linear elasticity", "content": "Linear elasticity is the mathematical study of how solid objects deform and become internally stressed due to prescribed loading conditions. Linear elasticity models materials as continua. Linear elasticity is a simplification of the more general nonlinear theory of elasticity and is a branch of continuum mechanics. The fundamental linearizing assumptions of linear elasticity are infinitesimal strains or small deformations or strains and linear relationships between the components of stress and strain. In addition linear elasticity is valid only for stress states that do not produce yielding. These assumptions are reasonable for many engineering materials and engineering design scenarios. Linear elasticity is therefore used extensively in structural analysis and engineering design often with the aid of finite element analysis. "}, {"topic": "Liquid", "content": "A liquid is a nearly incompressible fluid that conforms to the shape of its container but retains a nearly constant volume independent of pressure. As such it is one of the four fundamental states of matter the others being solid gas and plasma and is the only state with a definite volume but no fixed shape. A liquid is made up of tiny vibrating particles of matter such as atoms held together by intermolecular bonds. Water is by far the most common liquid on Earth. Like a gas a liquid is able to flow and take the shape of a container. Most liquids resist compression although others can be compressed. Unlike a gas a liquid does not disperse to fill every space of a container and maintains a fairly constant density. A distinctive property of the liquid state is surface tension leading to wetting phenomena. The density of a liquid is usually close to that of a solid and much higher than in a gas. Therefore liquid and solid are both termed condensed matter. On the other hand as liquids and gases share the ability to flow they are both called fluids. Although liquid water is abundant on Earth this state of matter is actually the least common in the known universe because liquids require a relatively narrow temperaturepressure range to exist. Most known matter in the universe is in gaseous form with traces of detectable solid matter as interstellar clouds or in plasma form within stars. "}, {"topic": "Liquid crystal", "content": "Liquid crystals LCs are matter in a state which has properties between those of conventional liquids and those of solid crystals. For instance a liquid crystal may flow like a liquid but its molecules may be oriented in a crystallike way. There are many different types of liquidcrystal phases which can be distinguished by their different optical properties such as birefringence. When viewed under a microscope using a polarized light source different liquid crystal phases will appear to have distinct textures. The contrasting areas in the textures correspond to domains where the liquidcrystal molecules are oriented in different directions. Within a domain however the molecules are well ordered. LC materials may not always be in a liquidcrystal phase just as water may turn into ice or steam. Liquid crystals can be divided into thermotropic lyotropic and metallotropic phases. Thermotropic and lyotropic liquid crystals consist mostly of organic molecules although a few minerals are also known. Thermotropic LCs exhibit a phase transition into the liquidcrystal phase as temperature is changed. Lyotropic LCs exhibit phase transitions as a function of both temperature and concentration of the liquidcrystal molecules in a solvent typically water. Metallotropic LCs are composed of both organic and inorganic molecules their liquidcrystal transition depends not only on temperature and concentration but also on the inorganicorganic composition ratio. Examples of liquid crystals can be found both in the natural world and in technological applications. Most contemporary electronic displays use liquid crystals. Lyotropic liquidcrystalline phases are abundant in living systems but can also be found in the mineral world. For example many proteins and cell membranes are liquid crystals. Other wellknown examples of liquid crystals are solutions of soap and various related detergents as well as the tobacco mosaic virus and some clays.  History and Properties of Liquid Crystals. Nobelprize.org. Retrieved June 6 2009. Definitions of basic terms relating to lowmolarmass and polymer liquid crystals IUPAC Recommendations 2001 An intelligible introduction to liquid crystals from Case Western Reserve University Liquid Crystal Physics tutorial from the Liquid Crystals Group University of Colorado Liquid Crystals  Photonics Group Ghent University Belgium good tutorial Simulation of light propagation in liquid crystals free program Liquid Crystals Interactive Online Liquid Crystal Institute Kent State University Liquid Crystals a journal by TaylorFrancis Molecular Crystals and Liquid Crystals a journal by Taylor  Francis Hotspot detection techniques for ICs What are liquid crystals from Chalmers University of Technology Sweden H. Kleinert  K. Maki 1981. Lattice Textures in Cholesteric Liquid Crystals PDF. Fortschritte der Physik 29 5 219. Bibcode1981ForPh..29..219K. doi10.1002prop.19810290503. Progress in liquid crystal chemistry Thematic series in the Open Access Beilstein Journal of Organic Chemistry DoITPoMS Teaching and Learning Package Liquid Crystals Bowlic liquid crystal from San Jose State University"}, {"topic": "M-theory", "content": "Mtheory is a theory in physics that unifies all consistent versions of superstring theory. The existence of such a theory was first conjectured by Edward Witten at a string theory conference at the University of Southern California in the spring of 1995. Wittens announcement initiated a flurry of research activity known as the second superstring revolution. Prior to Wittens announcement string theorists had identified five versions of superstring theory. Although these theories appeared at first to be very different work by several physicists showed that the theories were related in intricate and nontrivial ways. In particular physicists found that apparently distinct theories could be unified by mathematical transformations called Sduality and Tduality. Wittens conjecture was based in part on the existence of these dualities and in part on the relationship of the string theories to a field theory called elevendimensional supergravity. Although a complete formulation of Mtheory is not known the theory should describe two and fivedimensional objects called branes and should be approximated by elevendimensional supergravity at low energies. Modern attempts to formulate Mtheory are typically based on matrix theory or the AdSCFT correspondence. According to Witten M should stand for magic mystery or membrane according to taste and the true meaning of the title should be decided when a more fundamental formulation of the theory is known. Investigations of the mathematical structure of Mtheory have spawned important theoretical results in physics and mathematics. More speculatively Mtheory may provide a framework for developing a unified theory of all of the fundamental forces of nature. Attempts to connect Mtheory to experiment typically focus on compactifying its extra dimensions to construct candidate models of our fourdimensional world although so far none have been verified to give rise to physics as observed at for instance the Large Hadron Collider.  The Elegant UniverseA threehour miniseries with Brian Greene on the series Nova original PBS broadcast dates October 28 810 p.m. and November 4 89 p.m. 2003. Various images texts videos and animations explaining string theory and Mtheory. Superstringtheory.comThe Official String Theory Web Site created by Patricia Schwarz. References on string theory and Mtheory for the layperson and expert. Not Even WrongPeter Woits blog on physics in general and string theory in particular."}, {"topic": "Mach number", "content": "In fluid dynamics the Mach number M or Ma mx German ma is a dimensionless quantity representing the ratio of flow velocity past a boundary to the local speed of sound. M  u c   where qc is the dynamic pressure measured behind a normal shock As can be seen M appears on both sides of the equation. The easiest method to solve the supersonic M calculation is to enter both the subsonic and supersonic equations into a computer spreadsheet such as Microsoft Excel OpenOffice.org Calc or some equivalent program to solve it numerically. It is first determined whether M is indeed greater than 1.0 by calculating M from the subsonic equation. If M is greater than 1.0 at that point then the value of M from the subsonic equation is used as the initial condition in the supersonic equation. Then a simple iteration of the supersonic equation is performed each time using the last computed value of M until M converges to a valueusually in just a few iterations. Alternatively Newtons method can also be used.  Gas Dynamics Toolbox Calculate Mach number and normal shock wave parameters for mixtures of perfect and imperfect gases. NASAs page on Mach Number Interactive calculator for Mach number. NewByte standard atmosphere calculator and speed converter"}, {"topic": "Machine", "content": "A machine is a tool containing one or more parts that uses energy to perform an intended action. Machines are usually powered by mechanical chemical thermal or electrical means and are often motorized. Historically a power tool also required moving parts to classify as a machine. However the advent of electronics has led to the development of power tools without moving parts that are considered machines. A simple machine is a device that simply transforms the direction or magnitude of a force but a large number of more complex machines exist. Examples include vehicles electronic systems molecular machines computers television and radio.  Oberg Erik Franklin D. Jones Holbrook L. Horton Henry H. Ryffel 2000. Christopher J. McCauley Riccardo Heald Muhammed Iqbal Hussain eds. Machinerys Handbook 26th ed.. New York Industrial Press Inc. ISBN 0831126353. Reuleaux Franz 1876. The Kinematics of Machinery. Trans. and annotated by A. B. W. Kennedy. New York reprinted by Dover 1963. Uicker J. J. G. R. Pennock J. E. Shigley 2003. Theory of Machines and Mechanisms. New York Oxford University Press."}, {"topic": "Machine element", "content": "Machine element refers to an elementary component of a machine. These elements consist of three basic types structural components such as frame members bearings axles splines fasteners seals and lubricants mechanisms that control movement in various ways such as gear trains belt or chain drives linkages cam and follower systems including brakes and clutches and control components such as buttons switches indicators sensors actuators and computer controllers. While generally not considered to be a machine element the shape texture and color of covers are an important part of a machine that provide a styling and operational interface between the mechanical components of a machine and its users. Machine elements are basic mechanical parts and features used as the building blocks of most machines. Most are standardized to common sizes but customs are also common for specialized applications. Machine elements may be features of a part such as screw threads or integral plain bearings or they may be discrete parts in and of themselves such as wheels axles pulleys rollingelement bearings or gears. All of the simple machines may be described as machine elements and many machine elements incorporate concepts of one or more simple machines. For example a leadscrew incorporates a screw thread which is an inclined plane wrapped around a cylinder. Many mechanical design invention and engineering tasks involve a knowledge of various machine elements and an intelligent and creative combining of these elements into a component or assembly that fills a need serves an application. "}, {"topic": "Maclaurin series", "content": "In mathematics a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the functions derivatives at a single point. The concept of a Taylor series was formulated by the Scottish mathematician James Gregory and formally introduced by the English mathematician Brook Taylor in 1715. If the Taylor series is centered at zero then that series is also called a Maclaurin series named after the Scottish mathematician Colin Maclaurin who made extensive use of this special case of Taylor series in the 18th century. A function can be approximated by using a finite number of terms of its Taylor series. Taylors theorem gives quantitative estimates on the error introduced by the use of such an approximation. The polynomial formed by taking some initial terms of the Taylor series is called a Taylor polynomial. The Taylor series of a function is the limit of that functions Taylor polynomials as the degree increases provided that the limit exists. A function may not be equal to its Taylor series even if its Taylor series converges at every point. A function that is equal to its Taylor series in an open interval or a disc in the complex plane is known as an analytic function in that interval.  Hazewinkel Michiel ed. 2001 Taylor series Encyclopedia of Mathematics Springer ISBN 9781556080104 Weisstein Eric W. Taylor Series MathWorld. Taylor polynomial  practical introduction Madhava of Sangamagramma Taylor Series Representation Module by John H. Mathews Discussion of the ParkerSochacki Method Another Taylor visualisation where you can choose the point of the approximation and the number of derivatives Taylor series revisited for numerical methods at Numerical Methods for the STEM Undergraduate Cinderella 2 Taylor expansion Taylor series Inverse trigonometric functions Taylor series"}, {"topic": "Magnetic field", "content": "A magnetic field is the magnetic effect of electric currents and magnetic materials. The magnetic field at any given point is specified by both a direction and a magnitude or strength as such it is a vector field. The term is used for two distinct but closely related fields denoted by the symbols B and H where H is measured in units of amperes per meter symbol Am1 or Am in the SI. B is measured in teslas symbolT note that although the symbol is capital T tesla is written in lower case in the SI system and newtons per meter per ampere symbol Nm1A1 or NmA in the SI. B is most commonly defined in terms of the Lorentz force it exerts on moving electric charges. Magnetic fields can be produced by moving electric charges and the intrinsic magnetic moments of elementary particles associated with a fundamental quantum property their spin. In special relativity electric and magnetic fields are two interrelated aspects of a single object called the electromagnetic tensor the split of this tensor into electric and magnetic fields depends on the relative velocity of the observer and charge. In quantum physics the electromagnetic field is quantized and electromagnetic interactions result from the exchange of photons. In everyday life magnetic fields are most often encountered as a force created by permanent magnets which pull on ferromagnetic materials such as iron cobalt or nickel and attract or repel other magnets. Magnetic fields are widely used throughout modern technology particularly in electrical engineering and electromechanics. The Earth produces its own magnetic field which is important in navigation and it shields the Earths atmosphere from solar wind. Rotating magnetic fields are used in both electric motors and generators. Magnetic forces give information about the charge carriers in a material through the Hall effect. The interaction of magnetic fields in electric devices such as transformers is studied in the discipline of magnetic circuits. "}, {"topic": "Magnetism", "content": "Magnetism is a class of physical phenomena that are mediated by magnetic fields. Electric currents and the magnetic moments of elementary particles give rise to a magnetic field which acts on other currents and magnetic moments. Every material is influenced to some extent by a magnetic field. The most familiar effect is on permanent magnets which have persistent magnetic moments caused by ferromagnetism. The prefix ferro refers to iron because permanent magnetism was first observed in a form of natural iron ore called magnetite Fe3O4. Most materials do not have permanent moments. Some are attracted to a magnetic field paramagnetism others are repulsed by a magnetic field diamagnetism others have a more complex relationship with an applied magnetic field spin glass behavior and antiferromagnetism. Substances that are negligibly affected by magnetic fields are known as nonmagnetic substances. These include copper aluminium gases and plastic. Pure oxygen exhibits magnetic properties when cooled to a liquid state. The magnetic state or magnetic phase of a material depends on temperature and other variables such as pressure and the applied magnetic field. A material may exhibit more than one form of magnetism as these variables change.  Magnetism on In Our Time at the BBC. listen now The Exploratorium Science Snacks Snacks about Magnetism Electromagnetism  a chapter from an online textbook Video The physicist Richard Feynman answers the question Why do bar magnets attract or repel each other on YouTube On the Magnet 1600 First scientific book on magnetism by the father of electrical engineering. Full English text full text search. Magnetism and magnetization  Astronoo"}, {"topic": "Mass balance", "content": "A mass balance also called a material balance is an application of conservation of mass to the analysis of physical systems. By accounting for material entering and leaving a system mass flows can be identified which might have been unknown or difficult to measure without this technique. The exact conservation law used in the analysis of the system depends on the context of the problem but all revolve around mass conservation i.e. that matter cannot disappear or be created spontaneously. Therefore mass balances are used widely in engineering and environmental analyses. For example mass balance theory is used to design chemical reactors to analyse alternative processes to produce chemicals as well as to model pollution dispersion and other processes of physical systems. Closely related and complementary analysis techniques include the population balance energy balance and the somewhat more complex entropy balance. These techniques are required for thorough design and analysis of systems such as the refrigeration cycle. In environmental monitoring the term budget calculations is used to describe mass balance equations where they are used to evaluate the monitoring data comparing input and output etc. In biology the dynamic energy budget theory for metabolic organisation makes explicit use of mass and energy balances.  Material Balance Calculations Material Balance Fundamentals The Material Balance for Chemical Reactors Material and energy balance Heat and material balance method of process control for petrochemical plants and oil refineries United States Patent 6751527 Morris Arthur E. Geiger Gordon Fine H. Alan 2011. Handbook on Material and Energy Balance Calculations in Material Processing 3rd ed.. Wiley. ISBN 9781118065655."}, {"topic": "Mass density", "content": "The density or more precisely the volumetric mass density of a substance is its mass per unit volume. The symbol most often used for density is the lower case Greek letter rho although the Latin letter D can also be used. Mathematically density is defined as mass divided by volume  m V    Density. Encyclopdia Britannica 8 11th ed.. 1911. Density. The New Students Reference Work. 1914. Video Density Experiment with Oil and Alcohol Video Density Experiment with Whiskey and Water Glass Density Calculation Calculation of the density of glass at room temperature and of glass melts at 1000 1400C List of Elements of the Periodic Table Sorted by Density Calculation of saturated liquid densities for some components Field density test Online calculator for densities and partial molar volumes of aqueous solutions of some common electrolytes and their mixtures at temperatures up to 323.15 K. Water Density and specific weight Temperature dependence of the density of water Conversions of density units A delicious density experiment Water density calculator Water density for a given salinity and temperature. Liquid density calculator Select a liquid from the list and calculate density as a function of temperature. Gas density calculator Calculate density of a gas for as a function of temperature and pressure. Densities of various materials. Determination of Density of Solid instructions for performing classroom experiment. density prediction density prediction"}, {"topic": "Mass flux", "content": "In physics and engineering mass flux is the rate of mass flow per unit area perfectly overlapping with the momentum density the momentum per unit volume. The common symbols are j J q Q  or Greek lower or capital Phi sometimes with subscript m to indicate mass is the flowing quantity. Its SI units are kg s1 m2. Mass flux can also refer to an alternate form of flux in Ficks law that includes the molecular mass or in Darcys law that includes the mass density. Unfortunately sometimes the defining equation for mass flux in this article is used interchangeably with the defining equation in mass flow rate. For example Fluid Mechanics Schaums et al uses the definition of mass flux as the equation in the mass flow rate article. "}, {"topic": "Mass moment of inertia", "content": "The moment of inertia otherwise known as the angular mass or rotational inertia of a rigid body determines the torque needed for a desired angular acceleration about a rotational axis. It depends on the bodys mass distribution and the axis chosen with larger moments requiring more torque to change the bodys rotation. It is an extensive additive property the moment of inertia of a composite system is the sum of the moments of inertia of its component subsystems all taken about the same axis. One of its definitions is the second moment of mass with respect to distance from an axis r I  Q r 2 d m   Angular momentum and rigidbody rotation in two and three dimensions Lecture notes on rigidbody rotation and moments of inertia The moment of inertia tensor An introductory lesson on moment of inertia keeping a vertical pole not falling down Java simulation Tutorial on finding moments of inertia with problems and solutions on various basic shapes Notes on mechanics of manipulation the angular inertia tensor"}, {"topic": "Mass number", "content": "The mass number A also called atomic mass number or nucleon number is the total number of protons and neutrons together known as nucleons in an atomic nucleus. It determines the atomic mass of atoms. Because protons and neutrons both are baryons the mass number A is identical with the baryon number B as of the nucleus as of the whole atom or ion. The mass number is different for each different isotope of a chemical element. This is not the same as the atomic number Z which denotes the number of protons in a nucleus and thus uniquely identifies an element. Hence the difference between the mass number and the atomic number gives the number of neutrons N in a given nucleus NAZ. The mass number is written either after the element name or as a superscript to the left of an elements symbol. For example the most common isotope of carbon is carbon12 or 12C which has 6 protons and 6 neutrons. The full isotope symbol would also have the atomic number Z as a subscript to the left of the element symbol directly below the mass number 12 6C. This is technically redundant as each element is defined by its atomic number so it is often omitted.  Bishop Mark. The Structure of Matter and Chemical Elements ch. 3. An Introduction to Chemistry. Chiral Publishing. p. 93. ISBN 9780977810543. Retrieved 20080708."}, {"topic": "Mass spectrometry", "content": "Mass spectrometry MS is an analytical technique that ionizes chemical species and sorts the ions based on their mass to charge ratio. In simpler terms a mass spectrum measures the masses within a sample. Mass spectrometry is used in many different fields and is applied to pure samples as well as complex mixtures. A mass spectrum is a plot of the ion signal as a function of the masstocharge ratio. These spectra are used to determine the elemental or isotopic signature of a sample the masses of particles and of molecules and to elucidate the chemical structures of molecules such as peptides and other chemical compounds. In a typical MS procedure a sample which may be solid liquid or gas is ionized for example by bombarding it with electrons. This may cause some of the samples molecules to break into charged fragments. These ions are then separated according to their masstocharge ratio typically by accelerating them and subjecting them to an electric or magnetic field ions of the same masstocharge ratio will undergo the same amount of deflection. The ions are detected by a mechanism capable of detecting charged particles such as an electron multiplier. Results are displayed as spectra of the relative abundance of detected ions as a function of the masstocharge ratio. The atoms or molecules in the sample can be identified by correlating known masses to the identified masses or through a characteristic fragmentation pattern.  Mass Spectrometry at DMOZ Interactive tutorial on mass spectra National High Magnetic Field Laboratory Mass spectrometer simulation An interactive application simulating the console of a mass spectrometer Realtime Mass Spectra simulation Tool to simulate mass spectra in the browser"}, {"topic": "Material physics", "content": "Material physics is the use of physics to describe the physical properties of materials. It is a synthesis of physical sciences such as chemistry solid mechanics solid state physics and materials science. Materials physics is considered a subset of condensed matter physics and applies fundamental condensed matter concepts to complex multiphase media including materials of technological interest. Current fields that materials physicists work in include electronic optical and magnetic materials novel materials and structures quantum phenomena in materials nonequilibrium physics and soft condensed matter physics. New experimental and computational tools are constantly improving how materials systems are modeled and studied and are also fields when materials physicists work in.  Material science"}, {"topic": "Material properties", "content": "A materials property is an intensive often quantitative property of some material. Quantitative properties may be used as a metric by which the benefits of one material versus another can be assessed thereby aiding in materials selection. A property may be a constant or may be a function of one or more independent variables such as temperature. Materials properties often vary to some degree according to the direction in the material in which they are measured a condition referred to as anisotropy. Materials properties that relate to different physical phenomena often behave linearly or approximately so in a given operating range. Modeling them as linear can significantly simplify the differential constitutive equations that the property describes. Some materials properties are used in relevant equations to predict the attributes of a system a priori. For example if a material of a known specific heat gains or loses a known amount of heat the temperature change of that material can be determined. Materials properties are most reliably measured by standardized test methods. Many such test methods have been documented by their respective user communities and published through ASTM International.  Chemical property Physical property Supervenience Thermodynamic properties"}, {"topic": "Materials science", "content": "The interdisciplinary field of materials science also commonly known as materials science and engineering involves the discovery and design of new materials with an emphasis on solids. The intellectual origins of materials science stem from the Enlightenment when researchers began to use analytical thinking from chemistry physics and engineering to understand ancient phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics chemistry and engineering. As such the field was long thought of as a subfield of these related fields. In recent years materials science has become more widely recognized as a specific and distinct field of science and engineering. Many of the most pressing scientific problems humans currently face are due to the limitations of the materials that are available and as a result breakthroughs in materials science are likely to affect the future of technology significantly. Materials scientists emphasize understanding how the history of a material its processing influences its structure and thus the materials properties and performance. The understanding of processingstructureproperties relationships is called the materials paradigm. This paradigm is used to advance understanding in a variety of research areas including nanotechnology biomaterials and metallurgy. Materials science is also an important part of forensic engineering and failure analysis  investigating materials products structures or components which fail or which do not operate or function as intended causing personal injury or damage to property. Such investigations are key to understanding for example the causes of various aviation accidents.  The Institute of Making UCL Materials science at DMOZ"}, {"topic": "Mathematical physics", "content": "Mathematical physics refers to development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories. It is a branch of applied mathematics but deals with physical problems.  Baez John C. Muniain Javier P. 1994 Gauge fields knots and gravity Singapore  River Edge World Scientific ISBN 9810220340 pbk. Geroch Robert 1985 Mathematical physics Chicago University of Chicago Press ISBN 0226288625 pbk. Polyanin Andrei D. 2002 Handbook of linear partial differential equations for engineers and scientists Boca Raton Chapman  Hall  CRC Press ISBN 1584882999 Polyanin Alexei D. Zaitsev Valentin F. 2004 Handbook of nonlinear partial differential equations Boca Raton Chapman  Hall  CRC Press ISBN 1584883553 Szekeres Peter 2004 A course in modern mathematical physics groups Hilbert space and differential geometry Cambridge New York Cambridge University Press ISBN 0521536456 pbk. Yndurain Francisco J 2006 Theoretical and Mathematical Physics. The Theory of Quark and Gluon Interactions Berlin Springer ISBN 9783642069741 pbk."}, {"topic": "Mathematics", "content": "Mathematics from Greek mthma knowledge study learning is the study of topics such as quantity numbers structure space and change. There is a range of views among mathematicians and philosophers as to the exact scope and definition of mathematics. Mathematicians seek out patterns and use them to formulate new conjectures. Mathematicians resolve the truth or falsity of conjectures by mathematical proof. When mathematical structures are good models of real phenomena then mathematical reasoning can provide insight or predictions about nature. Through the use of abstraction and logic mathematics developed from counting calculation measurement and the systematic study of the shapes and motions of physical objects. Practical mathematics has been a human activity for as far back as written records exist. The research required to solve mathematical problems can take years or even centuries of sustained inquiry. Rigorous arguments first appeared in Greek mathematics most notably in Euclids Elements. Since the pioneering work of Giuseppe Peano 18581932 David Hilbert 18621943 and others on axiomatic systems in the late 19th century it has become customary to view mathematical research as establishing truth by rigorous deduction from appropriately chosen axioms and definitions. Mathematics developed at a relatively slow pace until the Renaissance when mathematical innovations interacting with new scientific discoveries led to a rapid increase in the rate of mathematical discovery that has continued to the present day. Galileo Galilei 15641642 said The universe cannot be read until we have learned the language and become familiar with the characters in which it is written. It is written in mathematical language and the letters are triangles circles and other geometrical figures without which means it is humanly impossible to comprehend a single word. Without these one is wandering about in a dark labyrinth. Carl Friedrich Gauss 17771855 referred to mathematics as the Queen of the Sciences. Benjamin Peirce 18091880 called mathematics the science that draws necessary conclusions. David Hilbert said of mathematics We are not speaking here of arbitrariness in any sense. Mathematics is not like a game whose tasks are determined by arbitrarily stipulated rules. Rather it is a conceptual system possessing internal necessity that can only be so and by no means otherwise. Albert Einstein 18791955 stated that as far as the laws of mathematics refer to reality they are not certain and as far as they are certain they do not refer to reality. Mathematics is essential in many fields including natural science engineering medicine finance and the social sciences. Applied mathematics has led to entirely new mathematical disciplines such as statistics and game theory. Mathematicians also engage in pure mathematics or mathematics for its own sake without having any application in mind. There is no clear line separating pure and applied mathematics and practical applications for what began as pure mathematics are often discovered. "}, {"topic": "Matrix (mathematics)", "content": "In mathematics a matrix plural matrices is a rectangular array of numbers symbols or expressions arranged in rows and columns. The dimensions of matrix 1 are 2 3 read two by three because there are two rows and three columns. The individual items in a matrix are called its elements or entries. Provided that they are the same size have the same number of rows and the same number of columns two matrices can be added or subtracted element by element. The rule for matrix multiplication however is that two matrices can be multiplied only when the number of columns in the first equals the number of rows in the second. Any matrix can be multiplied elementwise by a scalar from its associated field. A major application of matrices is to represent linear transformations that is generalizations of linear functions such as fx  4x. For example the rotation of vectors in three dimensional space is a linear transformation which can be represented by a rotation matrix R if v is a column vector a matrix with only one column describing the position of a point in space the product Rv is a column vector describing the position of that point after a rotation. The product of two transformation matrices is a matrix that represents the composition of two linear transformations. Another application of matrices is in the solution of systems of linear equations. If the matrix is square it is possible to deduce some of its properties by computing its determinant. For example a square matrix has an inverse if and only if its determinant is not zero. Insight into the geometry of a linear transformation is obtainable along with other information from the matrixs eigenvalues and eigenvectors. Applications of matrices are found in most scientific fields. In every branch of physics including classical mechanics optics electromagnetism quantum mechanics and quantum electrodynamics they are used to study physical phenomena such as the motion of rigid bodies. In computer graphics they are used to project a 3D model onto a 2 dimensional screen. In probability theory and statistics stochastic matrices are used to describe sets of probabilities for instance they are used within the PageRank algorithm that ranks the pages in a Google search. Matrix calculus generalizes classical analytical notions such as derivatives and exponentials to higher dimensions. A major branch of numerical analysis is devoted to the development of efficient algorithms for matrix computations a subject that is centuries old and is today an expanding area of research. Matrix decomposition methods simplify computations both theoretically and practically. Algorithms that are tailored to particular matrix structures such as sparse matrices and neardiagonal matrices expedite computations in finite element method and other computations. Infinite matrices occur in planetary theory and in atomic theory. A simple example of an infinite matrix is the matrix representing the derivative operator which acts on the Taylor series of a function.  Encyclopedic articles Hazewinkel Michiel ed. 2001 Matrix Encyclopedia of Mathematics Springer ISBN 9781556080104 History MacTutor Matrices and determinants Matrices and Linear Algebra on the Earliest Uses Pages Earliest Uses of Symbols for Matrices and Vectors Online books Kaw Autar K. Introduction to Matrix Algebra ISBN 9780615251264 The Matrix Cookbook PDF retrieved 24 March 2014 Brookes Mike 2005 The Matrix Reference Manual London Imperial College retrieved 10 Dec 2008 Online matrix calculators SimplyMath Matrix Calculator Free C Library Matrix Calculator DotNumerics Xiao Gang Matrix calculator retrieved 10 Dec 2008 Online matrix calculator retrieved 10 Dec 2008 Online matrix calculator ZK framework retrieved 26 Nov 2009 Oehlert Gary W. Bingham Christopher MacAnova University of Minnesota School of Statistics retrieved 10 Dec 2008  a freeware package for matrix algebra and statistics Online matrix calculator retrieved 14 Dec 2009 Operation with matrices in R determinant track inverse adjoint transpose"}, {"topic": "Matter", "content": "Before the 20th century the term matter included ordinary matter composed of atoms and excluded other energy phenomena such as light or sound. This concept of matter may be generalized from atoms to include any objects having mass even when at rest but this is illdefined because an objects mass can arise from its possibly massless constituents motion and interaction energies. Thus matter does not have a universal definition nor is it a fundamental concept in physics today. Matter is also used loosely as a general term for the substance that makes up all observable physical objects. All the objects from everyday life that we can bump into touch or squeeze are composed of atoms. This atomic matter is in turn made up of interacting subatomic particlesusually a nucleus of protons and neutrons and a cloud of orbiting electrons. Typically science considers these composite particles matter because they have both rest mass and volume. By contrast massless particles such as photons are not considered matter because they have neither rest mass nor volume. However not all particles with rest mass have a classical volume since fundamental particles such as quarks and leptons sometimes equated with matter are considered point particles with no effective size or volume. Nevertheless quarks and leptons together make up ordinary matter and their interactions contribute to the effective volume of the composite particles that make up ordinary matter. Matter exists in states or phases the classical solid liquid and gas as well as the more exotic plasma BoseEinstein condensates fermionic condensates and quarkgluon plasma. For much of the history of the natural sciences people have contemplated the exact nature of matter. The idea that matter was built of discrete building blocks the socalled particulate theory of matter was first put forward by the Greek philosophers Leucippus 490 BC and Democritus 470380 BC.  Visionlearning Module on Matter Matter in the universe How much Matter is in the Universe NASA on superfluid core of neutron star Matter and Energy A False Dichotomy Conversations About Science with Theoretical Physicist Matt Strassler"}, {"topic": "Maxwell's equations", "content": "Maxwells equations are a set of partial differential equations that together with the Lorentz force law form the foundation of classical electrodynamics classical optics and electric circuits. These fields in turn underlie modern electrical and communications technologies. Maxwells equations describe how electric and magnetic fields are generated and altered by each other and by charges and currents. They are named after the physicist and mathematician James Clerk Maxwell who published an early form of those equations between 1861 and 1862. The equations have two major variants. The microscopic set of Maxwells equations uses total charge and total current including the complicated charges and currents in materials at the atomic scale it has universal applicability but may be infeasible to calculate. The macroscopic set of Maxwells equations defines two new auxiliary fields that describe largescale behaviour without having to consider these atomic scale details but it requires the use of parameters characterizing the electromagnetic properties of the relevant materials. The term Maxwells equations is often used for other forms of Maxwells equations. For example spacetime formulations are commonly used in high energy and gravitational physics. These formulations defined on spacetime rather than space and time separately are manifestly compatible with special and general relativity. In quantum mechanics and analytical mechanics versions of Maxwells equations based on the electric and magnetic potentials are preferred. Since the mid20th century it has been understood that Maxwells equations are not exact but are a classical field theory approximation to the more accurate and fundamental theory of quantum electrodynamics. In many situations though deviations from Maxwells equations are immeasurably small. Exceptions include nonclassical light photonphoton scattering quantum optics and many other phenomena related to photons or virtual photons.  Feynmans derivation of Maxwell equations and extra dimensions Nature Milestones Photons Milestone 2 1861 Maxwells equations"}, {"topic": "Mean", "content": "In mathematics mean has several different definitions depending on the context. In probability and statistics mean and expected value are used synonymously to refer to one measure of the central tendency either of a probability distribution or of the random variable characterized by that distribution. In the case of a discrete probability distribution of a random variable X the mean is equal to the sum over every possible value weighted by the probability of that value that is it is computed by taking the product of each possible value x of X and its probability Px and then adding all these products together giving  x P  x   Often since the population variance is an unknown parameter it is estimated by the mean sum of squares when this estimated value is used the distribution of the sample mean is no longer a normal distribution but rather a Students t distribution with n 1 degrees of freedom.  Weisstein Eric W. Mean MathWorld. Weisstein Eric W. Arithmetic Mean MathWorld. Comparison between arithmetic and geometric mean of two numbers Some relationships involving means"}, {"topic": "Measures of central tendency", "content": "In statistics a central tendency or more commonly a measure of central tendency is a central or typical value for a probability distribution. It may also be called a center or location of the distribution. Colloquially measures of central tendency are often called averages. The term central tendency dates from the late 1920s. The most common measures of central tendency are the arithmetic mean the median and the mode. A central tendency can be calculated for either a finite set of values or for a theoretical distribution such as the normal distribution. Occasionally authors use central tendency to denote the tendency of quantitative data to cluster around some central value. The central tendency of a distribution is typically contrasted with its dispersion or variability dispersion and central tendency are the often characterized properties of distributions. Analysts may judge whether data has a strong or a weak central tendency based on its dispersion. "}, {"topic": "Mechanical filter", "content": "A mechanical filter is a signal processing filter usually used in place of an electronic filter at radio frequencies. Its purpose is the same as that of a normal electronic filter to pass a range of signal frequencies but to block others. The filter acts on mechanical vibrations which are the analogue of the electrical signal. At the input and output of the filter transducers convert the electrical signal into and then back from these mechanical vibrations. The components of a mechanical filter are all directly analogous to the various elements found in electrical circuits. The mechanical elements obey mathematical functions which are identical to their corresponding electrical elements. This makes it possible to apply electrical network analysis and filter design methods to mechanical filters. Electrical theory has developed a large library of mathematical forms that produce useful filter frequency responses and the mechanical filter designer is able to make direct use of these. It is only necessary to set the mechanical components to appropriate values to produce a filter with an identical response to the electrical counterpart. Steel and nickeliron alloys are common materials for mechanical filter components nickel is sometimes used for the input and output couplings. Resonators in the filter made from these materials need to be machined to precisely adjust their resonance frequency before final assembly. While the meaning of mechanical filter in this article is one that is used in an electromechanical role it is possible to use a mechanical design to filter mechanical vibrations or sound waves which are also essentially mechanical directly. For example filtering of audio frequency response in the design of loudspeaker cabinets can be achieved with mechanical components. In the electrical application in addition to mechanical components which correspond to their electrical counterparts transducers are needed to convert between the mechanical and electrical domains. A representative selection of the wide variety of component forms and topologies for mechanical filters are presented in this article. The theory of mechanical filters was first applied to improving the mechanical parts of phonographs in the 1920s. By the 1950s mechanical filters were being manufactured as selfcontained components for applications in radio transmitters and highend receivers. The high quality factor Q that mechanical resonators can attain far higher than that of an allelectrical LC circuit made possible the construction of mechanical filters with excellent selectivity. Good selectivity being important in radio receivers made such filters highly attractive. Contemporary researchers are working on microelectromechanical filters the mechanical devices corresponding to electronic integrated circuits. "}, {"topic": "Mechanical wave", "content": "A mechanical wave is a wave that is an oscillation of matter and therefore transfers energy through a medium. While waves can move over long distances the movement of the medium of transmissionthe materialis limited. Therefore oscillating material does not move far from its initial equilibrium position. Mechanical waves transport energy. This energy propagates in the same direction as the wave. Any kind of wave mechanical or electromagnetic has a certain energy. Mechanical waves can be produced only in media which possess elasticity and inertia. A mechanical wave requires an initial energy input. Once this initial energy is added the wave travels through the medium until all its energy is transferred. In contrast electromagnetic waves require no medium but can still travel through one. One important property of mechanical waves is that their amplitudes are measured in an unusual way displacement divided by reduced wavelength. When this gets comparable to unity significant nonlinear effects such as harmonic generation may occur and if large enough may result in chaotic effects. For example waves on the surface of a body of water break when this dimensionless amplitude exceeds 1 resulting in a foam on the surface and turbulent mixing. Some of the most common examples of mechanical waves are water waves sound waves and seismic waves. There are three types of mechanical waves transverse waves longitudinal waves and surface waves. "}, {"topic": "Mechanics", "content": "Mechanics Greek  is an area of science concerned with the behavior of physical bodies when subjected to forces or displacements and the subsequent effects of the bodies on their environment. The scientific discipline has its origins in Ancient Greece with the writings of Aristotle and Archimedes see History of classical mechanics and Timeline of classical mechanics. During the early modern period scientists such as Khayaam Galileo Kepler and Newton laid the foundation for what is now known as classical mechanics. It is a branch of classical physics that deals with particles that are either at rest or are moving with velocities significantly less than the speed of light. It can also be defined as a branch of science which deals with the motion of and forces on objects.  iMechanica the web of mechanics and mechanicians Mechanics Definition Mechanics Blog by a Purdue University Professor The Mechanics program at Virginia Tech Physclips Mechanics with animations and video clips from the University of New South Wales U.S. National Committee on Theoretical and Applied Mechanics Interactive learning resources for teaching Mechanics The Archimedes Project Engineering Fundamental Solid  Fluid Mechanics"}, {"topic": "Median", "content": "The median is the value separating the higher half of a data sample a population or a probability distribution from the lower half. In simple terms it may be thought of as the middle value of a data set. For example in the data set  to obtain the mean. The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s. Antoine Augustin Cournot in 1843 was the first to use the term median valeur mdiane for the value that divides a probability distribution into two equal halves. Gustav Theodor Fechner used the median Centralwerth in sociological and psychological phenomena. It had earlier been used only in astronomy and related fields. Gustav Fechner popularized the median into the formal analysis of data although it had been used previously by Laplace. Francis Galton used the English term median in 1881 having earlier used the terms middlemost value in 1869 and the medium in 1880.  Hazewinkel Michiel ed. 2001 Median in statistics Encyclopedia of Mathematics Springer ISBN 9781556080104 Median as a weighted arithmetic mean of all Sample Observations Online calculator Calculating the median A problem involving the mean the median and the mode. Weisstein Eric W. Statistical Median MathWorld. Python script for Median computations and income inequality metrics Fast Computation of the Median by Successive Binning This article incorporates material from Median of a distribution on PlanetMath which is licensed under the Creative Commons AttributionShareAlike License."}, {"topic": "Medical imaging", "content": "Medical imaging is the technique and process of creating visual representations of the interior of a body for clinical analysis and medical intervention as well as visual representation of the function of some organs or tissues physiology. Medical imaging seeks to reveal internal structures hidden by the skin and bones as well as to diagnose and treat disease. Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. Although imaging of removed organs and tissues can be performed for medical reasons such procedures are usually considered part of pathology instead of medical imaging. As a discipline and in its widest sense it is part of biological imaging and incorporates radiology which uses the imaging technologies of Xray radiography magnetic resonance imaging medical ultrasonography or ultrasound endoscopy elastography tactile imaging thermography medical photography and nuclear medicine functional imaging techniques as positron emission tomography PET and Singlephoton emission computed tomography SPECT. Measurement and recording techniques which are not primarily designed to produce images such as electroencephalography EEG magnetoencephalography MEG electrocardiography ECG and others represent other technologies which produce data susceptible to representation as a parameter graph vs. time or maps which contain data about the measurement locations. In a limited comparison these technologies can be considered as forms of medical imaging in another discipline. Up until 2010 5 billion medical imaging studies had been conducted worldwide. Radiation exposure from medical imaging in 2006 made up about 50 of total ionizing radiation exposure in the United States. Medical imaging is often perceived to designate the set of techniques that noninvasively produce images of the internal aspect of the body. In this restricted sense medical imaging can be seen as the solution of mathematical inverse problems. This means that cause the properties of living tissue is inferred from effect the observed signal. In the case of medical ultrasonography the probe consists of ultrasonic pressure waves and echoes that go inside the tissue to show the internal structure. In the case of projectional radiography the probe uses Xray radiation which is absorbed at different rates by different tissue types such as bone muscle and fat. The term noninvasive is used to denote a procedure where no instrument is introduced into a patients body which is the case for most imaging techniques used.  Oldest Medical Imaging Group UPenn Medical Imaging Processing Group MIPG Medical imaging at DMOZ MedPix Free Medical Image Database Search  Download Images IPRG Open group related to image processing research resources An introduction to some of the more exciting recent advances and dynamic current areas of development in biomedical Raman spectroscopy including videorate biomedical imaging. Illuminating disease and enlightening biomedicine Raman spectroscopy as a diagnostic tool. httppubs.rsc.orgencontentarticlepdf2013ANC3AN00698K Analyst 2013138 38713884 Medical Imaging Explained Understanding the types and principles of medical imaging will support knowledge about a variety of machines and techniques about diagnostic imaging. Biomedical Imaging and Signal Analysis Henry Stewart Talks Series on Medical Imaging Imaging Techniques for Preclinical and Clinical Applications"}, {"topic": "Medical physics", "content": "Medical physics also called biomedical physics medical biophysics or applied physics in medicine is generally speaking the application of physics concepts theories and methods to medicine or healthcare. Medical physics departments may be found in hospitals or universities. In the case of hospital work the term Medical Physicist is the title of a specific healthcare profession with a specific mission statement see below. Such Medical Physicists are often found in the following healthcare specialties diagnostic and intervention radiology also known as medical imaging nuclear medicine and radiation oncology also known as radiotherapy. However areas of specialty are widely varied in scope and breadth e.g. clinical physiology also known as physiological measurement several countries neurophysiology Finland radiation protection many countries and audiology Netherlands. University departments are of two types. The first type are mainly concerned with preparing students for a career as a hospital medical physicist and research focuses on improving the practice of the profession. A second type increasingly called biomedical physics has a much wider scope and may include research in any applications of physics to medicine from the study of biomolecular structure to microscopy and nanomedicine.  Human Health Campus The official website of the International Atomic Energy Agency dedicated to Professionals in Radiation Medicine. This site is managed by the Division of Human Health Department of Nuclear Sciences and Applications The American Association of Physicists in Medicine Romanian College of Medical Physicists medicalphysicsweb.org from the Institute of Physics AIP Medical Physics portal Institute of Physics  Engineering in Medicine IPEM  UK European Federation of Organizations for Medical Physics EFOMP International Organization for Medical Physics IOMP"}, {"topic": "Melting", "content": "Melting also known as fusion is a physical process that results in the phase transition of a substance from a solid to a liquid. This occurs when the internal energy of the solid increases typically by the application of heat or pressure which increases the substances temperature to the melting point. At the melting point the ordering of ions or molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. An object that has melted completely is molten although this word is typically used for substances that melt only at a high temperature such as molten iron or molten lava. Substances in the molten state generally have reduced viscosity as the temperature increases. An exception to this principle is the element sulfur whose viscosity increases to a point due to polymerization and then decreases with higher temperatures in its molten state. Some organic compounds melt through mesophases states of partial order between solid and liquid. "}, {"topic": "Meson", "content": "In particle physics mesons miznz or mznz are hadronic subatomic particles composed of one quark and one antiquark bound together by the strong interaction. Because mesons are composed of subparticles they have a physical size with a diameter of roughly one fermi which is about 23 the size of a proton or neutron. All mesons are unstable with the longestlived lasting for only a few hundredths of a microsecond. Charged mesons decay sometimes through intermediate particles to form electrons and neutrinos. Uncharged mesons may decay to photons. Both of these decays imply that color is no longer a property of the by products. Mesons are not produced by radioactive decay but appear in nature only as shortlived products of very highenergy interactions in matter between particles made of quarks. In cosmic ray interactions for example such particles are ordinary protons and neutrons. Mesons are also frequently produced artificially in highenergy particle accelerators that collide protons antiprotons or other particles. In nature the importance of lighter mesons is that they are the associated quantumfield particles that transmit the nuclear force in the same way that photons are the particles that transmit the electromagnetic force. The higher energy more massive mesons were created momentarily in the Big Bang but are not thought to play a role in nature today. However such particles are regularly created in experiments in order to understand the nature of the heavier types of quark that compose the heavier mesons. Mesons are part of the hadron particle family and are defined simply as particles composed of two quarks. The other members of the hadron family are the baryons subatomic particles composed of three quarks rather than two. Some experiments show evidence of exotic mesons which do not have the conventional valence quark content of one quark and one antiquark. Because quarks have a spin of 12 the difference in quarknumber between mesons and baryons results in conventional twoquark mesons being bosons whereas baryons are fermions. Each type of meson has a corresponding antiparticle antimeson in which quarks are replaced by their corresponding antiquarks and vice versa. For example a positive pion  is made of one up quark and one down antiquark and its corresponding antiparticle the negative pion  is made of one up antiquark and one down quark. Because mesons are composed of quarks they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content total angular momentum parity and various other properties such as Cparity and Gparity. Although no meson is stable those of lower mass are nonetheless more stable than the most massive mesons and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons meaning that they are more easily produced in experiments and thus exhibit certain higher energy phenomena more readily than baryons composed of the same quarks would. For example the charm quark was first seen in the JPsi meson J in 1974 and the bottom quark in the upsilon meson  in 1977.  What Happened to the Antimatter Fermilabs DZero Experiment Finds Clues in QuickChange Meson CDF experiments definitive observation of matterantimatter oscillations in the Bs meson"}, {"topic": "Mesoscopic physics", "content": "Disambiguation This page refers to the subdiscipline of condensed matter physics not the branch of meteorology concerned with the study of weather systems smaller than synoptic scale systems. Mesoscopic physics is a subdiscipline of condensed matter physics that deals with materials of an intermediate length. The scale of these materials can be described as being between the size of a quantity of atoms such as a molecule and of materials measuring micrometres. The lower limit can also be defined as being the size of individual atoms. At the micrometre level are bulk materials. Both mesoscopic and macroscopic objects contain a large number of atoms. Whereas average properties derived from its constituent materials describe macroscopic objects as they usually obey the laws of classical mechanics a mesoscopic object by contrast is affected by fluctuations around the average and is subject to quantum mechanics. In other words a macroscopic device when scaled down to a mesosize starts revealing quantum mechanical properties. For example at the macroscopic level the conductance of a wire increases continuously with its diameter. However at the mesoscopic level the wires conductance is quantized the increases occur in discrete or individual whole steps. During research mesoscopic devices are constructed measured and observed experimentally and theoretically in order to advance understanding of the physics of insulators semiconductors metals and superconductors. The applied science of mesoscopic physics deals with the potential of building nanodevices. Mesoscopic physics also addresses fundamental practical problems which occur when a macroscopic object is miniaturized as with the miniaturization of transistors in semiconductor electronics. The physical properties of materials change as their size approaches the nanoscale where the percentage of atoms at the surface of the material becomes significant. For bulk materials larger than one micrometre the percentage of atoms at the surface is insignificant in relation to the number of atoms in the entire material. The subdiscipline has dealt primarily with artificial structures of metal or semiconducting material which have been fabricated by the techniques employed for producing microelectronic circuits. There is no rigid definition for mesoscopic physics but the systems studied are normally in the range of 100 nm the size of a typical virus to 1 000 nm the size of a typical bacterium 100 nanometers is the approximate upper limit for a nanoparticle. Thus mesoscopic physics has a close connection to the fields of nanofabrication and nanotechnology. Devices used in nanotechnology are examples of mesoscopic systems. Three categories of new phenomena in such systems are interference effects quantum confinement effects and charging effects.  How quantum dots work Quantum dots synthesis Biological application Mesoscopic transport and quantum chaos Rodolfo A. Jalabert Scholarpedia 11130946. doi10.4249scholarpedia.30946"}, {"topic": "Mode (statistics)", "content": "The mode is the value that appears most often in a set of data. The mode of a discrete probability distribution is the value x at which its probability mass function takes its maximum value. In other words it is the value that is most likely to be sampled. The mode of a continuous probability distribution is the value x at which its probability density function has its maximum value so the mode is at the peak. Like the statistical mean and median the mode is a way of expressing in a single number important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution and it may be very different in highly skewed distributions. The mode is not necessarily unique to a given distribution since the probability mass function or probability density function may take the same maximum value at several points x1 x2 etc. The most extreme case occurs in uniform distributions where all values occur equally frequently. When a probability density function has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal as opposed to unimodal. In symmetric unimodal distributions such as the normal distribution the mean if defined median and mode all coincide. For samples if it is known that they are drawn from a symmetric distribution the sample mean can be used as an estimate of the population mode.  Hazewinkel Michiel ed. 2001 Mode Encyclopedia of Mathematics Springer ISBN 9781556080104 A Guide to Understanding  Calculating the Mode Weisstein Eric W. Mode MathWorld. Mean Median and Mode short beginner video from Khan Academy"}, {"topic": "Modulus of elasticity", "content": "The modulus of elasticity also known as the elastic modulus the tensile modulus or Youngs modulus is a number that measures an object or substances resistance to being deformed elastically i.e. nonpermanently when a force is applied to it. The elastic modulus of an object is defined as the slope of its stressstrain curve in the elastic deformation region A stiffer material will have a higher elastic modulus. An elastic modulus has the form.  def stress strain   describes an objects tendency to shear the deformation of shape at constant volume when acted upon by opposing forces it is defined as shear stress over shear strain. The shear modulus is part of the derivation of viscosity. The bulk modulus K describes volumetric elasticity or the tendency of an object to deform in all directions when uniformly loaded in all directions it is defined as volumetric stress over volumetric strain and is the inverse of compressibility. The bulk modulus is an extension of Youngs modulus to three dimensions. Three other elastic moduli are Axial Modulus Lams first parameter and Pwave modulus. Homogeneous and isotropic similar in all directions materials solids have their linear elastic properties fully described by two elastic moduli and one may choose any pair. Given a pair of elastic moduli all other elastic moduli can be calculated according to formulas in the table below at the end of page. Inviscid fluids are special in that they cannot support shear stress meaning that the shear modulus is always zero. This also implies that Youngs modulus is always zero. In some English texts the here described quantity is called elastic constant while the inverse quantity is referred to as elastic modulus.  Hartsuijker C. Welleman J. W. 2001. Engineering Mechanics. Volume 2. Springer. ISBN 9781402041235. De Jong Maarten Chen Wei 2015. Charting the complete elastic properties of inorganic crystalline compounds. Scientific Data 2 150009. doi10.1038sdata.2015.9."}, {"topic": "Molality", "content": "Molality also called molal concentration is a measure of the concentration of a solute in a solution in terms of amount of substance in a specified amount of mass of the solvent. This contrasts with the definition of molarity which is based on a specified volume of solution. A commonly used unit for molality in chemistry is molkg. A solution of concentration 1 molkg is also sometimes denoted as 1 molal. "}, {"topic": "Molar concentration", "content": "Molar concentration also called molarity amount concentration or substance concentration is a measure of the concentration of a solute in a solution or of any chemical species in terms of amount of substance in a given volume. A commonly used unit for molar concentration used in chemistry is molL. A solution of concentration 1 molL is also denoted as 1 molar 1 M.  Molar Solution Concentration Calculator Experiment to determine the molar concentration of vinegar by titration"}, {"topic": "Molar mass", "content": "In chemistry the molar mass M is a physical property defined as the mass of a given substance chemical element or chemical compound divided by the amount of substance. The base SI unit for molar mass is kgmol. However for historical reasons molar masses are almost always expressed in gmol. As an example the molar mass of water MH2O 699818000000000000018 gmol  HTML5 Molar Mass Calculator web and mobile application. Online Molar Mass Calculator with the uncertainty of M and all the calculations shown Molar Mass Calculator Online Molar Mass and Elemental Composition Calculator Stoichiometry AddIn for Microsoft Excel for calculation of molecular weights reaction coefficients and stoichiometry. It includes both average atomic weights and isotopic weights."}, {"topic": "Molarity", "content": "Morality from the Latin moralitas manner character proper behavior is the differentiation of intentions decisions and actions between those that are distinguished as proper and those that are improper. Morality can be a body of standards or principles derived from a code of conduct from a particular philosophy religion or culture or it can derive from a standard that a person believes should be universal. Morality may also be specifically synonymous with goodness or rightness. Moral philosophy includes moral ontology or the origin of morals as well as moral epistemology or knowledge about morals. Different systems of expressing morality have been proposed including deontological ethical systems which adhere to a set of established rules and normative ethical systems which consider the merits of actions themselves. An example of normative ethical philosophy is the Golden Rule which states that One should treat others as one would like others to treat oneself. Immorality is the active opposition to morality i.e. opposition to that which is good or right while amorality is variously defined as an unawareness of indifference toward or disbelief in any set of moral standards or principles.  The Stanford Encyclopedia of Philosophy on the Definition of Morality Boston Colleges Morality Lab Objective Morality An evolutionary approach Morality and Judaism chabad.org Wiki site for discussing and taking action on shared morals WorldMoralMovement.org Stephen Pinker on the Psychology and Evolutionary Biology of Morality"}, {"topic": "Molecular physics", "content": "Molecular physics is the study of the physical properties of molecules the chemical bonds between atoms as well as the molecular dynamics. Its most important experimental techniques are the various types of spectroscopy scattering is also used. The field is closely related to atomic physics and overlaps greatly with theoretical chemistry physical chemistry and chemical physics. Additionally to the electronic excitation states which are known from atoms molecules are able to rotate and to vibrate. These rotations and vibrations are quantized there are discrete energy levels. The smallest energy differences exist between different rotational states therefore pure rotational spectra are in the far infrared region about 30  150 m wavelength of the electromagnetic spectrum. Vibrational spectra are in the near infrared about 1  5 m and spectra resulting from electronic transitions are mostly in the visible and ultraviolet regions. From measuring rotational and vibrational spectra properties of molecules like the distance between the nuclei can be calculated. One important aspect of molecular physics is that the essential atomic orbital theory in the field of atomic physics expands to the molecular orbital theory.  BornOppenheimer approximation Molecular energy state Molecular modeling Rigid rotor Spectroscopy"}, {"topic": "Molecule", "content": "A molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. However in quantum physics organic chemistry and biochemistry the term molecule is often used less strictly also being applied to polyatomic ions. In the kinetic theory of gases the term molecule is often used for any gaseous particle regardless of its composition. According to this definition noble gas atoms are considered molecules as they are in fact monoatomic molecules. A molecule may be homonuclear that is it consists of atoms of a single chemical element as with oxygen O2 or it may be heteronuclear a chemical compound composed of more than one element as with water H2O. Atoms and complexes connected by noncovalent bonds such as hydrogen bonds or ionic bonds are generally not considered single molecules. Molecules as components of matter are common in organic substances and therefore biochemistry. They also make up most of the oceans and atmosphere. However the majority of familiar solid substances on Earth including most of the minerals that make up the crust mantle and core of the Earth contain many chemical bonds but are not made of identifiable molecules. Also no typical molecule can be defined for ionic crystals salts and covalent crystals network solids although these are often composed of repeating unit cells that extend either in a plane such as in graphene or threedimensionally such as in diamond quartz or sodium chloride. The theme of repeated unitcellularstructure also holds for most condensed phases with metallic bonding which means that solid metals are also not made of molecules. In glasses solids that exist in a vitreous disordered state atoms may also be held together by chemical bonds without presence of any definable molecule but also without any of the regularity of repeating units that characterizes crystals.  Molecule of the Month School of Chemistry University of Bristol"}, {"topic": "Moment of inertia", "content": "The moment of inertia otherwise known as the angular mass or rotational inertia of a rigid body determines the torque needed for a desired angular acceleration about a rotational axis. It depends on the bodys mass distribution and the axis chosen with larger moments requiring more torque to change the bodys rotation. It is an extensive additive property the moment of inertia of a composite system is the sum of the moments of inertia of its component subsystems all taken about the same axis. One of its definitions is the second moment of mass with respect to distance from an axis r I  Q r 2 d m   Angular momentum and rigidbody rotation in two and three dimensions Lecture notes on rigidbody rotation and moments of inertia The moment of inertia tensor An introductory lesson on moment of inertia keeping a vertical pole not falling down Java simulation Tutorial on finding moments of inertia with problems and solutions on various basic shapes Notes on mechanics of manipulation the angular inertia tensor"}, {"topic": "Momentum", "content": "In classical mechanics linear momentum translational momentum or simply momentum pl. momenta SI unit kg ms or equivalently newtonsecond is the product of the mass and velocity of an object. For example a heavy truck moving rapidly has a large momentumit takes a large or prolonged force to get the truck up to this speed and it takes a large or prolonged force to bring it to a stop afterwards. If the truck were lighter or moving more slowly then it would have less momentum. Like velocity linear momentum is a vector quantity possessing a direction as well as a magnitude p  m v   where c is the speed of sound. In a solid similar equations can be obtained for propagation of pressure Pwaves and shear Swaves. The flux or transport per unit area of a momentum component vj by a velocity vi is equal to vjvj. In the linear approximation that leads to the above acoustic equation the time average of this flux is zero. However nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.  Conservation of momentum A chapter from an online textbook"}, {"topic": "Motion (physics)", "content": "In physics motion is a change in position of an object with respect to time. Motion is typically described in terms of displacement distance velocity acceleration time and speed. Motion of a body is observed by attaching a frame of reference to an observer and measuring the change in position of the body relative to that frame. If the position of a body is not changing with respect to a given frame of reference the body is said to be at rest motionless immobile stationary or to have constant timeinvariant position. An objects motion cannot change unless it is acted upon by a force as described. Momentum is a quantity which is used for measuring motion of an object. An objects momentum is directly related to the objects mass and velocity and the total momentum of all objects in an isolated system one not affected by external forces does not change with time as described by the law of conservation of momentum. As there is no absolute frame of reference absolute motion cannot be determined. Thus everything in the universe can be considered to be moving. More generally motion is a concept that applies to objects bodies and matter particles to radiation radiation fields and radiation particles and to space its curvature and spacetime. One can also speak of motion of shapes and boundaries. So the term motion in general signifies a continuous change in the configuration of a physical system. For example one can talk about motion of a wave or about motion of a quantum particle where the configuration consists of probabilities of occupying specific positions. "}, {"topic": "Muon", "content": "The muon mjun from the Greek letter mu  used to represent it is an elementary particle similar to the electron with an electric charge of 1 e and a spin of 12 but with a much greater mass. It is classified as a lepton. As is the case with other leptons the muon is not believed to have any substructurethat is it is not thought to be composed of any simpler particles. The muon is an unstable subatomic particle with a mean lifetime of 69942200000000000002.2 s. Among all known unstable subatomic particles only the neutron lasting around 15 minutes has a longer decay lifetime others decay significantly faster. Muon decay is mediated by the weak interaction exclusively as is neutron decay. Muon decay always produces at least three particles which must include an electron of the same charge as the muon and two neutrinos of different types. Like all elementary particles the muon has a corresponding antiparticle of opposite charge 1 e but equal mass and spin the antimuon also called a positive muon. Muons are denoted by and antimuons by . Muons were previously called mu mesons but are not classified as mesons by modern particle physicists see History and that name is no longer used by the physics community. Muons have a mass of 7002105700000000000105.7 MeVc2 which is about 207 times that of the electron. Due to their greater mass muons are not as sharply accelerated when they encounter electromagnetic fields and do not emit as much bremsstrahlung deceleration radiation. This allows muons of a given energy to penetrate far more deeply into matter than electrons since the deceleration of electrons and muons is primarily due to energy loss by the bremsstrahlung mechanism. As an example socalled secondary muons generated by cosmic rays hitting the atmosphere can penetrate to the Earths surface and even into deep mines. Because muons have a very large mass and energy compared with the decay energy of radioactivity they are never produced by radioactive decay. They are however produced in copious amounts in highenergy interactions in normal matter in certain particle accelerator experiments with hadrons or naturally in cosmic ray interactions with matter. These interactions usually produce pi mesons initially which most often decay to muons. As with the case of the other charged leptons the muon has an associated muon neutrino denoted by  which is not the same particle as the electron neutrino and does not participate in the same nuclear reactions.  Muon anomalous magnetic moment and supersymmetry g2 muon anomalous magnetic moment experiment muLan Measurement of the Positive Muon Lifetime experiment The Review of Particle Physics The TRIUMF Weak Interaction Symmetry Test The MEG Experiment Search for the decay Muon Positron  Gamma King Philip. Making Muons. Backstage Science. Brady Haran."}, {"topic": "Nanoengineering", "content": "Nanoengineering is the practice of engineering on the nanoscale. It derives its name from the nanometre a unit of measurement equalling one billionth of a meter. Nanoengineering is largely a synonym for nanotechnology but emphasizes the engineering rather than the pure science aspects of the field.  Nanoengineering Alliance of the Fraunhofer Society"}, {"topic": "Nanotechnology", "content": "Nanotechnology nanotech is manipulation of matter on an atomic molecular and supramolecular scale. The earliest widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantumrealm scale and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form nanotechnologies as well as nanoscale technologies to refer to the broad range of research and applications whose common trait is size. Because of the variety of potential applications including industrial and military governments have invested billions of dollars in nanotechnology research. Until 2012 through its National Nanotechnology Initiative the USA has invested 3.7 billion dollars the European Union has invested 1.2 billion and Japan 750 million dollars. Nanotechnology as defined by size is naturally very broad including fields of science as diverse as surface science organic chemistry molecular biology semiconductor physics microfabrication etc. The associated research and applications are equally diverse ranging from extensions of conventional device physics to completely new approaches based upon molecular selfassembly from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale. Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications such as in nanomedicine nanoelectronics biomaterials energy production and consumer products. On the other hand nanotechnology raises many of the same issues as any new technology including concerns about the toxicity and environmental impact of nanomaterials and their potential effects on global economics as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.  Nanotechnology at DMOZ What is Nanotechnology A VegaBBCOU Video Discussion."}, {"topic": "Navier-Stokes equations", "content": "In physics the NavierStokes equations nvje stoks named after ClaudeLouis Navier and George Gabriel Stokes describe the motion of viscous fluid substances. These balance equations arise from applying Newtons second law to fluid motion together with the assumption that the stress in the fluid is the sum of a diffusing viscous term proportional to the gradient of velocity and a pressure termhence describing viscous flow. The main difference between them and the simpler Euler equations for inviscid flow is that NavierStokes equations also in the Froude limit no external field are not conservation equations but rather a dissipative system in the sense that they cannot be put into the quasilinear homogeneous form y t  A  y  y x  0  is a constant neither does it deal with the uniqueness of the NavierStokes equations with respect to any turbulence properties. It is also worth pointing out that the components of the velocity vector are exactly those from the Pythagorean quadruple parametrization. Other choices of density and pressure are possible with the same velocity field  Simplified derivation of the NavierStokes equations Millennium Prize problem description CFD online software list A compilation of codes including NavierStokes solvers Online fluid dynamics simulator Solves the NavierStokes equation numerically and visualizes it using Javascript Threedimensional unsteady form of the NavierStokes equations Glenn Research Center NASA"}, {"topic": "Neurophysics", "content": "Neurophysics or neural physics is the branch of medical physics dealing with the nervous system including the brain and the spinal cord and the nerves. The term is a portmanteau of neuron and physics to represent an emerging science which investigates the fundamentally physical basis for the brain hence the physical structure involved in the cognition process. The field covers a wide spectrum of phenomena from molecular and cellular mechanisms to techniques to measure and influence the brain and to theories of brain function. It can be viewed as an approach to neuroscience that is based on solid understanding of the fundamental laws of nature. Yasser Roudi received the Eric Kandel Young Neuroscientists Prize for his contributions to the applications of statistical physics to network reconstruction and the understanding of information processing in neuronal networks in 2015.  Dynamical Neuroscience Laboratory Georgia State University Atlanta Ga Institute of Theoretical Neurophysics Bremen Germany The Neurophysics Lab The Hebrew University of Jerusalem Neurophysics Group at the University of Pennsylvania Max Planck Institute for Dynamics and Self Organization"}, {"topic": "Neutrino", "content": "A neutrino nutrino or njutrino denoted by the Greek letter  is a lepton an elementary particle with halfinteger spin that interacts only via the weak subatomic force and gravity. The mass of the neutrino is tiny compared to other subatomic particles. Neutrinos were so named because they are electrically neutral are not affected by the electromagnetic or strong forces and because their rest mass is so small ino that it was originally thought to be zero. The weak force is a very shortrange interaction and gravity is extremely weak on the subatomic scale. Thus neutrinos typically pass through normal matter unimpeded and undetected. Neutrinos come in three flavors electron neutrinos  e muon neutrinos   and tau neutrinos   associated with the electron muon and tau respectively. Each neutrino also has a corresponding antiparticle called an antineutrino which also has no electric charge and halfinteger spin. Neutrinos are produced such that there is no overall change in lepton number that is electron neutrinos are produced together with positrons antielectrons and electron antineutrinos are produced with electrons. Neutrinos can be created in several ways including in certain types of radioactive decay in nuclear reactions such as those that take place in a star in nuclear reactors when cosmic rays hit atoms and in supernovae. The majority of neutrinos in the vicinity of the Earth are from nuclear reactions in the Sun. About 65 billion 70106500000000000006.51010 solar neutrinos per second pass through every square centimeter perpendicular to the direction of the Sun in the region of the Earth. Neutrinos oscillate between different flavors in flight. That is an electron neutrino produced in a beta decay reaction may arrive in a detector as a muon or tau neutrino. This oscillation requires that the different neutrino flavors have different masses and although the value of the masses is not known experiments have shown that these masses are tiny. From cosmological measurements it has been calculated that the sum of the three neutrino masses must be less than one millionth that of the electron. There are several active research areas involving the neutrino. Large neutrino detectors near nuclear reactors or in neutrino beams from particle accelerators attempt to measure the neutrino masses and determine the precise values for the magnitude and rates of oscillations between neutrino flavors. These experiments are also searching for the existence of CP violation in the neutrino sector that is whether or not the laws of physics treat neutrinos and antineutrinos differently. Many are looking for evidence of a sterile neutrino a fourth neutrino flavor that does not interact with matter like the three known neutrino flavors. There are also experiments searching for neutrinoless doublebeta decay which if it exists would require that the neutrino and antineutrino are really the same particle. Then there are solar and cosmic neutrino experiments which use neutrinos from space to understand the universe around us. Neutrinos are also the only identified candidate for dark matter specifically hot dark matter.  Whats a Neutrino Dave Casper University of California Irvine Neutrino unbound Online review and earchive on Neutrino Physics and Astrophysics Nova The Ghost Particle Documentary on US public television from WGBH Universe submerged in a sea of chilled neutrinos New Scientist 5 March 2008 The neutrino oscillation industry Search for neutrinoless double beta decay with enriched 76Ge in Gran Sasso 19902003 Cosmic Weight Gain A Wispy Particle Bulks Up by George Johnson Neutrino ghost particle sized up by astronomers BBC News 22 June 2010 Merrifield Michael Copeland Ed Bowley Roger 2010. Neutrinos. Sixty Symbols. Brady Haran for the University of Nottingham. The Neutrino with Dr. Clyde L. Cowan Lecture on Project Poltergeist by Clyde Cowan Nuclear Reactor as the Source of Antineutrinos Paulis letter December 1930 the hypothesis of the neutrino online and analyzed for English version click Tlcharger"}, {"topic": "Newton's law of universal gravitation", "content": "Newtons law of universal gravitation states that a particle attracts every other particle in the universe using a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between them. This is a general physical law derived from empirical observations by what Isaac Newton called induction. It is a part of classical mechanics and was formulated in Newtons work Philosophi Naturalis Principia Mathematica the Principia first published on 5 July 1687. When Newtons book was presented in 1686 to the Royal Society Robert Hooke made a claim that Newton had obtained the inverse square law from him see the History section below. In modern language the law states Every point mass attracts every single other point mass by a force pointing along the line intersecting both points. The force is proportional to the product of the two masses and inversely proportional to the square of the distance between them. The first test of Newtons theory of gravitation between masses in the laboratory was the Cavendish experiment conducted by the British scientist Henry Cavendish in 1798. It took place 111 years after the publication of Newtons Principia and approximately 71 years after his death. Newtons law of gravitation resembles Coulombs law of electrical forces which is used to calculate the magnitude of the electrical force arising between two charged bodies. Both are inversesquare laws where force is inversely proportional to the square of the distance between the bodies. Coulombs law has the product of two charges in place of the product of the masses and the electrostatic constant in place of the gravitational constant. Newtons law has since been superseded by Einsteins theory of general relativity but it continues to be used as an excellent approximation of the effects of gravity in most applications. Relativity is required only when there is a need for extreme precision or when dealing with very strong gravitational fields such as those found near extremely massive and dense objects or at very close distances such as Mercurys orbit around the sun.  Feather  Hammer Drop on Moon on YouTube Newtons Law of Universal Gravitation Javascript calculator"}, {"topic": "Newton's laws of motion", "content": "Newtons laws of motion are three physical laws that together laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it and its motion in response to those forces. They have been expressed in several different ways over nearly three centuries and can be summarised as follows. The three laws of motion were first compiled by Isaac Newton in his Philosophi Naturalis Principia Mathematica Mathematical Principles of Natural Philosophy first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example in the third volume of the text Newton showed that these laws of motion combined with his law of universal gravitation explained Keplers laws of planetary motion.  MIT Physics video lecture on Newtons three laws Light and Matter an online textbook Simulation on Newtons first law of motion Newtons Second Law by Enrique Zeleny Wolfram Demonstrations Project. Newtons 3rd Law demonstrated in a vacuum on YouTube"}, {"topic": "Newtonian fluid", "content": "In continuum mechanics a Newtonian fluid is a fluid in which the viscous stresses arising from its flow at every point are linearly proportional to the local strain ratethe rate of change of its deformation over time. That is equivalent to saying that those forces are proportional to the rates of change of the fluids velocity vector as one moves away from the point in question in various directions. More precisely a fluid is Newtonian only if the tensors that describe the viscous stress and the strain rate are related by a constant viscosity tensor that does not depend on the stress state and velocity of the flow. If the fluid is also isotropic that is its mechanical properties are the same along any direction the viscosity tensor reduces to two real coefficients describing the fluids resistance to continuous shear deformation and continuous compression or expansion respectively. Newtonian fluids are the simplest mathematical models of fluids that account for viscosity. While no real fluid fits the definition perfectly many common liquids and gases such as water and air can be assumed to be Newtonian for practical calculations under ordinary conditions. However nonNewtonian fluids are relatively common and include oobleck which becomes stiffer when vigorously sheared or nondrip paint which becomes thinner when sheared. Other examples include many polymer solutions which exhibit the Weissenberg effect molten polymers many solid suspensions blood and most highly viscous fluids. Newtonian fluids are named after Isaac Newton who first postulated the relation between the shear strain rate and shear stress for such fluids in differential form. "}, {"topic": "Nonlinear optics", "content": "Nonlinear optics NLO is the branch of optics that describes the behavior of light in nonlinear media that is media in which the dielectric polarization P responds nonlinearly to the electric field E of the light. This nonlinearity is typically only observed at very high light intensities values of the electric field comparable to interatomic electric fields typically 108 Vm such as those provided by lasers. Above the Schwinger limit the vacuum itself is expected to become nonlinear. In nonlinear optics the superposition principle no longer holds. Nonlinear optics remained unexplored until the discovery in 1961 of Second harmonic generation by Peter Franken et al. at University of Michigan shortly after the construction of the first laser by Theodore Harold Maiman however some nonlinear effects were discovered before the development of the laser. The theoretical basis for many nonlinear processes were first described in Bloembergens monograph Nonlinear Optics  Encyclopedia of laser physics and technology with content on nonlinear optics by Rdiger Paschotta An Intuitive Explanation of Phase Conjugation AdvR  NLO Frequency Conversion in KTP Waveguides SNLO  Nonlinear Optics Design Software Topics on photophysics and nonlinear optics Robert Boyd plenary presentation Quantum Nonlinear Optics Nonlinear Optics Meets the Quantum World SPIE Newsroom"}, {"topic": "Nonlinear theory of elasticity", "content": "Linear elasticity is the mathematical study of how solid objects deform and become internally stressed due to prescribed loading conditions. Linear elasticity models materials as continua. Linear elasticity is a simplification of the more general nonlinear theory of elasticity and is a branch of continuum mechanics. The fundamental linearizing assumptions of linear elasticity are infinitesimal strains or small deformations or strains and linear relationships between the components of stress and strain. In addition linear elasticity is valid only for stress states that do not produce yielding. These assumptions are reasonable for many engineering materials and engineering design scenarios. Linear elasticity is therefore used extensively in structural analysis and engineering design often with the aid of finite element analysis. "}, {"topic": "Nth root", "content": "In mathematics the nth root of a number x where n is a positive integer is a number r which when raised to the power n yields x r n  x   cannot be expressed in terms of radicals. cf. quintic equation "}, {"topic": "Nuclear astrophysics", "content": "Nuclear astrophysics is an interdisciplinary branch of physics involving close collaboration among researchers in various subfields of nuclear physics and astrophysics with significant emphasis in areas such as stellar modeling measurement and theoretical estimation of nuclear reaction rates cosmology cosmochemistry gamma ray optical and Xray astronomy and extending our knowledge about nuclear lifetimes and masses. In general terms nuclear astrophysics aims to understand the origin of the chemical elements and the energy generation in stars. "}, {"topic": "Nuclear fission", "content": "In nuclear physics and nuclear chemistry nuclear fission is either a nuclear reaction or a radioactive decay process in which the nucleus of an atom splits into smaller parts lighter nuclei. The fission process often produces free neutrons and gamma photons and releases a very large amount of energy even by the energetic standards of radioactive decay. Nuclear fission of heavy elements was discovered on December 17 1938 by German Otto Hahn and his assistant Fritz Strassmann and explained theoretically in January 1939 by Lise Meitner and her nephew Otto Robert Frisch. Frisch named the process by analogy with biological fission of living cells. It is an exothermic reaction which can release large amounts of energy both as electromagnetic radiation and as kinetic energy of the fragments heating the bulk material where fission takes place. In order for fission to produce energy the total binding energy of the resulting elements must be less negative higher energy than that of the starting element. Fission is a form of nuclear transmutation because the resulting fragments are not the same element as the original atom. The two nuclei produced are most often of comparable but slightly different sizes typically with a mass ratio of products of about 3 to 2 for common fissile isotopes. Most fissions are binary fissions producing two charged fragments but occasionally 2 to 4 times per 1000 events three positively charged fragments are produced in a ternary fission. The smallest of these fragments in ternary processes ranges in size from a proton to an argon nucleus. Apart from fission induced by a neutron harnessed and exploited by humans a natural form of spontaneous radioactive decay not requiring a neutron is also referred to as fission and occurs especially in very highmassnumber isotopes. Spontaneous fission was discovered in 1940 by Flyorov Petrzhak and Kurchatov in Moscow when they decided to confirm that without bombardment by neutrons the fission rate of uranium was indeed negligible as predicted by Niels Bohr it wasnt. The unpredictable composition of the products which vary in a broad probabilistic and somewhat chaotic manner distinguishes fission from purely quantumtunnelling processes such as proton emission alpha decay and cluster decay which give the same products each time. Nuclear fission produces energy for nuclear power and drives the explosion of nuclear weapons. Both uses are possible because certain substances called nuclear fuels undergo fission when struck by fission neutrons and in turn emit neutrons when they break apart. This makes possible a selfsustaining nuclear chain reaction that releases energy at a controlled rate in a nuclear reactor or at a very rapid uncontrolled rate in a nuclear weapon. The amount of free energy contained in nuclear fuel is millions of times the amount of free energy contained in a similar mass of chemical fuel such as gasoline making nuclear fission a very dense source of energy. The products of nuclear fission however are on average far more radioactive than the heavy elements which are normally fissioned as fuel and remain so for significant amounts of time giving rise to a nuclear waste problem. Concerns over nuclear waste accumulation and over the destructive potential of nuclear weapons may counterbalance the desirable qualities of fission as an energy source and give rise to ongoing political debate over nuclear power.  The Effects of Nuclear Weapons Annotated bibliography for nuclear fission from the Alsos Digital Library The Discovery of Nuclear Fission Historical account complete with audio and teachers guides from the American Institute of Physics History Center atomicarchive.com Nuclear Fission Explained Nuclear Files.org What is Nuclear Fission Nuclear Fission Animation"}, {"topic": "Nuclear fusion", "content": "In nuclear physics nuclear fusion is a nuclear reaction in which two or more atomic nuclei come close enough to react and form one or more different atomic nuclei and subatomic particles neutrons andor protons. The difference in mass between the products and reactants leads to the release of large amounts of energy. This difference in mass arises due to the difference in atomic binding energy between the atomic nuclei before and after the reaction. Fusion is the process that powers active or main sequence stars or other high magnitude stars. The fusion of two nuclei with lower masses than iron56 which along with nickel62 has the largest binding energy per nucleon generally releases energy while the fusion of nuclei heavier than iron requires energy input. The opposite is true for the reverse process nuclear fission. This means that only lighter elements are fusable such as hydrogen and helium and likewise that only heavier elements are fissionable such as uranium and plutonium. There are however extreme astrophysical events that can lead to short periods of fusion with heavier nuclei. This is the process that gives rise to the creation of the heavy elements during events such as a supernova. Following the discovery of quantum tunneling by physicist Friedrich Hund in 1929 Robert Atkinson and Fritz Houtermans used the measured masses of light elements to predict that large amounts of energy could be released by fusing small nuclei. Building upon the nuclear transmutation experiments by Ernest Rutherford carried out several years earlier the laboratory fusion of hydrogen isotopes was first accomplished by Mark Oliphant in 1932. During the remainder of that decade the steps of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1 1952 in the Ivy Mike hydrogen bomb test. Research into developing controlled thermonuclear fusion for civil purposes also began in earnest in the 1950s and it continues to this day.  NuclearFiles.orgA repository of documents related to nuclear power. Annotated bibliography for nuclear fusion from the Alsos Digital Library for Nuclear Issues 1NRL Fusion Formulary Organizations Fusion for Energy website ITER International Thermonuclear Experimental Reactor website CCFE Culham Centre for Fusion Energy website JET Joint European Torus website Naka Fusion Institute at JAEA Japan Atomic Energy Agency website"}, {"topic": "Nuclear medicine", "content": "Nuclear medicine is a medical specialty involving the application of radioactive substances in the diagnosis and treatment of disease. Nuclear medicine in a sense is radiology done inside out or endoradiology because it records radiation emitting from within the body rather than radiation that is generated by external sources like Xrays. In addition nuclear medicine scans differ from radiology as the emphasis is not on imaging anatomy but the function and for such reason it is called a physiological imaging modality. Single Photon Emission Computed Tomography or SPECT and Positron Emission Tomography or PET scans are the two most common imaging modalities in nuclear medicine.  Nuclear medicine at DMOZ The Nuclear Medicine and Molecular Medicine Podcast Keyword Search on Nuclear Medicine at the United States Department of Labor US DOL Occupational Information Network ONET database  Shows several occupational categories where people may use nuclear medicine technologies in their work. Linked occupation summaries include information about work and worker characteristics updated using aggregate level input from people who have performed this work. Solving the Medical Isotope Crisis Hearing before the Subcommittee on Energy and Environment of the Committee on Energy and Commerce House of Representatives One Hundred Eleventh Congress First Session September 9 2009"}, {"topic": "Nuclear physics", "content": "Nuclear physics is the field of physics that studies atomic nuclei and their constituents and interactions. The most commonly known application of nuclear physics is nuclear power generation but the research has led to applications in many fields including nuclear medicine and magnetic resonance imaging nuclear weapons ion implantation in materials engineering and radiocarbon dating in geology and archaeology. The field of particle physics evolved out of nuclear physics and is typically taught in close association with nuclear physics.  Ernest Rutherfords biography at the American Institute of Physics American Physical Society Division of Nuclear Physics American Nuclear Society Boiling Water Reactor Plant BWR Simulator Program Annotated bibliography on nuclear physics from the Alsos Digital Library for Nuclear Issues Nucleonica ..web driven nuclear science Nuclear science wiki Nuclear Data Services  IAEA"}, {"topic": "Nuclear transmutation", "content": "Nuclear transmutation is the conversion of one chemical element or an isotope into another. Because any element isotope is defined by its number of protons and neutrons in its atoms i.e. in the atomic nucleus nuclear transmutation occurs in any process where this number is changed. A transmutation can be achieved either by nuclear reactions in which an outside particle reacts with a nucleus or by radioactive decay where no outside particle is needed. "}, {"topic": "Ohm", "content": "The ohm symbol  is the SI derived unit of electrical resistance named after German physicist Georg Simon Ohm. Although several empirically derived standard units for expressing electrical resistance were developed in connection with early telegraphy practice the British Association for the Advancement of Science proposed a unit derived from existing units of mass length and time and of a convenient size for practical work as early as 1861. The definition of the ohm was revised several times. Today the definition of the ohm is expressed from the quantum Hall effect.  Scanned books of Georg Simon Ohm at the library of the University of Applied Sciences Nuernberg Official SI brochure NIST Special Publication 811 History of the ohm at sizes.com History of the electrical units."}, {"topic": "Ohm's law", "content": "Ohms law states that the current through a conductor between two points is directly proportional to the voltage across the two points. Introducing the constant of proportionality the resistance one arrives at the usual mathematical equation that describes this relationship I  V R   .  John C. Shedd and Mayo D. HersheyThe History of Ohms Law Popular Science December 1913 pages 599614 Bonnier Corporation ISSN 01617370 gives the history of Ohms investigations prior work Ohms false equation in the first paper illustration of Ohms experimental apparatus. Morton L. Schagrin Resistance to Ohms Law American Journal of Physics July 1963 Volume 31 Issue 7 pp. 53647. Explores the conceptual change underlying Ohms experimental work. Kenneth L. Caneva Ohm Georg Simon. Complete Dictionary of Scientific Biography. 2008"}, {"topic": "Optics", "content": "Optics is the branch of physics which involves the behaviour and properties of light including its interactions with matter and the construction of instruments that use or detect it. Optics usually describes the behaviour of visible ultraviolet and infrared light. Because light is an electromagnetic wave other forms of electromagnetic radiation such as Xrays microwaves and radio waves exhibit similar properties. Most optical phenomena can be accounted for using the classical electromagnetic description of light. Complete electromagnetic descriptions of light are however often difficult to apply in practice. Practical optics is usually done using simplified models. The most common of these geometric optics treats light as a collection of rays that travel in straight lines and bend when they pass through or reflect from surfaces. Physical optics is a more comprehensive model of light which includes wave effects such as diffraction and interference that cannot be accounted for in geometric optics. Historically the raybased model of light was developed first followed by the wave model of light. Progress in electromagnetic theory in the 19th century led to the discovery that light waves were in fact electromagnetic radiation. Some phenomena depend on the fact that light has both wavelike and particlelike properties. Explanation of these effects requires quantum mechanics. When considering lights particlelike properties the light is modelled as a collection of particles called photons. Quantum optics deals with the application of quantum mechanics to optical systems. Optical science is relevant to and studied in many related disciplines including astronomy various engineering fields photography and medicine particularly ophthalmology and optometry. Practical applications of optics are found in a variety of technologies and everyday objects including mirrors lenses telescopes microscopes lasers and fibre optics.  Relevant discussions Optics on In Our Time at the BBC. listen now Textbooks and tutorials Optics an opensource optics textbook Optics2001 Optics library and community Fundamental Optics Melles Griot Technical Guide Physics of Light and Optics Brigham Young University Undergraduate Book Wikibooks modules Further reading Optics and photonics Physics enhancing our lives by Institute of Physics publications Societies"}, {"topic": "Outline of physics", "content": "The following outline is provided as an overview of and topical guide to physics Physics natural science that involves the study of matter and its motion through spacetime along with related concepts such as energy and force. More broadly it is the general analysis of nature conducted in order to understand how the universe behaves.  AIP.org is the website of the American Institute of Physics IOP.org is the website of the Institute of Physics APS.org is the website of the American Physical Society SPS National is the website of the American Society of Physics Students CAP.ca is the website of the Canadian Association of Physicists EPS.org is the website of the European Physical Society Meta Institute for Computational Physics  Popular Talks Physics  Channel  MIT Video How to become a GOOD Theoretical Physicist a website with outline of theoretical physics by Gerard t Hooft The Feynman Lectures on Physics 3 vols. free online Caltech  The Feynman Lectures Website Resource recommendations  List of freely available physics books  Physics Stack Exchange"}, {"topic": "PH", "content": "In chemistry pH piet is a numeric scale used to specify the acidity or basicity of an aqueous solution. It is approximately the negative of the logarithm to base 10 of the molar concentration measured in units of moles per liter of hydrogen ions. More precisely it is the negative of the logarithm to base 10 of the activity of the hydrogen ion. Solutions with a pH less than 7 are acidic and solutions with a pH greater than 7 are basic. Pure water is neutral being neither an acid nor a base. Contrary to popular belief the pH value can be less than 0 or greater than 14 for very strong acids and bases respectively. pH measurements are important in agronomy medicine biology chemistry agriculture forestry food science environmental science oceanography civil engineering chemical engineering nutrition water treatment and water purification as well as many other applications. The pH scale is traceable to a set of standard solutions whose pH is established by international agreement. Primary pH standard values are determined using a concentration cell with transference by measuring the potential difference between a hydrogen electrode and a standard electrode such as the silver chloride electrode. The pH of aqueous solutions can be measured with a glass electrode and a pH meter or an indicator.  The pH Scale Chem1 Virtual Textbook Acidbase Equilibria and Calculations Red Cabbage pH Indicator Food and Foodstuff pH Values Online pH Calculator for about 100 inorganic Acids Bases and Salts"}, {"topic": "Paraffin wax", "content": "Paraffin wax is a white or colourless soft solid derivable from petroleum coal or oil shale that consists of a mixture of hydrocarbon molecules containing between twenty and forty carbon atoms. It is solid at room temperature and begins to melt above approximately 37 C 99 F its boiling point is 370 C 698 F. Common applications for paraffin wax include lubrication electrical insulation and candles. It is distinct from kerosene another petroleum product that is sometimes called paraffin. Paraffin candles are odorless and bluishwhite in color. Paraffin wax was first created in the 1850s and marked a major advancement in candlemaking technology as it burned more cleanly and reliably than tallow candles and was cheaper to produce. In chemistry paraffin is used synonymously with alkane indicating hydrocarbons with the general formula CnH2n2. The name is derived from Latin parum barely  affinis meaning lacking affinity or lacking reactivity referring to paraffins unreactive nature. "}, {"topic": "Parity (mathematics)", "content": "Parity is a mathematical term that describes the property of an integers inclusion in one of two categories even or odd. An integer is even if it is evenly divisible by two the oldfashioned term evenly divisible is now almost always shortened to divisible and odd if it is not even. For example 6 is even because there is no remainder when dividing it by 2. By contrast 3 5 7 21 leave a remainder of 1 when divided by 2. Examples of even numbers include 4 0 8 and 1738. In particular zero is an even number. Some examples of odd numbers are 5 3 9 and 73. Parity does not apply to noninteger numbers. A formal definition of an even number is that it is an integer of the form n  2k where k is an integer it can then be shown that an odd number is an integer of the form n  2k  1. This classification applies only to integers i.e. nonintegers like 12 4.201 or infinity are neither even nor odd. The sets of even and odd numbers can be defined as following Even   maybe called odd. As an example let RZ2 be the localization of Z at the prime ideal 2. Then an element of R is even or odd if and only if its numerator is so in Z. "}, {"topic": "Parity (physics)", "content": "In quantum mechanics a parity transformation also called parity inversion is the flip in the sign of one spatial coordinate. In three dimensions it is also often described by the simultaneous flip in the sign of all three spatial coordinates a point reflection P   x y z   x y z  .   Poynting vector.  General Perkins Donald H. 2000. Introduction to High Energy Physics. ISBN 9780521621960. Sozzi M. S. 2008. Discrete symmetries and CP violation. Oxford University Press. ISBN 9780199296668. Bigi I. I. Sanda A. I. 2000. CP Violation. Cambridge Monographs on Particle Physics Nuclear Physics and Cosmology. Cambridge University Press. ISBN 0521443490. Weinberg S. 1995. The Quantum Theory of Fields. Cambridge University Press. ISBN 0521670535. Specific"}, {"topic": "Particle accelerator", "content": "A particle accelerator is a machine that uses electromagnetic fields to propel charged particles to nearly light speed and to contain them in welldefined beams. Large accelerators are used in particle physics as colliders e.g. the LHC at CERN KEKB at KEK in Japan RHIC at Brookhaven National Laboratory and Tevatron at Fermilab or as synchrotron light sources for the study of condensed matter physics. Smaller particle accelerators are used in a wide variety of applications including particle therapy for oncological purposes radioisotope production for medical diagnostics ion implanters for manufacture of semiconductors and accelerator mass spectrometers for measurements of rare isotopes such as radiocarbon. There are currently more than 30000 accelerators in operation around the world. There are two basic classes of accelerators electrostatic and electrodynamic or electromagnetic accelerators. Electrostatic accelerators use static electric fields to accelerate particles. A smallscale example of this class is the cathode ray tube in an ordinary old television set. Other examples are the CockcroftWalton generator and the Van de Graaff generator. The achievable kinetic energy for particles in these devices is limited by electrical breakdown. Electrodynamic or electromagnetic accelerators on the other hand use changing electromagnetic fields either magnetic induction or oscillating radio frequency fields to accelerate particles and reduce the breakdown problem. This class which was first developed in the 1920s is the basis for most modern accelerator concepts and largescale facilities. Rolf Widere Gustav Ising Le Szilrd Max Steenbeck and Ernest Lawrence are considered pioneers of this field conceiving and building the first operational linear particle accelerator the betatron and the cyclotron. Because colliders can give evidence of the structure of the subatomic world accelerators were commonly referred to as atom smashers in the 20th century. Despite the fact that most accelerators but not ion facilities actually propel subatomic particles the term persists in popular usage when referring to particle accelerators in general. "}, {"topic": "Particle displacement", "content": "Particle displacement or displacement amplitude is a measurement of distance of the movement of a particle from its equilibrium position in a medium as it transmits a sound wave. The SI unit of particle displacement is the metre m. In most cases this is a longitudinal wave of pressure such as sound but it can also be a transverse wave such as the vibration of a taut string. In the case of a sound wave travelling through air the particle displacement is evident in the oscillations of air molecules with and against the direction in which the sound wave is travelling. A particle of the medium undergoes displacement according to the particle velocity of the sound wave traveling through the medium while the sound wave itself moves at the speed of sound equal to 343 ms in air at 20 C.  Acoustic ParticleImage Velocimetry. Development and Applications Ohms Law as Acoustic Equivalent. Calculations Relationships of Acoustic Quantities Associated with a Plane Progressive Acoustic Sound Wave"}, {"topic": "Particle physics", "content": "Particle physics also high energy physics is the branch of physics that studies the nature of the particles that constitute matter particles with mass and radiation massless particles. Although the word particle can refer to various types of very small objects e.g. protons gas particles or even household dust particle physics usually investigates the irreducibly smallest detectable particles and the irreducibly fundamental force fields necessary to explain them. By our current understanding these elementary particles are excitations of the quantum fields that also govern their interactions. The currently dominant theory explaining these fundamental particles and fields along with their dynamics is called the Standard Model. Thus modern particle physics generally investigates the Standard Model and its various possible extensions e.g. to the newest known particle the Higgs boson or even to the oldest known force field gravity.  Symmetry magazine Fermilab Particle physics it matters the Institute of Physics Nobes Matthew 2002 Introduction to the Standard Model of Particle Physics on Kuro5hin Part 1 Part 2 Part 3a Part 3b. CERN European Organization for Nuclear Research The Particle Adventure educational project sponsored by the Particle Data Group of the Lawrence Berkeley National Laboratory LBNL"}, {"topic": "Pascal's Law", "content": "Pascals law or the principle of transmission of fluidpressure also Pascals Principle is a principle in fluid mechanics that states that a pressure change occurring anywhere in a confined incompressible fluid is transmitted throughout the fluid such that the same change occurs everywhere. The law was established by French mathematician Blaise Pascal. "}, {"topic": "Pauli exclusion principle", "content": "The Pauli exclusion principle is the quantum mechanical principle that states that two identical fermions particles with halfinteger spin cannot occupy the same quantum state simultaneously. In the case of electrons it can be stated as follows it is impossible for two electrons of a polyelectron atom to have the same values of the four quantum numbers n the principal quantum number  the angular momentum quantum number m the magnetic quantum number and ms the spin quantum number. For example if two electrons reside in the same orbital and if their n  and m values are the same then their ms must be different and thus the electrons must have opposite halfinteger spins of 12 and 12. This principle was formulated by Austrian physicist Wolfgang Pauli in 1925 for electrons and later extended to all fermions with his spinstatistics theorem of 1940. A more rigorous statement is that the total wave function for two identical fermions is antisymmetric with respect to exchange of the particles. This means that the wave function changes its sign if the space and spin coordinates of any two particles are interchanged. Particles with an integer spin or bosons are not subject to the Pauli exclusion principle any number of identical bosons can occupy the same quantum state as with for instance photons produced by a laser and BoseEinstein condensate.  Nobel Lecture Exclusion Principle and Quantum Mechanics Paulis own account of the development of the Exclusion Principle."}, {"topic": "Pendulum", "content": "A pendulum is a weight suspended from a pivot so that it can swing freely. When a pendulum is displaced sideways from its resting equilibrium position it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released the restoring force combined with the pendulums mass causes it to oscillate about the equilibrium position swinging back and forth. The time for one complete cycle a left swing and a right swing is called the period. The period depends on the length of the pendulum and also to a slight degree on the amplitude the width of the pendulums swing. From the first scientific investigations of the pendulum around 1602 by Galileo Galilei the regular motion of pendulums was used for timekeeping and was the worlds most accurate timekeeping technology until the 1930s. The pendulum clock invented by Christian Huygens in 1658 became the worlds standard timekeeper used in homes and offices for 270 years and achieved accuracy of about one second per year before it was superseded as a time standard by quartz clocks in the 1930s. Pendulums are also used in scientific instruments such as accelerometers and seismometers. Historically they were used as gravimeters to measure the acceleration of gravity in geophysical surveys and even as a standard of length. The word pendulum is new Latin from the Latin pendulus meaning hanging. The simple gravity pendulum is an idealized mathematical model of a pendulum. This is a weight or bob on the end of a massless cord suspended from a pivot without friction. When given an initial push it will swing back and forth at a constant amplitude. Real pendulums are subject to friction and air drag so the amplitude of their swings declines. "}, {"topic": "Periodic table", "content": "The periodic table is a tabular arrangement of the chemical elements ordered by their atomic number number of protons electron configurations and recurring chemical properties. This ordering shows periodic trends such as elements with similar behavior in the same column. It also shows four rectangular blocks with some approximately similar chemical properties. In general within one row period the elements are metals on the left and nonmetals on the right. The rows of the table are called periods the columns are called groups. Six groups columns have names as well as numbers for example group 17 elements are the halogens and group 18 the noble gases. The periodic table can be used to derive relationships between the properties of the elements and predict the properties of new elements yet to be discovered or synthesized. The periodic table provides a useful framework for analyzing chemical behavior and is widely used in chemistry and other sciences. Dmitri Mendeleev published in 1869 the first widely recognized periodic table. He developed his table to illustrate periodic trends in the properties of the thenknown elements. Mendeleev also predicted some properties of thenunknown elements that would be expected to fill gaps in this table. Most of his predictions were proved correct when the elements in question were subsequently discovered. Mendeleevs periodic table has since been expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behavior. All elements from atomic numbers 1 hydrogen to 118 ununoctium have been discovered or synthesized with the most recent additions elements 113 115 117 and 118 being confirmed by the IUPAC on December 30 2015. The first 94 elements exist naturally although some are found only in trace amounts and were synthesized in laboratories before being found in nature. Elements with atomic numbers from 95 to 118 have only been synthesized in laboratories or nuclear reactors. Synthesis of elements having higher atomic numbers is being pursued. Numerous synthetic radionuclides of naturally occurring elements have also been produced in laboratories.  IUPAC. Periodic Table of the Elements. Retrieved 24 May 2016. Note There is no IUPAC ruling on the composition of Group 3. M. Dayah. Dynamic Periodic Table. Retrieved 14 May 2012. Brady Haran. The Periodic Table of Videos. University of Nottingham. Retrieved 14 May 2012. Mark Winter. WebElements the periodic table on the web. University of Sheffield. Retrieved 14 May 2012. Mark R. Leach. The INTERNET Database of Periodic Tables. Retrieved 14 May 2012."}, {"topic": "Phase (matter)", "content": "In the physical sciences a phase is a region of space a thermodynamic system throughout which all physical properties of a material are essentially uniform. Examples of physical properties include density index of refraction magnetization and chemical composition. A simple description is that a phase is a region of material that is chemically uniform physically distinct and often mechanically separable. In a system consisting of ice and water in a glass jar the ice cubes are one phase the water is a second phase and the humid air over the water is a third phase. The glass of the jar is another separate phase. See state of matterGlass The term phase is sometimes used as a synonym for state of matter but there can be several immiscible phases of the same state of matter. Also the term phase is sometimes used to refer to a set of equilibrium states demarcated in terms of state variables such as pressure and temperature by a phase boundary on a phase diagram. Because phase boundaries relate to changes in the organization of matter such as a change from liquid to solid or a more subtle change from one crystal structure to another this latter usage is similar to the use of phase as a synonym for state of matter. However the state of matter and phase diagram usages are not commensurate with the formal definition given above and the intended meaning must be determined in part from the context in which the term is used.  French physicists find a solution that reversibly solidifies with a rise in temperature cyclodextrin water and 4methylpyridine"}, {"topic": "Phase (waves)", "content": "Phase is the position of a point in time an instant on a waveform cycle. A complete cycle is defined as the interval required for the waveform to return to its arbitrary initial value. The graphic to the right shows how one cycle constitutes 360 of phase. The graphic also shows how phase is sometimes expressed in radians where one radian of phase equals approximately 57.3. Phase can also be an expression of relative displacement between two corresponding features for example peaks or zero crossings of two waveforms having the same frequency. In sinusoidal functions or in waves phase has two different but closely related meanings. One is the initial angle of a sinusoidal function at its origin and is sometimes called phase offset or phase difference. Another usage is the fraction of the wave cycle that has elapsed relative to the origin.  What is a phase. Prof. Jeffrey Hass. An Acoustics Primer Section 8. Indiana University. 2003. See also pages 1 thru 3. 2013 Phase angle phase difference time delay and frequency ECE 209 Sources of Phase Shift Discusses the timedomain sources of phase shift in simple linear timeinvariant circuits. Phase Difference Java Applet"}, {"topic": "Phase equilibrium", "content": "Gibbs phase rule was proposed by Josiah Willard Gibbs in his landmark paper titled On the Equilibrium of Heterogeneous Substances published from 1875 to 1878. The rule applies to nonreactive multicomponent heterogeneous systems in thermodynamic equilibrium and is given by the equality F  C P  2  where F is the number of degrees of freedom C is the number of components and P is the number of phases in thermodynamic equilibrium with each other. The number of degrees of freedom is the number of independent intensive variables i.e. the largest number of thermodynamic parameters such as temperature or pressure that can be varied simultaneously and arbitrarily without affecting one another. An example of onecomponent system is a system involving one pure chemical while twocomponent systems such as mixtures of water and ethanol have two chemically independent components and so on. Typical phases are solids liquids and gases.  Mogk David Teaching Phase Equilibria. Gibbs Phase Rule Where it all Begins The phase rule in geology Predel Bruno Hoch Michael J. R. Pool Monte. Phase Diagrams and Heterogeneous Equilibria  A Practical Introduction. Springer. ISBN 3540140115. White Mary Anne. Properties of Materials. Oxford University Press 1999. ISBN 0195113314. Chapter 9. Thermodynamics Aspects of Stability"}, {"topic": "Phenomenology (particle physics)", "content": "Particle physics phenomenology is the part of theoretical particle physics that deals with the application of theoretical physics to highenergy particle physics experiments. Phenomenology forms a bridge between the mathematical models of theoretical physics such as quantum field theories and theories of the structure of spacetime and experimental particle physics. Within the Standard Model phenomenology is the calculating of detailed predictions for experiments usually at high precision e.g. including radiative corrections. Beyond the Standard Model phenomenology addresses the experimental consequences of new models how their new particles could be searched for how the model parameters could be measured and how the model could be distinguished from other competing models.  Papers on phenomenology are available on the hepph archive of the ArXiv.org eprint archive List of topics on phenomenology from IPPP the Institute for Particle Physics Phenomenology at University of Durham UK Collider Phenomenology Basic knowledge and techniques lectures by Tao Han Pheno 08 Symposium on particle physics phenomenology including slides from the talks linked from the symposium program."}, {"topic": "Phosphorescence", "content": "Phosphorescence is a specific type of photoluminescence related to fluorescence. Unlike fluorescence a phosphorescent material does not immediately reemit the radiation it absorbs. The slower time scales of the reemission are associated with forbidden energy state transitions in quantum mechanics. As these transitions occur very slowly in certain materials absorbed radiation may be reemitted at a lower intensity for up to several hours after the original excitation. Commonly seen examples of phosphorescent materials are the glowinthedark toys paint and clock dials that glow for some time after being charged with a bright light such as in any normal reading or room light. Typically the glowing then slowly fades out within minutes to up to a few hours in a dark room. The study of phosphorescent materials led to the discovery of radioactivity in 1896. Ironically white phosphorus from which phosphorescence takes its name does not actually exhibit this property but rather chemiluminescence.  Luminosity in Watches SPECTRA OF GITD GLOWINTHEDARK MATERIALS"}, {"topic": "Photon", "content": "A photon is an elementary particle the quantum of all forms of electromagnetic radiation including light. It is the force carrier for electromagnetic force even when static via virtual photons. The photon has zero rest mass and as a result the interactions of this force with matter at long distance are observable at the microscopic and macroscopic levels. Like all elementary particles photons are currently best explained by quantum mechanics but exhibit waveparticle duality exhibiting properties of both waves and particles. For example a single photon may be refracted by a lens and exhibit wave interference with itself and it can behave as a particle with definite and finite measurable position and momentum. The photons wave and quanta qualities are two observable aspects of a single phenomenon and cannot be described by any mechanical model a representation of this dual property of light which assumes certain points on the wavefront to be the seat of the energy is not possible. The quanta in a light wave cannot be spatially localized. Some defined physical parameters of a photon are listed. The modern concept of the photon was developed gradually by Albert Einstein in the early 20th century to explain experimental observations that did not fit the classical wave model of light. The benefit of the photon model was that it accounted for the frequency dependence of lights energy and explained the ability of matter and electromagnetic radiation to be in thermal equilibrium. The photon model accounted for anomalous observations including the properties of blackbody radiation that others notably Max Planck had tried to explain using semiclassical models. In that model light was described by Maxwells equations but material objects emitted and absorbed light in quantized amounts i.e. they change energy only by certain particular discrete amounts. Although these semiclassical models contributed to the development of quantum mechanics many further experiments beginning with the phenomenon of Compton scattering of single photons by electrons validated Einsteins hypothesis that light itself is quantized. In 1926 the optical physicist Frithiof Wolfers and the chemist Gilbert N. Lewis coined the name photon for these particles. After Arthur H. Compton won the Nobel Prize in 1927 for his scattering studies most scientists accepted that light quanta have an independent existence and the term photon was accepted. In the Standard Model of particle physics photons and other elementary particles are described as a necessary consequence of physical laws having a certain symmetry at every point in spacetime. The intrinsic properties of particles such as charge mass and spin are determined by this gauge symmetry. The photon concept has led to momentous advances in experimental and theoretical physics including as lasers BoseEinstein condensation quantum field theory and the probabilistic interpretation of quantum mechanics. It has been applied to photochemistry highresolution microscopy and measurements of molecular distances. Recently photons have been studied as elements of quantum computers and for applications in optical imaging and optical communication such as quantum cryptography.  The dictionary definition of photon at Wiktionary Media related to Photon at Wikimedia Commons"}, {"topic": "Physical chemistry", "content": "Physical chemistry is the study of macroscopic atomic subatomic and particulate phenomena in chemical systems in terms of laws and concepts of physics. It applies the principles practices and concepts of physics such as motion energy force time thermodynamics quantum chemistry statistical mechanics and dynamics equilibrium. Physical chemistry in contrast to chemical physics is predominantly but not always a macroscopic or supramolecular science as the majority of the principles on which physical chemistry was founded are concepts related to the bulk rather than on molecularatomic structure alone for example chemical equilibrium and colloids. Some of the relationships that physical chemistry strives to resolve include the effects of Intermolecular forces that act upon the physical properties of materials plasticity tensile strength surface tension in liquids. Reaction kinetics on the rate of a reaction. The identity of ions and the electrical conductivity of materials. Surface chemistry and electrochemistry of membranes. Interaction of one body with another in terms of quantities of heat and work called thermodynamics. Transfer of heat between a chemical system and its surroundings during change of phase or chemical reaction taking place called thermochemistry Study of colligative properties of number of species present in solution. Number of phases number of components and degree of freedom or variance can be correlated with one another with help of phase rule. Reactions of electrochemical cells.  Physical Chemistry Keith J. Laidler John H. Meiser and Bryan C. Sanctuary The World of Physical Chemistry Keith J. Laidler 1993 Physical Chemistry from Ostwald to Pauling John W. Servos 1996 100 Years of Physical Chemistry Royal Society of Chemistry 2004 Physical Chemistry neither Fish nor Fowl Joachim Schummer The Autonomy of Chemistry Wrzburg Knigshausen  Neumann 1998 pp. 135148 Cathedrals of Science Patrick Coffey 2008 The Cambridge History of Science The modern physical and mathematical sciences Mary Jo Nye 2003"}, {"topic": "Physical laws", "content": "A physical law or scientific law is a theoretical statement inferred from particular facts applicable to a defined group or class of phenomena and expressible by the statement that a particular phenomenon always occurs if certain conditions be present. Physical laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. The production of a summary description of our environment in the form of such laws is a fundamental aim of science. These terms are not used the same way by all authors. The distinction between natural law in the politicallegal sense and law of nature or physical law in the scientific sense is a modern one both concepts being equally derived from physis the Greek word translated into Latin as natura for nature.  Stanford Encyclopedia of Philosophy Laws of Nature by John W. Carroll. Baaquie Belal E. Laws of Physics  A Primer. Core Curriculum National University of Singapore. Francis Erik Max. The laws list.. Physics. Alcyone Systems Pazameta Zoran. The laws of nature. Committee for the scientific investigation of Claims of the Paranormal. The Internet Encyclopedia of Philosophy. Laws of Nature By Norman Swartz"}, {"topic": "Physical optics", "content": "In physics physical optics or wave optics is the branch of optics which studies interference diffraction polarization and other phenomena for which the ray approximation of geometric optics is not valid. This usage tends not to include effects such as quantum noise in optical communication which is studied in the subbranch of coherence theory.  Serway Raymond A. Jewett John W. 2004. Physics for Scientists and Engineers 6th ed.. BrooksCole. ISBN 0534408427. Akhmanov A Nikitin S. Yu 1997. Physical Optics. Oxford University Press. ISBN 0198517955. A doubleedgediffraction Gaussianseries method for efficient physical optics analysis of dualshapedreflector antennas. IEEE Transactions on Antennas and Propagation 2597. August 2005. Asvestas J. S. February 1980. The physical optics method in electromagnetic scattering. Journal of Mathematical Physics 21 2 290299. Bibcode1980JMP....21..290A. doi10.10631.524413."}, {"topic": "Physical quantity", "content": "A physical quantity or physical magnitude is a physical property of a phenomenon body or substance that can be quantified by measurement. A physical quantity can be expressed as the combination of a number usually a real number and a unit or combination of units for example 69731674927499999991.67492751027 kg the mass of the neutron or 7008299792458000000299792458 metres per second the speed of light. Physical quantities are measured as nu where n is the number and u is the unit. For example A boy measured the length of a room as 3m. Here 3 is the number and mmetre is the unit. 3m can also be written as 300cm. This shows that n1u1 n2u2. Almost all matters have quantity.  Cook Alan H. The observational foundations of physics Cambridge 1994. ISBN 0521455979 Essential Principles of Physics P.M. Whelan M.J. Hodgeson 2nd Edition 1978 John Murray ISBN 0719533821 Encyclopaedia of Physics R.G. Lerner G.L. Trigg 2nd Edition VHC Publishers Hans Warlimont Springer 2005 pp 1213 Physics for Scientists and Engineers With Modern Physics 6th Edition P.A. Tipler G. Mosca W.H. Freeman and Co 2008 9781429202657"}, {"topic": "Physics", "content": "Physics from Ancient Greek  phusik epistm knowledge of nature from phsis nature is the natural science that involves the study of matter and its motion through space and time along with related concepts such as energy and force. One of the most fundamental scientific disciplines the main goal of physics is to understand how the universe behaves. Physics is one of the oldest academic disciplines perhaps the oldest through its inclusion of astronomy. Over the last two millennia physics was a part of natural philosophy along with chemistry biology and certain branches of mathematics but during the scientific revolution in the 17th century the natural sciences emerged as unique research programs in their own right. Physics intersects with many interdisciplinary areas of research such as biophysics and quantum chemistry and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms of other sciences while opening new avenues of research in areas such as mathematics and philosophy. Physics also makes significant contributions through advances in new technologies that arise from theoretical breakthroughs. For example advances in the understanding of electromagnetism or nuclear physics led directly to the development of new products that have dramatically transformed modernday society such as television computers domestic appliances and nuclear weapons advances in thermodynamics led to the development of industrialization and advances in mechanics inspired the development of calculus.  General Encyclopedia of Physics at Scholarpedia de Haas Paul Historic Papers in Physics 20th Century at the Wayback Machine archived August 26 2009 PhysicsCentral Web portal run by the American Physical Society Physics.org Web portal run by the Institute of Physics The Skeptics Guide to Physics Usenet Physics FAQ A FAQ compiled by sci.physics and other physics newsgroups Website of the Nobel Prize in physics World of Physics An online encyclopedic dictionary of physics Nature Physics Physics announced 17 July 2008 by the American Physical Society PhysicsPublications at DMOZ Physicsworld.com News website from Institute of Physics Publishing Physics Central includes articles on astronomy particle physics and mathematics. The Vega Science Trust science videos including physics Video Physics Lightning Tour with Justin Morgan 52part video course The Mechanical Universe...and Beyond Note also available at 01 Introduction at Google Videos HyperPhysics website HyperPhysics a physics and astronomy mindmap from Georgia State University Organizations AIP.org Website of the American Institute of Physics APS.org Website of the American Physical Society IOP.org Website of the Institute of Physics PlanetPhysics.org Royal Society Although not exclusively a physics institution it has a strong history of physics SPS National Website of the Society of Physics Students"}, {"topic": "Planck constant", "content": "The Planck constant denoted h also called Plancks constant is a physical constant that is the quantum of action central in quantum mechanics. First recognized in 1900 by Max Planck it was originally the proportionality constant between the minimal increment of energy E of a hypothetical electrically charged oscillator in a cavity that contained black body radiation and the frequency f of its associated electromagnetic wave. In 1905 the value E the minimal energy increment of a hypothetical oscillator was theoretically associated by Einstein with a quantum or minimal element of the energy of the electromagnetic wave itself. The light quantum behaved in some respects as an electrically neutral particle as opposed to an electromagnetic wave. It was eventually called the photon. The PlanckEinstein relation connects the particulate photon energy E with its associated wave frequency f E  h f  with KJ90 and RK90 being exactly defined constants. Atomic units and conventional electrical units are very useful in their respective fields because the uncertainty in the final result does not depend on an uncertain conversion factor only on the uncertainty of the measurement itself. There are a number of proposals to redefine certain of the SI base units in terms of fundamental physical constants. This has already been done for the metre which is defined in terms of a fixed value of the speed of light. The most urgent unit on the list for redefinition is the kilogram whose value has been fixed for all science since 1889 by the mass of a small cylinder of platinumiridium alloy kept in a vault just outside Paris. While nobody knows if the mass of the International Prototype Kilogram has changed since 1889 the value 1 kg of its mass expressed in kilograms is by definition unchanged and therein lies one of the problems it is known that over such a timescale the many similar PtIr alloy cylinders kept in national laboratories around the world have changed their relative mass by several tens of parts per million however carefully they are stored and the more so the more they have been taken out and used as mass standards. A change of several tens of micrograms in one kilogram is equivalent to the current uncertainty in the value of the Planck constant in SI units. The legal process to change the definition of the kilogram is already underway but it had been decided that no final decision would be made before the next meeting of the General Conference on Weights and Measures in 2011. For more detailed information see kilogram definitions. The Planck constant is a leading contender to form the basis of the new definition although not the only one. Possible new definitions include the mass of a body at rest whose equivalent energy equals the energy of photons whose frequencies sum to 70501356392739999991356392741042 Hz or simply the kilogram is defined so that the Planck constant equals 69666626068959999996.626068961034 Js. The BIPM provided Draft Resolution A in anticipation of the 24th General Conference on Weights and Measures meeting 20111017 through 20111021 detailing the considerations On the possible future revision of the International System of Units the SI. Watt balances already measure mass in terms of the Planck constant at present standard mass is taken as fixed and the measurement is performed to determine the Planck constant but were the Planck constant to be fixed in SI units the same experiment would be a measurement of the mass. The relative uncertainty in the measurement would remain the same. Mass standards could also be constructed from silicon crystals or by other atomcounting methods. Such methods require a knowledge of the Avogadro constant which fixes the proportionality between atomic mass and macroscopic mass but with a defined value of the Planck constant NA would be known to the same level of uncertainty if not better than current methods of comparing macroscopic mass.  Quantum of Action and Quantum of Spin Numericana Moriarty Philip Eaves Laurence Merrifield Michael 2009. h Plancks Constant. Sixty Symbols. Brady Haran for the University of Nottingham."}, {"topic": "Plane (geometry)", "content": "In mathematics a plane is a flat twodimensional surface that extends infinitely far. A plane is the twodimensional analogue of a point zero dimensions a line one dimension and threedimensional space. Planes can arise as subspaces of some higherdimensional space as with a rooms walls extended infinitely far or they may enjoy an independent existence in their own right as in the setting of Euclidean geometry. When working exclusively in twodimensional Euclidean space the definite article is used so the plane refers to the whole space. Many fundamental tasks in mathematics geometry trigonometry graph theory and graphing are performed in a twodimensional space or in other words in the plane.  Hazewinkel Michiel ed. 2001 Plane Encyclopedia of Mathematics Springer ISBN 9781556080104 Weisstein Eric W. Plane MathWorld. Easing the Difficulty of Arithmetic and Planar Geometry is an Arabic manuscript from the 15th century that serves as a tutorial about plane geometry and arithmetic ."}, {"topic": "Plasma (physics)", "content": "Plasma from Greek  anything formed is one of the four fundamental states of matter the others being solid liquid and gas. A plasma has properties unlike those of the other states. A plasma can be created by heating a gas or subjecting it to a strong electromagnetic field applied with a laser or microwave generator. This decreases or increases the number of electrons creating positive or negative charged particles called ions and is accompanied by the dissociation of molecular bonds if present. The presence of a significant number of charge carriers makes plasma electrically conductive so that it responds strongly to electromagnetic fields. Like gas plasma does not have a definite shape or a definite volume unless enclosed in a container. Unlike gas under the influence of a magnetic field it may form structures such as filaments beams and double layers. Plasma is the most abundant form of ordinary matter in the Universe of the forms proven to exist the more abundant dark matter is hypothetical and may or may not be explained by ordinary matter most of which is in the rarefied intergalactic regions particularly the intracluster medium and in stars including the Sun. A common form of plasma on Earth is produced in neon signs. Much of the understanding of plasma has come from the pursuit of controlled nuclear fusion and fusion power for which plasma physics provides the scientific foundation.  Free plasma physics books and notes Plasmas the Fourth State of Matter Plasma Science and Technology Plasma on the Internet a list of plasma related links. Introduction to Plasma Physics Graduate course given by Richard FitzpatrickM.I.T. Introduction by I.H.Hutchinson Plasma Material Interaction How to make a glowing ball of plasma in your microwave with a grapeMore Video How to make plasma in your microwave with only one match video OpenPIC3D 3D Hybrid ParticleInCell simulation of plasma dynamics Plasma Formulary Interactive"}, {"topic": "Plasma physics", "content": "Plasma from Greek  anything formed is one of the four fundamental states of matter the others being solid liquid and gas. A plasma has properties unlike those of the other states. A plasma can be created by heating a gas or subjecting it to a strong electromagnetic field applied with a laser or microwave generator. This decreases or increases the number of electrons creating positive or negative charged particles called ions and is accompanied by the dissociation of molecular bonds if present. The presence of a significant number of charge carriers makes plasma electrically conductive so that it responds strongly to electromagnetic fields. Like gas plasma does not have a definite shape or a definite volume unless enclosed in a container. Unlike gas under the influence of a magnetic field it may form structures such as filaments beams and double layers. Plasma is the most abundant form of ordinary matter in the Universe of the forms proven to exist the more abundant dark matter is hypothetical and may or may not be explained by ordinary matter most of which is in the rarefied intergalactic regions particularly the intracluster medium and in stars including the Sun. A common form of plasma on Earth is produced in neon signs. Much of the understanding of plasma has come from the pursuit of controlled nuclear fusion and fusion power for which plasma physics provides the scientific foundation.  Free plasma physics books and notes Plasmas the Fourth State of Matter Plasma Science and Technology Plasma on the Internet a list of plasma related links. Introduction to Plasma Physics Graduate course given by Richard FitzpatrickM.I.T. Introduction by I.H.Hutchinson Plasma Material Interaction How to make a glowing ball of plasma in your microwave with a grapeMore Video How to make plasma in your microwave with only one match video OpenPIC3D 3D Hybrid ParticleInCell simulation of plasma dynamics Plasma Formulary Interactive"}, {"topic": "Plasticity (physics)", "content": "In physics and materials science plasticity describes the deformation of a solid material undergoing nonreversible changes of shape in response to applied forces. For example a solid piece of metal being bent or pounded into a new shape displays plasticity as permanent changes occur within the material itself. In engineering the transition from elastic behavior to plastic behavior is called yield. Plastic deformation is observed in most materials particularly metals soils rocks concrete foams bone and skin. However the physical mechanisms that cause plastic deformation can vary widely. At a crystalline scale plasticity in metals is usually a consequence of dislocations. Such defects are relatively rare in most crystalline materials but are numerous in some and part of their crystal structure in such cases plastic crystallinity can result. In brittle materials such as rock concrete and bone plasticity is caused predominantly by slip at microcracks. For many ductile metals tensile loading applied to a sample will cause it to behave in an elastic manner. Each increment of load is accompanied by a proportional increment in extension. When the load is removed the piece returns to its original size. However once the load exceeds a threshold the yield strength the extension increases more rapidly than in the elastic region now when the load is removed some degree of extension will remain. Elastic deformation however is an approximation and its quality depends on the time frame considered and loading speed. If as indicated in the graph opposite the deformation includes elastic deformation it is also often referred to as elastoplastic deformation or elasticplastic deformation. Perfect plasticity is a property of materials to undergo irreversible deformation without any increase in stresses or loads. Plastic materials with hardening necessitate increasingly higher stresses to result in further plastic deformation. Generally plastic deformation is also dependent on the deformation speed i.e. higher stresses usually have to be applied to increase the rate of deformation. Such materials are said to deform viscoplastically.  R. Hill The Mathematical Theory of Plasticity Oxford University Press 1998. Jacob Lubliner Plasticity Theory Macmillan Publishing New York 1990. L. M. Kachanov Fundamentals of the Theory of Plasticity Dover Books. A.S. Khan and S. Huang Continuum Theory of Plasticity Wiley 1995. J. C. Simo T. J. Hughes Computational Inelasticity Springer. M. F. Ashby. Plastic Deformation of Cellular Materials. Encyclopedia of Materials Science and Technology Elsevier Oxford 2001 Pages 70687071. Van Vliet K. J. 3.032 Mechanical Behavior of Materials MIT 2006 International Journal of Plasticity Elsevier Science. Han W and Reddy BD Plasticity Mathematical Theory and Numerical Analysis. 2nd edition Springer New York 2013."}, {"topic": "Pneumatics", "content": "Pneumatics is a branch of engineering that makes use of gas or pressurized air. Pneumatic systems used extensively in industry are commonly powered by compressed air or compressed inert gases. A centrally located and electrically powered compressor powers cylinders air motors and other pneumatic devices. A pneumatic system controlled through manual or automatic solenoid valves is selected when it provides a lower cost more flexible or safer alternative to electric motors and actuators. Pneumatics also has applications in dentistry construction mining and other areas.  Four Ways to Boost Pneumatic Efficiency"}, {"topic": "Point (geometry)", "content": "In modern mathematics a point refers usually to an element of some set called a space. More specifically in Euclidean geometry a point is a primitive notion upon which the geometry is built. Being a primitive notion means that a point cannot be defined in terms of previously defined objects. That is a point is defined only by some properties called axioms that it must satisfy. In particular the geometric points do not have any length area volume or any other dimensional attribute. A common interpretation is that the concept of a point is meant to capture the notion of a unique location in Euclidean space.  Definition of Point with interactive applet Points definition pages with interactive animations that are also useful in a classroom setting. Math Open Reference Point at PlanetMath.org. Weisstein Eric W. Point MathWorld."}, {"topic": "Polymer physics", "content": "Polymer physics is the field of physics that studies polymers their fluctuations mechanical properties as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively. While it focuses on the perspective of condensed matter physics polymer physics is originally a branch of statistical physics. Polymer physics and polymer chemistry are also related with the field of polymer science where this is considered the applicative part of polymers. Polymers are large molecules and thus are very complicated for solving using a deterministic method. Yet statistical approaches can yield results and are often pertinent since large polymers i.e. polymers with a large number of monomers are describable efficiently in the thermodynamic limit of infinitely many monomers although the actual size is clearly finite. Thermal fluctuations continuously affect the shape of polymers in liquid solutions and modeling their effect requires using principles from statistical mechanics and dynamics. As a corollary temperature strongly affects the physical behavior of polymers in solution causing phase transitions melts and so on. The statistical approach for polymer physics is based on an analogy between a polymer and either a Brownian motion or other type of a random walk the selfavoiding walk. The simplest possible polymer model is presented by the ideal chain corresponding to a simple random walk. Experimental approaches for characterizing polymers are also common using Polymer characterization methods such as size exclusion chromatography Viscometry Dynamic light scattering and Automatic Continuous Online Monitoring of Polymerization Reactions ACOMP for determining the chemical physical and material properties of polymers. These experimental methods also helped the mathematical modeling of polymers and even for a better understanding of the properties of polymers. Flory is considered the first scientist establishing the field of polymer physics. French scientists contributed a lot since the 70s e.g. de Gennes J. des Cloizeaux. Doi and Edwards wrote a very famous book in polymer physics. Russian and Soviet schools of physics I. M. Lifshitz A. Yu. Grosberg A.R. Khokhlov  have been very active in the development of polymer physics. "}, {"topic": "Power (physics)", "content": "In physics power is the rate of doing work. It is the amount of energy consumed per unit time. Having no direction it is a scalar quantity. In the SI system the unit of power is the joule per second Js known as the watt in honour of James Watt the eighteenthcentury developer of the steam engine. Another common and traditional measure is horsepower comparing to the power of a horse. Being the rate of work the equation for power can be written P  W t  are equal. These ratios are called the duty cycle of the pulse train. "}, {"topic": "Pressure", "content": "Pressure symbol p or P is the force applied perpendicular to the surface of an object per unit area over which that force is distributed. Gauge pressure also spelled gage pressure is the pressure relative to the ambient pressure. Various units are used to express pressure. Some of these derive from a unit of force divided by a unit of area the SI unit of pressure the pascal Pa for example is one newton per square metre similarly the poundforce per square inch psi is the traditional unit of pressure in the imperial and US customary systems. Pressure may also be expressed in terms of standard atmospheric pressure the atmosphere atm is equal to this pressure and the torr is defined as 1760 of this. Manometric units such as the centimetre of water millimetre of mercury and inch of mercury are used to express pressures in terms of the height of column of a particular fluid in a manometer.  Introduction to Fluid Statics and Dynamics on Project PHYSNET Pressure being a scalar quantity"}, {"topic": "Probability", "content": "Probability is the measure of the likelihood that an event will occur. Probability is quantified as a number between 0 and 1 where 0 indicates impossibility and 1 indicates certainty. The higher the probability of an event the more certain that the event will occur. A simple example is the tossing of a fair unbiased coin. Since the coin is unbiased the two outcomes head and tail are both equally probable the probability of head equals the probability of tail. Since no other outcomes are possible the probability is 12 or 50 of either head or tail. In other words the probability of head is 1 out of 2 outcomes and the probability of tail is also 1 out of 2 outcomes expressed as 0.5 when converted to decimal with the above mentioned quantification system. These concepts have been given an axiomatic mathematical formalization in probability theory see probability axioms which is used widely in such areas of study as mathematics statistics finance gambling science in particular physics artificial intelligencemachine learning computer science game theory and philosophy to for example draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems.  Virtual Laboratories in Probability and Statistics Univ. of Ala.Huntsville Probability on In Our Time at the BBC. InOurTimeProbability listen now Probability and Statistics EBook Edwin Thompson Jaynes. Probability Theory The Logic of Science. Preprint Washington University 1996. HTML index with links to PostScript files and PDF first three chapters People from the History of Probability and Statistics Univ. of Southampton Probability and Statistics on the Earliest Uses Pages Univ. of Southampton Earliest Uses of Symbols in Probability and Statistics on Earliest Uses of Various Mathematical Symbols A tutorial on probability and Bayes theorem devised for firstyear Oxford University students 1 pdf file of An Anthology of Chance Operations 1963 at UbuWeb Introduction to Probability  eBook by Charles Grinstead Laurie Snell Source GNU Free Documentation License English Italian Bruno de Finetti Probabilit e induzione Bologna CLUEB 1993. ISBN 8880911767 digital version Richard P. Feynmans Lecture on probability."}, {"topic": "Probability distribution", "content": "In probability and statistics a probability distribution assigns a probability to each measurable subset of the possible outcomes of a random experiment survey or procedure of statistical inference. Examples are found in experiments whose sample space is nonnumerical where the distribution would be a categorical distribution experiments whose sample space is encoded by discrete random variables where the distribution can be specified by a probability mass function and experiments with sample spaces encoded by continuous random variables where the distribution can be specified by a probability density function. More complex experiments such as those involving stochastic processes defined in continuous time may demand the use of more general probability measures. In applied probability a probability distribution can be specified in a number of different ways often chosen for mathematical convenience by supplying a valid probability mass function or probability density function by supplying a valid cumulative distribution function or survival function by supplying a valid hazard function by supplying a valid characteristic function by supplying a rule for constructing a new random variable from other random variables whose joint probability distribution is known. A probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values a multivariate distribution a joint probability distribution gives the probabilities of a random vectora set of two or more random variablestaking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution the hypergeometric distribution and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.  Hazewinkel Michiel ed. 2001 Probability distribution Encyclopedia of Mathematics Springer ISBN 9781556080104"}, {"topic": "Probability theory", "content": "Probability theory is the branch of mathematics concerned with probability the analysis of random phenomena. The central objects of probability theory are random variables stochastic processes and events mathematical abstractions of nondeterministic events or measured quantities that may either be single occurrences or evolve over time in an apparently random fashion. It is not possible to predict precisely results of random events. However if a sequence of individual events such as coin flipping or the roll of dice is influenced by other factors such as friction it will exhibit certain patterns which can be studied and predicted. Two representative mathematical results describing such patterns are the law of large numbers and the central limit theorem. As a mathematical foundation for statistics probability theory is essential to many human activities that involve quantitative analysis of large sets of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state as in statistical mechanics. A great discovery of twentieth century physics was the probabilistic nature of physical phenomena at atomic scales described in quantum mechanics.  Animation on YouTube on the probability space of dice."}, {"topic": "Proton", "content": "A proton is a subatomic particle symbol p or p with a positive electric charge of 1e elementary charge and mass slightly less than that of a neutron. Protons and neutrons each with masses of approximately one atomic mass unit are collectively referred to as nucleons. One or more protons are present in the nucleus of every atom. The number of protons in the nucleus is the defining property of an element and is referred to as the atomic number represented by the symbol Z. Since each element has a unique number of protons each element has its own unique atomic number. The word proton is Greek for first and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years Rutherford had discovered that the hydrogen nucleus known to be the lightest nucleus could be extracted from the nuclei of nitrogen by collision. Protons were therefore a candidate to be a fundamental particle and a building block of nitrogen and all other heavier atomic nuclei. In the modern Standard Model of particle physics protons are hadrons and like neutrons the other nucleon particle present in atomic nuclei are composed of three quarks. Although protons were originally considered fundamental or elementary particles they are now known to be composed of three valence quarks two up quarks and one down quark. The rest masses of quarks contribute only about 1 of a protons mass however. The remainder of a protons mass is due to quantum chromodynamics binding energy which includes the kinetic energy of the quarks and the energy of the gluon fields that bind the quarks together. Because protons are not fundamental particles they possess a physical size the radius of a proton is about 0.840.87 fm or 69848400000000000000.841015 to 69848700000000000000.871015 m. At sufficiently low temperatures free protons will bind to electrons. However the character of such bound protons does not change and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei until it is captured by the electron cloud of an atom. The result is a protonated atom which is a chemical compound of hydrogen. In vacuum when free electrons are present a sufficiently slow proton may pick up a single free electron becoming a neutral hydrogen atom which is chemically a free radical. Such free hydrogen atoms tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other they form neutral hydrogen molecules H2 which are the most common molecular component of molecular clouds in interstellar space. Such molecules of hydrogen on Earth may then serve among many other uses as a convenient source of protons for accelerators as used in proton therapy and other hadron particle physics experiments that require protons to accelerate with the most powerful and noted example being the Large Hadron Collider.  Particle Data Group at LBL Large Hadron Collider Eaves Laurence Copeland Ed Padilla Antonio Tony 2010. The shrinking proton. Sixty Symbols. Brady Haran for the University of Nottingham."}, {"topic": "Psi particle", "content": "The J JPsi meson or psion is a subatomic particle a flavorneutral meson consisting of a charm quark and a charm antiquark. Mesons formed by a bound state of a charm quark and a charm antiquark are generally known as charmonium. The J is most common form of charmonium due to its low rest mass. The J has a rest mass of 70003096900000000003.0969 GeVc2 just above that of the c 70002983600000000002.9836 GeVc2 and a mean lifetime of 69797200000000000007.21021 s. This lifetime was about a thousand times longer than expected. Its discovery was made independently by two research groups one at the Stanford Linear Accelerator Center headed by Burton Richter and one at the Brookhaven National Laboratory headed by Samuel Ting of MIT. They discovered they had actually found the same particle and both announced their discoveries on 11 November 1974. The importance of this discovery is highlighted by the fact that the subsequent rapid changes in highenergy physics at the time have become collectively known as the November Revolution. Richter and Ting were rewarded for their shared discovery with the 1976 Nobel Prize in Physics.  Glashow S. L. Iliopoulos J. Maiani L. 1970. Weak Interactions with LeptonHadron Symmetry. Physical Review D 2 7 12851292. Bibcode1970PhRvD...2.1285G. doi10.1103PhysRevD.2.1285. Aubert J. et al. 1974. Experimental Observation of a Heavy Particle J. Physical Review Letters 33 23 14041406. Bibcode1974PhRvL..33.1404A. doi10.1103PhysRevLett.33.1404. Augustin J. et al. 1974. Discovery of a Narrow Resonance in ee Annihilation. Physical Review Letters 33 23 14061408. Bibcode1974PhRvL..33.1406A. doi10.1103PhysRevLett.33.1406. Bobra M. 2005. Logbook J particle. Symmetry Magazine 2 7 34. Yao W.M. Particle Data Group et al. 2006. Review of Particle Physics Naming Scheme for Hadrons PDF. Journal of Physics G 33 108. arXivastroph0601168. Bibcode2006JPhG...33....1Y. doi10.108809543899331001."}, {"topic": "Psychophysics", "content": "Psychophysics quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they produce. Psychophysics has been described as the scientific study of the relation between stimulus and sensation or more completely as the analysis of perceptual processes by studying the effect on a subjects experience or behaviour of systematically varying the properties of a stimulus along one or more physical dimensions. Psychophysics also refers to a general class of methods that can be applied to study a perceptual system. Modern applications rely heavily on threshold measurement ideal observer analysis and signal detection theory. Psychophysics has widespread and important practical applications. For example in the study of digital signal processing psychophysics has informed the development of models and methods of lossy compression. These models explain why humans perceive very little loss of signal quality when audio and video signals are formatted using lossy compression.  Media related to Psychophysics at Wikimedia Commons Link German website about a dissertation project with an animation about the staricase method Transformed UpDown Staricase Method"}, {"topic": "Pulley", "content": "A pulley is a wheel on an axle or shaft that is designed to support movement and change of direction of a taut cable or belt along its circumference. Pulleys are used in a variety of ways to lift loads apply forces and to transmit power. In nautical contexts the assembly of wheel axle and supporting shell is referred to as a block. A pulley may also be called a sheave or drum and may have a groove or grooves between two flanges around its circumference. The drive element of a pulley system can be a rope cable belt or chain that runs over the pulley inside the groove or grooves. Hero of Alexandria identified the pulley as one of six simple machines used to lift weights. Pulleys are assembled to form a block and tackle in order to provide mechanical advantage to apply large forces. Pulleys are also assembled as part of belt and chain drives in order to transmit power from one rotating shaft to another.  httpwww.khanacademy.orgsciencephysicsmechanicsvmechanicaladvantagepart3"}, {"topic": "Pulse (physics)", "content": "In physics a pulse is a single disturbance that moves through a medium from one point to the next point. "}, {"topic": "Pulse wave", "content": "A pulse wave or pulse train is a kind of nonsinusoidal waveform that is similar to a square wave but does not have the symmetrical shape associated with a perfect square wave. It is a term common to synthesizer programming and is a typical waveform available on many synthesizers. The exact shape of the wave is determined by the duty cycle of the oscillator. In many synthesizers the duty cycle can be modulated sometimes called pulsewidth modulation for a more dynamic timbre. The pulse wave is also known as the rectangular wave the periodic version of the rectangular function. The Fourier series expansion for a rectangular pulse wave with period T and pulse time is f  t   T  n  1 2 n sin  n T  cos  2 n T t   Note that for symmetry the starting time t  0 in this expansion is halfway through the first pulse. The phase can be offset to match the accompanying graph by replacing t with t  2. A pulse wave can be created by subtracting a sawtooth wave from a phaseshifted version of itself. If the sawtooth waves are bandlimited the resulting pulse wave is bandlimited too. Another way to create one is with a single ramp wave sawtooth or triangle and a comparator with the ramp wave on one input and a variable DC threshold on the other. The result will be a precisely controlled pulse width but it will not be bandlimited. Acoustically the rectangular wave has been described as having a more narrow and nasal sound than a perfect square wave and its characteristic sound features prominently in many Steve Winwood songs. "}, {"topic": "Quantum", "content": "In physics a quantum plural quanta is the minimum amount of any physical entity involved in an interaction. Behind this one finds the fundamental notion that a physical property may be quantized referred to as the hypothesis of quantization. This means that the magnitude of the physical property can take on only certain discrete values. For example a photon is a single quantum of visible light as well as a single quantum of all other forms of electromagnetic radiation and can be referred to as a light quantum. The energy of an electron bound to an atom is also quantized and thus can only exist in certain discrete values. As a result atoms are stable and hence matter in general is stable. As incorporated into the theory of quantum mechanics this quantization of the energy of electrons and the resulting implications are regarded by physicists as part of the fundamental framework for understanding and describing nature.  B. Hoffmann The Strange Story of the Quantum Pelican 1963. Lucretius On the Nature of the Universe transl. from the Latin by R.E. Latham Penguin Books Ltd. Harmondsworth 1951. There are of course many translations and the translations title varies. Some put emphasis on how things work others on what things are found in nature. J. Mehra and H. Rechenberg The Historical Development of Quantum Theory Vol.1 Part 1 SpringerVerlag New York Inc. New York 1982. M. Planck A Survey of Physical Theory transl. by R. Jones and D.H. Williams Methuen  Co. Ltd. London 1925 Dover editions 1960 and 1993 including the Nobel lecture. Rodney Brooks 2011 Fields of Color The theory that escaped Einstein. Allegra Print  Imaging."}, {"topic": "Quantum electrodynamics", "content": "In particle physics quantum electrodynamics QED is the relativistic quantum field theory of electrodynamics. In essence it describes how light and matter interact and is the first theory where full agreement between quantum mechanics and special relativity is achieved. QED mathematically describes all phenomena involving electrically charged particles interacting by means of exchange of photons and represents the quantum counterpart of classical electromagnetism giving a complete account of matter and light interaction. In technical terms QED can be described as a perturbation theory of the electromagnetic quantum vacuum. Richard Feynman called it the jewel of physics for its extremely accurate predictions of quantities like the anomalous magnetic moment of the electron and the Lamb shift of the energy levels of hydrogen.  Feynmans Nobel Prize lecture describing the evolution of QED and his role in it Feynmans New Zealand lectures on QED for nonphysicists httpqed.wikina.org  Animations demonstrating QED"}, {"topic": "Quantum field theory", "content": "In theoretical physics quantum field theory QFT is the theoretical framework for constructing quantum mechanical models of subatomic particles in particle physics and quasiparticles in condensed matter physics. A QFT treats particles as excited states of the underlying physical field so these are called field quanta. In quantum field theory quantum mechanical interactions between particles are described by interaction terms between the corresponding underlying quantum fields. These interactions are conveniently visualized by Feynman diagrams that also serve as a formal tool to evaluate various processes. Historically the development began in the 1920s with the quantization of the electromagnetic field the quantization being based on an analogy of the eigenmode expansion of a vibrating string with fixed endpoints. In Weinberg 2005 QFT is brought forward as an unavoidable consequence of the reconciliation of quantum mechanics with special relativity.  Hazewinkel Michiel ed. 2001 Quantum field theory Encyclopedia of Mathematics Springer ISBN 9781556080104 Stanford Encyclopedia of Philosophy Quantum Field Theory by Meinard Kuhlmann. Siegel Warren 2005. Fields. A free text also available from arXivhepth9912205. Quantum Field Theory by P. J. Mulders"}, {"topic": "Quantum mechanics", "content": "Quantum mechanics QM also known as quantum physics or quantum theory including quantum field theory is a fundamental branch of physics concerned with processes involving for example atoms and photons. Systems such as these which obey quantum mechanics can be in a quantum superposition of different states unlike in classical physics. Quantum mechanics gradually arose from Max Plancks solution in 1900 to the blackbody radiation problem reported 1859 and Albert Einsteins 1905 paper which offered a quantumbased theory to explain the photoelectric effect reported 1887. Early quantum theory was profoundly reconceived in the mid1920s. The reconceived theory is formulated in various specially developed mathematical formalisms. In one of them a mathematical function the wave function provides information about the probability amplitude of position momentum and other physical properties of a particle. Important applications of quantum theory include superconducting magnets lightemitting diodes and the laser the transistor and semiconductors such as the microprocessor medical and research imaging such as magnetic resonance imaging and electron microscopy and explanations for many biological and physical phenomena.  3D animations applications and research for basic quantum effects animations also available in commons.wikimedia.org Universit paris Sud Quantum Cook Book by R. Shankar Open Yale PHYS 201 material 4pp The Modern Revolution in Physics  an online textbook. J. OConnor and E. F. Robertson A history of quantum mechanics. Introduction to Quantum Theory at Quantiki. Quantum Physics Made Relatively Simple three video lectures by Hans Bethe H is for hbar. Quantum Mechanics Books Collection Collection of free books Course material Quantum Physics Database  Fundamentals and Historical Background of Quantum Theory. Doron Cohen Lecture notes in Quantum Mechanics comprehensive with advanced topics. MIT OpenCourseWare Chemistry. MIT OpenCourseWare Physics. See 8.04 Stanford Continuing Education PHY 25 Quantum Mechanics by Leonard Susskind see course description Fall 2007 5 Examples in Quantum Mechanics Imperial College Quantum Mechanics Course. Spark Notes  Quantum Physics. Quantum Physics Online  interactive introduction to quantum mechanics RS applets. Experiments to the foundations of quantum physics with single photons. AQME  Advancing Quantum Mechanics for Engineers by T.Barzso D.Vasileska and G.Klimeck online learning resource with simulation tools on nanohub Quantum Mechanics by Martin Plenio Quantum Mechanics by Richard Fitzpatrick Online course on Quantum Transport FAQs Manyworlds or relativestate interpretation. Measurement in Quantum mechanics. Media PHYS 201 Fundamentals of Physics II by Ramamurti Shankar Open Yale Course Lectures on Quantum Mechanics by Leonard Susskind Everything you wanted to know about the quantum world archive of articles from New Scientist. Quantum Physics Research from Science Daily Overbye Dennis December 27 2005. Quantum Trickery Testing Einsteins Strangest Theory. The New York Times. Retrieved April 12 2010. Audio Astronomy Cast Quantum Mechanics June 2009. Fraser Cain interviews Pamela L. Gay. Philosophy Ismael Jenann. Quantum Mechanics. Stanford Encyclopedia of Philosophy. Krips Henry. Measurement in Quantum Theory. Stanford Encyclopedia of Philosophy."}, {"topic": "Quantum optics", "content": "Quantum optics is a field of research that uses semiclassical and quantummechanical physics to investigate phenomena involving light and its interactions with matter at submicroscopic levels.  An introduction to quantum optics of the light field Encyclopedia of laser physics and technology with content on quantum optics particularly quantum noise in lasers by Rdiger Paschotta. Qwiki  A quantum physics wiki devoted to providing technical resources for practicing quantum physicists. Quantiki  a freecontent WWW resource in quantum information science that anyone can edit. Various Quantum Optics Reports"}, {"topic": "Quantum physics", "content": "Quantum mechanics QM also known as quantum physics or quantum theory including quantum field theory is a fundamental branch of physics concerned with processes involving for example atoms and photons. Systems such as these which obey quantum mechanics can be in a quantum superposition of different states unlike in classical physics. Quantum mechanics gradually arose from Max Plancks solution in 1900 to the blackbody radiation problem reported 1859 and Albert Einsteins 1905 paper which offered a quantumbased theory to explain the photoelectric effect reported 1887. Early quantum theory was profoundly reconceived in the mid1920s. The reconceived theory is formulated in various specially developed mathematical formalisms. In one of them a mathematical function the wave function provides information about the probability amplitude of position momentum and other physical properties of a particle. Important applications of quantum theory include superconducting magnets lightemitting diodes and the laser the transistor and semiconductors such as the microprocessor medical and research imaging such as magnetic resonance imaging and electron microscopy and explanations for many biological and physical phenomena.  3D animations applications and research for basic quantum effects animations also available in commons.wikimedia.org Universit paris Sud Quantum Cook Book by R. Shankar Open Yale PHYS 201 material 4pp The Modern Revolution in Physics  an online textbook. J. OConnor and E. F. Robertson A history of quantum mechanics. Introduction to Quantum Theory at Quantiki. Quantum Physics Made Relatively Simple three video lectures by Hans Bethe H is for hbar. Quantum Mechanics Books Collection Collection of free books Course material Quantum Physics Database  Fundamentals and Historical Background of Quantum Theory. Doron Cohen Lecture notes in Quantum Mechanics comprehensive with advanced topics. MIT OpenCourseWare Chemistry. MIT OpenCourseWare Physics. See 8.04 Stanford Continuing Education PHY 25 Quantum Mechanics by Leonard Susskind see course description Fall 2007 5 Examples in Quantum Mechanics Imperial College Quantum Mechanics Course. Spark Notes  Quantum Physics. Quantum Physics Online  interactive introduction to quantum mechanics RS applets. Experiments to the foundations of quantum physics with single photons. AQME  Advancing Quantum Mechanics for Engineers by T.Barzso D.Vasileska and G.Klimeck online learning resource with simulation tools on nanohub Quantum Mechanics by Martin Plenio Quantum Mechanics by Richard Fitzpatrick Online course on Quantum Transport FAQs Manyworlds or relativestate interpretation. Measurement in Quantum mechanics. Media PHYS 201 Fundamentals of Physics II by Ramamurti Shankar Open Yale Course Lectures on Quantum Mechanics by Leonard Susskind Everything you wanted to know about the quantum world archive of articles from New Scientist. Quantum Physics Research from Science Daily Overbye Dennis December 27 2005. Quantum Trickery Testing Einsteins Strangest Theory. The New York Times. Retrieved April 12 2010. Audio Astronomy Cast Quantum Mechanics June 2009. Fraser Cain interviews Pamela L. Gay. Philosophy Ismael Jenann. Quantum Mechanics. Stanford Encyclopedia of Philosophy. Krips Henry. Measurement in Quantum Theory. Stanford Encyclopedia of Philosophy."}, {"topic": "Quantum realm", "content": "The quantum realm also called the quantum scale is a term of art in physics referring to scales where quantum mechanical effects become important when studied as an isolated system. Typically this means distances of 100 nanometers 109 meters or less or at very low temperature. More precisely it is where the action or angular momentum is quantized. While originating on the nanometer scale such effects can operate on a macro level generating some paradoxes like in the Schrdingers cat thought experiment. Two classical examples are electron tunneling and the doubleslit experiment. Most fundamental processes in molecular electronics organic electronics and organic semiconductors also originate in the quantum realm. The quantum realm can also sometimes involve actions. An example is David Bohms 1951 version of the famous thought experiment that Albert Einstein Boris Podolsky and Nathan Rosen proposed in 1935 the EPR paradox. Pairs of particles are emitted from a source in the socalled spin singlet state and rush in opposite directions. When the particles are widely separated from each other they each encounter a measuring apparatus that can be set to measure their spin components along various directions. Although the measurement events are distant from each other so that no slowerthanlight or light signal can travel between them the measurement outcomes are curiously correlated. "}, {"topic": "Quantum state", "content": "In quantum physics quantum state refers to the state of an isolated quantum system. A quantum state provides a probability distribution for the value of each observable i.e. for the outcome of each possible measurement on the system. Knowledge of the quantum state together with the rules for the systems evolution in time exhausts all that can be predicted about the systems behavior. A mixture of quantum states is again a quantum state. Quantum states that cannot be written as a mixture of other states are called pure quantum states all other states are called mixed quantum states. Mathematically a pure quantum state can be represented by a ray in a Hilbert space over the complex numbers. The ray is a set of nonzero vectors differing by just a complex scalar factor any of them can be chosen as a state vector to represent the ray and thus the state. A unit vector is usually picked but its phase factor can be chosen freely anyway. Nevertheless such factors are important when state vectors are added together to form a superposition. Hilbert space is a generalization of the ordinary Euclidean space and it contains all possible pure quantum states of the given system. If this Hilbert space by choice of representation essentially a choice of basis corresponding to a complete set of observables is exhibited as a function space a Hilbert space in its own right then the representatives are called wave functions. For example when dealing with the energy spectrum of the electron in a hydrogen atom the relevant state vectors are identified by the principal quantum number n the angular momentum quantum number l the magnetic quantum number m and the spin zcomponent sz. A more complicated case is given in braket notation by the spin part of a state vector   1 2       of the pure states and the other being a statistical said incoherent average with the probabilities ps of those states. According to Wigner the concept of mixture was put forward by Landau.  The concept of quantum states in particular the content of the section Formalism in quantum physics above is covered in most standard textbooks on quantum mechanics. For a discussion of conceptual aspects and a comparison with classical states see Isham Chris J 1995. Lectures on Quantum Theory Mathematical and Structural Foundations. Imperial College Press. ISBN 9781860940019. For a more detailed coverage of mathematical aspects see Bratteli Ola Robinson Derek W 1987. Operator Algebras and Quantum Statistical Mechanics 1. Springer. ISBN 9783540170938. 2nd edition. In particular see Sec. 2.3. For a discussion of purifications of mixed quantum states see Chapter 2 of John Preskills lecture notes for Physics 219 at Caltech."}, {"topic": "Quark", "content": "A quark kwrk or kwrk is an elementary particle and a fundamental constituent of matter. Quarks combine to form composite particles called hadrons the most stable of which are protons and neutrons the components of atomic nuclei. Due to a phenomenon known as color confinement quarks are never directly observed or found in isolation they can be found only within hadrons such as baryons of which protons and neutrons are examples and mesons. For this reason much of what is known about quarks has been drawn from observations of the hadrons themselves. Quarks have various intrinsic properties including electric charge mass color charge and spin. Quarks are the only elementary particles in the Standard Model of particle physics to experience all four fundamental interactions also known as fundamental forces electromagnetism gravitation strong interaction and weak interaction as well as the only known particles whose electric charges are not integer multiples of the elementary charge. There are six types of quarks known as flavors up down strange charm top and bottom. Up and down quarks have the lowest masses of all quarks. The heavier quarks rapidly change into up and down quarks through a process of particle decay the transformation from a higher mass state to a lower mass state. Because of this up and down quarks are generally stable and the most common in the universe whereas strange charm bottom and top quarks can only be produced in high energy collisions such as those involving cosmic rays and in particle accelerators. For every quark flavor there is a corresponding type of antiparticle known as an antiquark that differs from the quark only in that some of its properties have equal magnitude but opposite sign. The quark model was independently proposed by physicists Murray GellMann and George Zweig in 1964. Quarks were introduced as parts of an ordering scheme for hadrons and there was little evidence for their physical existence until deep inelastic scattering experiments at the Stanford Linear Accelerator Center in 1968. Accelerator experiments have provided evidence for all six flavors. The top quark was the last to be discovered at Fermilab in 1995.  1969 Physics Nobel Prize lecture by Murray GellMann 1976 Physics Nobel Prize lecture by Burton Richter 1976 Physics Nobel Prize lecture by Samuel C.C. Ting 2008 Physics Nobel Prize lecture by Makoto Kobayashi 2008 Physics Nobel Prize lecture by Toshihide Maskawa The Top Quark And The Higgs Particle by T.A. Heppenheimer A description of CERNs experiment to count the families of quarks. Bowley Roger Copeland Ed. Quarks. Sixty Symbols. Brady Haran for the University of Nottingham."}, {"topic": "Quasiparticles", "content": "In physics quasiparticles and collective excitations which are closely related are emergent phenomena that occur when a microscopically complicated system such as a solid behaves as if it contained different weakly interacting particles in free space. For example as an electron travels through a semiconductor its motion is disturbed in a complex way by its interactions with all of the other electrons and nuclei however it approximately behaves like an electron with a different mass effective mass traveling unperturbed through free space. This electron with a different mass is called an electron quasiparticle. In another example the aggregate motion of electrons in the valence band of a semiconductor is the same as if the semiconductor contained instead positively charged quasiparticles called holes. Other quasiparticles or collective excitations include phonons particles derived from the vibrations of atoms in a solid plasmons particles derived from plasma oscillations and many others. These particles are typically called quasiparticles if they are related to fermions like electrons and holes and called collective excitations if they are related to bosons like phonons and plasmons although the precise distinction is not universally agreed upon. The quasiparticle concept is most important in condensed matter physics since it is one of the few known ways of simplifying the quantum mechanical manybody problem.  PhysOrg.com Scientists find new quasiparticles Curious quasiparticles baffle physicists by Jacqui Hayes Cosmos 6 June 2008. Accessed June 2008"}, {"topic": "Radiant energy", "content": "In radiometry radiant energy is the energy of electromagnetic and gravitational radiation. The SI unit of radiant energy is the joule J. The quantity of radiant energy may be calculated by integrating radiant flux or power with respect to time. The symbol Qe is often used throughout literature to denote radiant energy e for energetic to avoid confusion with photometric quantities. In branches of physics other than radiometry electromagnetic energy is referred to using E or W. The term is used particularly when electromagnetic radiation is emitted by a source into the surrounding environment. This radiation may be visible or invisible to the human eye. "}, {"topic": "Radiation", "content": "In physics radiation is the emission or transmission of energy in the form of waves or particles through space or through a material medium. This includes electromagnetic radiation such as radio waves visible light xrays and gamma radiation  particle radiation such as alpha radiation  beta radiation  and neutron radiation particles of nonzero rest energy acoustic radiation such as ultrasound sound and seismic waves dependent on a physical transmission medium gravitational radiation radiation that takes the form of gravitational waves or ripples in the curvature of spacetime. Radiation is often categorized as either ionizing or nonionizing depending on the energy of the radiated particles. Ionizing radiation carries more than 10 eV which is enough to ionize atoms and molecules and break chemical bonds. This is an important distinction due to the large difference in harmfulness to living organisms. A common source of ionizing radiation is radioactive materials that emit   or radiation consisting of helium nuclei electrons or positrons and photons respectively. Other sources include Xrays from medical radiography examinations and muons mesons positrons neutrons and other particles that constitute the secondary cosmic rays that are produced after primary cosmic rays interact with Earths atmosphere. Gamma rays Xrays and the higher energy range of ultraviolet light constitute the ionizing part of the electromagnetic spectrum. The lowerenergy longerwavelength part of the spectrum including visible light infrared light microwaves and radio waves is nonionizing its main effect when interacting with tissue is heating. This type of radiation only damages cells if the intensity is high enough to cause excessive heating. Ultraviolet radiation has some features of both ionizing and nonionizing radiation. While the part of the ultraviolet spectrum that penetrates the Earths atmosphere is nonionizing this radiation does far more damage to many molecules in biological systems than can be accounted for by heating effects sunburn being a wellknown example. These properties derive from ultraviolets power to alter chemical bonds even without having quite enough energy to ionize atoms. The word radiation arises from the phenomenon of waves radiating i.e. traveling outward in all directions from a source. This aspect leads to a system of measurements and physical units that are applicable to all types of radiation. Because such radiation expands as it passes through space and as its energy is conserved in vacuum the intensity of all types of radiation from a point source follows an inversesquare law in relation to the distance from its source. This law does not apply close to an extended source of radiation or for focused beams.  Radiation on In Our Time at the BBC. listen now Health Physics Society Public Education Website Ionizing Radiation and Radon from World Health Organization QA Health effects of radiation exposure BBC News 21 July 2011."}, {"topic": "Radius of curvature (optics)", "content": "Radius of curvature ROC has specific meaning and sign convention in optical design. A spherical lens or mirror surface has a center of curvature located in x y z either along or decentered from the system local optical axis. The vertex of the lens surface is located on the local optical axis. The distance from the vertex to the center of curvature is the radius of curvature of the surface. The sign convention for the optical radius of curvature is as follows If the vertex lies to the left of the center of curvature the radius of curvature is positive. If the vertex lies to the right of the center of curvature the radius of curvature is negative. Thus when viewing a biconvex lens from the side the left surface radius of curvature is positive and the right surface has a negative radius of curvature. Note however that in areas of optics other than design other sign conventions are sometimes used. In particular many undergraduate physics textbooks use an alternate sign convention in which convex surfaces of lenses are always positive. Care should be taken when using formulas taken from different sources.  Radius of curvature applications Radius Base curve radius Cardinal point optics Vergence optics"}, {"topic": "Redshift", "content": "In physics redshift happens when light or other electromagnetic radiation from an object is increased in wavelength or shifted to the red end of the spectrum. In general whether or not the radiation is within the visible spectrum redder means an increase in wavelength equivalent to a lower frequency and a lower photon energy in accordance with respectively the wave and quantum theories of light. Some redshifts are an example of the Doppler effect familiar in the change of apparent pitches of sirens and frequency of the sound waves emitted by speeding vehicles. A redshift occurs whenever a light source moves away from an observer. Another kind of redshift is cosmological redshift which is due to the expansion of the universe and sufficiently distant light sources generally more than a few million light years away show redshift corresponding to the rate of increase in their distance from Earth. Finally gravitational redshift is a relativistic effect observed in electromagnetic radiation moving out of gravitational fields. Conversely a decrease in wavelength is called blueshift and is generally seen when a lightemitting object moves toward an observer or when electromagnetic radiation moves into a gravitational field. However redshift is a more common term and sometimes blueshift is referred to as negative redshift. Knowledge of redshifts and blueshifts has been applied to develop several terrestrial technologies such as Doppler radar and radar guns. Redshifts are also seen in the spectroscopic observations of astronomical objects. Its value is represented by the letter z. A special relativistic redshift formula and its classical approximation can be used to calculate the redshift of a nearby object when spacetime is flat. However in many contexts such as black holes and Big Bang cosmology redshifts must be calculated using general relativity. Special relativistic gravitational and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation including scattering and optical effects however the resulting changes are distinguishable from true redshift and are not generally referred to as such see section on physical optics and radiative transfer.  Ned Wrights Cosmology tutorial Cosmic reference guide entry on redshift Mike Luciuks Astronomical Redshift tutorial Animated GIF of Cosmological Redshift by Wayne Hu Merrifield Michael Hill Richard 2009. Z Redshift. SIXT SYMBLS. Brady Haran for the University of Nottingham."}, {"topic": "Refraction", "content": "Refraction is the change in direction of propagation of a wave due to a change in its transmission medium. The phenomenon is explained by the conservation of energy and the conservation of momentum. Due to the change of medium the phase velocity of the wave is changed but its frequency remains constant. This is most commonly observed when a wave passes from one medium to another at any angle other than 0 from the normal. Refraction of light is the most commonly observed phenomenon but any type of wave can refract when it interacts with a medium for example when sound waves pass from one medium into another or when water waves move into water of a different depth. Refraction is described by Snells law which states that for a given pair of media and a wave with a single frequency the ratio of the sines of the angle of incidence 1 and angle of refraction 2 is equivalent to the ratio of phase velocities v1  v2 in the two media or equivalently to the opposite ratio of the indices of refraction n2  n1 sin 1 sin 2  v 1 v 2  n 2 n 1 .  In general the incident wave is partially refracted and partially reflected the details of this behavior are described by the Fresnel equations.  Java illustration of refraction Java simulation of refraction through a prism Reflections and Refractions in Ray Tracing a simple but thorough discussion of the mathematics behind refraction and reflection. Flash refraction simulation includes source Explains refraction and Snells Law. Animations demonstrating optical refraction by QED"}, {"topic": "Refractive index", "content": "In optics the refractive index or index of refraction n of a material is a dimensionless number that describes how light propagates through that medium. It is defined as n  c v   where is the density and M is the molar mass.  NIST calculator for determining the refractive index of air Dielectric materials Science World Filmetrics online database Free database of refractive index and absorption coefficient information RefractiveIndex.INFO Refractive index database featuring online plotting and parameterisation of data soprasa.com Refractive index database as text files signup required LUXPOP Thin film and bulk index of refraction and photonics calculations"}, {"topic": "Relative atomic mass", "content": "Relative atomic mass symbol Ar is a dimensionless physical quantity the ratio of the average mass of atoms of an element from a single given sample or source to 112 of the mass of an atom of carbon12 known as the unified atomic mass unit. The relative atomic mass is a statistical term referring to an abundanceweighted figure involving measurement of many atoms. As in all related terms the word relative refers to making the figure relative to carbon12 so that the final figure is dimensionless. The term relative atomic mass is exactly equivalent to atomic weight which is the older term. In technical usage these values are samplespecific i.e. element sourcespecific when a natural element source is composed of more than one isotope. Thus two samples of a chemical element which is naturally found as being composed of more than one isotope collected from two substantially different sources are expected to give slightly different relative atomic masses atomic weights because isotopic concentrations typically vary slightly due to the history origin of the source. These values differences are real and repeatable and can be used to identify specific samples. For example a sample of elemental carbon from volcanic methane will have a different relative atomic mass atomic weight than one collected from plant or animal tissues for more see isotope geochemistry. In short the atomic weight relative atomic mass of carbon varies slightly from place to place and from source to source a fact that can be useful. However a typical standard figure also can be useful as follows. Both the terms relative atomic mass and atomic weight are sometimes loosely used to refer to a technically different standardized expectation value called the standard atomic weight. This value is the mean value of atomic weights of a number of normal samples of the element in question. For this definition a normal sample is any reasonably possible source of the element or its compounds in commerce for industry and science and has not been subject to significant modification of isotopic composition within a geologically brief period. These standard atomic weights are published at regular intervals by the Commission on Isotopic Abundances and Atomic Weights of the International Union of Pure and Applied Chemistry IUPAC The standard values are intended as mean values that compensate for small variances in the isotopic composition of the chemical elements across a range of ordinary samples on Earth and thus to be applicable to normal laboratory materials. However they may not accurately reflect values from samples from unusual locations or extraterrestrial objects which often have more widely variant isotopic compositions. The standard atomic weights are reprinted in a wide variety of textbooks commercial catalogues Periodic Table wall charts etc. and in the table below. They are what chemists loosely call atomic weights. The continued use of the term atomic weight of any element as opposed to relative atomic mass has attracted considerable controversy since at least the 1960s mainly due to the technical difference between weight and mass in physics. see below. Both terms are officially sanctioned by IUPAC. The term relative atomic mass now seems to be gaining as the preferred term over atomic weight although in the case of standard atomic weight this shorter term as opposed to standard relative atomic mass continues to be preferred.  IUPAC Commission on Isotopic Abundances and Atomic Weights NIST relative atomic masses of all isotopes and the standard atomic weights of the elements Atomic Weights of the Elements 2011"}, {"topic": "Rigid body", "content": "In physics a rigid body is an idealization of a solid body in which deformation is neglected. In other words the distance between any two given points of a rigid body remains constant in time regardless of external forces exerted on it. Even though such an object cannot physically exist due to relativity objects can normally be assumed to be perfectly rigid if they are not moving near the speed of light. In classical mechanics a rigid body is usually considered as a continuous mass distribution while in quantum mechanics a rigid body is usually thought of as a collection of point masses. For instance in quantum mechanics molecules consisting of the point masses electrons and nuclei are often seen as rigid bodies see classification of molecules as rigid rotors. "}, {"topic": "Roche limit", "content": "In celestial mechanics the Roche limit pronounced o in IPA similar to the sound of rosh sometimes referred to as the Roche radius is the distance within which a celestial body held together only by its own gravity will disintegrate due to a second celestial bodys tidal forces exceeding the first bodys gravitational selfattraction. Inside the Roche limit orbiting material disperses and forms rings whereas outside the limit material tends to coalesce. The term is named after douard Roche who is the French astronomer who first calculated this theoretical limit in 1848.  Discussion of the Roche Limit Audio CainGay Astronomy Cast Tidal Forces Across the Universe July 2007. Cp nht 682007"}, {"topic": "Rotational energy", "content": "Rotational energy or angular kinetic energy is kinetic energy due to the rotation of an object and is part of its total kinetic energy. Looking at rotational energy separately around an objects axis of rotation the following dependence on the objects moment of inertia is observed E r o t a t i o n a l  1 2 I 2   takes the role of the linear velocity v. The rotational energy of a rolling cylinder varies from one half of the translational energy if it is massive to the same as the translational energy if it is hollow. An example is the calculation of the rotational kinetic energy of the Earth. As the Earth has a period of about 23.93 hours it has an angular velocity of 7.29105 rads. The Earth has a moment of inertia I  8.041037 kgm2. Therefore it has a rotational kinetic energy of 2.1381029 J. Part of it can be tapped using tidal power. Additional friction of the two global tidal waves creates energy in a physical manner infinitesimally slowing down Earths angular velocity . Due to the conservation of angular momentum this process transfers angular momentum to the Moons orbital motion increasing its distance from Earth and its orbital period see tidal locking for a more detailed explanation of this process. "}, {"topic": "Rotational speed", "content": "Rotational speed or speed of revolution of an object rotating around an axis is the number of turns of the object divided by time specified as revolutions per minute rpm revolutions per second revs or radians per second rads. Rotational speed is equal to the angular velocity or  divided by 2. The symbol for rotational speed is c y c  is angular speed in degrees per second For example a stepper motor might turn exactly one complete revolution each second. Its angular speed is 360 degrees per second 360s or 2 radians per second 2 rads while the rotational speed is 60 rpm. Rotational speed is not to be confused with tangential speed despite some relation between the two concepts. Imagine a rotating merrygoround. No matter how close or far you stand from the axis of rotation your rotational speed will remain constant. However your tangential speed does not remain constant. If you stand two meters from the axis of rotation your tangential speed will be double the amount if you were standing only one meter from the axis of rotation. "}, {"topic": "Rydberg formula", "content": "The Rydberg formula is used in atomic physics to describe the wavelengths of spectral lines of many chemical elements. It was formulated by the Swedish physicist Johannes Rydberg and presented on 5 November 1888. In 1880 Rydberg worked on a formula describing the relation between the wavelengths in spectral lines of alkali metals. He noticed that lines came in series and he found that he could simplify his calculations by using the wavenumber the number of waves occupying the unit length equal to 1 the inverse of the wavelength as his unit of measurement. He plotted the wavenumbers n of successive lines in each series against consecutive integers which represented the order of the lines in that particular series. Finding that the resulting curves were similarly shaped he sought a single function which could generate all of them when appropriate constants were inserted. First he tried the formula n  n 0 C 0 m  m   corresponding to the principal quantum numbers of the orbitals occupied before and after. Its important to notice that this formula can be directly applied only to hydrogenlike also called hydrogenic atoms of chemical elements i.e. atoms with only one electron being affected by an effective nuclear charge which is easily estimated. Examples would include He Li2 Be3 etc. where no other electrons exist in the atom. But the Rydberg formula also provides correct wavelengths for distant electrons where the effective nuclear charge can be estimated as the same as that for hydrogen since all but one of the nuclear charges have been screened by other electrons and the core of the atom has an effective positive charge of 1. Finally with certain modifications replacement of Z by Z1 and use of the integers 1 and 2 for the ns to give a numerical value of 34 for the difference of their inverse squares the Rydberg formula provides correct values in the special case of Kalpha lines since the transition in question is the Kalpha transition of the electron from the 1s orbital to the 2p orbital. This is analogous to the Lymanalpha line transition for hydrogen and has the same frequency factor. Because the 2p electron is not screened by any other electrons in the atom from the nucleus the nuclear charge is diminished only by the single remaining 1s electron causing the system to be effectively a hydrogenic atom but with a diminished nuclear charge Z1. Its frequency is thus the Lymanalpha hydrogen frequency increased by a factor of Z12. This formula of f  c  Lymanalpha frequencyZ12 is historically known as Moseleys law having added a factor c to convert wavelength to frequency and can be used to predict wavelengths of the K Kalpha Xray spectral emission lines of chemical elements from aluminum to gold. See the biography of Henry Moseley for the historical importance of this law which was derived empirically at about the same time it was explained by the Bohr model of the atom. For other spectral transitions in multielectron atoms the Rydberg formula generally provides incorrect results since the magnitude of the screening of inner electrons for outerelectron transitions is variable and not possible to compensate for in the simple manner above.  Sutton Mike July 2004. Getting the numbers right The lonely struggle of the 19th century physicistchemist Johannes Rydberg. Chemistry World 1 7 3841. ISSN 14737604. Martinson I. Curtis L.J. 2005. Janne Rydberg his life and work PDF. NIM B 235 1722. Bibcode2005NIMPB.235...17M. doi10.1016j.nimb.2005.03.137."}, {"topic": "SI derived unit", "content": "The International System of Units SI specifies a set of seven base units from which all other SI units of measurement are derived. Each of these other units SI derived units is either dimensionless or can be expressed as a product of positive or negative but usually integral powers of one or more of the base units. For example the SI derived unit of area is the square metre m2 and the SI derived unit of density is the kilogram per cubic metre kgm3 or kg m3. The degree Celsius see the table below has a somewhat unclear status and is arguably an exception to this rule. The names of SI units are written in lowercase. The symbols for units named after persons however are always written with an uppercase initial letter e.g. the symbol for the hertz is Hz but the symbol for the metre is m.  I. Mills Tomislav Cvitas Klaus Homann Nikola Kallay IUPAC June 1993. Quantities Units and Symbols in Physical Chemistry 2nd ed.. Blackwell Science Inc. p. 72. CS1 maint Multiple names authors list link"}, {"topic": "SI units", "content": "The International System of Units French Systme international dunits SI is the modern form of the metric system and is the most widely used system of measurement. It comprises a coherent system of units of measurement built on seven base units. It defines twentytwo named units and includes many more unnamed coherent derived units. The system also establishes a set of twenty prefixes to the unit names and unit symbols that may be used when specifying multiples and fractions of the units. The system was published in 1960 as the result of an initiative that began in 1948. It is based on the metrekilogramsecond system of units MKS rather than any variant of the centimetregramsecond system CGS. SI is intended to be an evolving system so prefixes and units are created and unit definitions are modified through international agreement as the technology of measurement progresses and the precision of measurements improves. The 24th and 25th General Conferences on Weights and Measures CGPM in 2011 and 2014 for example discussed a proposal to change the definition of the kilogram linking it to an invariant of nature rather than to the mass of a material artefact thereby ensuring longterm stability. The motivation for the development of the SI was the diversity of units that had sprung up within the CGS systems and the lack of coordination between the various disciplines that used them. The CGPM which was established by the Metre Convention of 1875 brought together many international organisations to not only agree on the definitions and standards of the new system but also agree on the rules for writing and presenting measurements in a standardised manner around the world. The International System of Units has been adopted by most developed countries however the adoption has not been universal in all Englishspeaking countries. While metrication in the United States is consistent in science medicine government and various fields of technology and engineering common measurements are mostly performed in United States customary units although these have officially been defined in terms of SI units. The United Kingdom has officially adopted a policy of partial metrication. Canada has adopted the SI for most governmental medical and scientific purposes and for such varied uses as grocery weights weather reports traffic signs and gasoline sales but imperial units are still legally permitted and remain in common use throughout many sectors of Canadian society particularly in the building trade and the railway sector.  Official BIPM Bureau International des Poids et Mesures SI maintenance agency home page BIPM brochure SI reference ISO 8000012009 Quantities and units Part 1 General NIST Official Publications NIST Special Publication 330 2008 Edition The International System of Units SI NIST Special Publication 811 2008 Edition Guide for the Use of the International System of Units NIST Special Pub 814 Interpretation of the SI for the United States and Federal Government Metric Conversion Policy Rules for SAE Use of SI Metric Units International System of Units at DMOZ EngNet Metric Conversion Chart Online Categorised Metric Conversion Calculator U.S. Metric Association. 2008. A Practical Guide to the International System of Units History LaTeX SIunits package manual gives a historical background to the SI system. Research The metrological triangle Recommendation of ICWM 1 CI2005 Prometric advocacy groups The UK Metric Association The US Metric Association Procustomary measures pressure groups Procustomary measures groups at DMOZ"}, {"topic": "Scalar (mathematics)", "content": "A scalar is an element of a field which is used to define a vector space. Scalars in physics are usually real numbers or any quantity that can be measured using a single real number such as temperature length and mass and is usually said to have magnitude but no direction. A quantity described by multiple scalars such as having both direction and magnitude is called a vector. In linear algebra real numbers or other elements of a field are called scalars and relate to vectors in a vector space through the operation of scalar multiplication in which a vector can be multiplied by a number to produce another vector. More generally a vector space may be defined by using any field instead of real numbers such as complex numbers. Then the scalars of that vector space will be the elements of the associated field. A scalar product operation not to be confused with scalar multiplication may be defined on a vector space allowing two vectors to be multiplied to produce a scalar. A vector space equipped with a scalar product is called an inner product space. The real component of a quaternion is also called its scalar part. The term is also sometimes used informally to mean a vector matrix tensor or other usually compound value that is actually reduced to a single component. Thus for example the product of a 1n matrix and an n1 matrix which is formally a 11 matrix is often said to be a scalar. The term scalar matrix is used to denote a matrix of the form kI where k is a scalar and I is the identity matrix.  Hazewinkel Michiel ed. 2001 Scalar Encyclopedia of Mathematics Springer ISBN 9781556080104 Weisstein Eric W. Scalar MathWorld. Mathwords.com Scalar"}, {"topic": "Scalar (physics)", "content": "A scalar in physics is a physical quantity that can be described by a single element of a number field such as a real number often accompanied by units of measurement. A scalar is usually said to be a physical quantity that only has magnitude and no other characteristics. This is in contrast to vectors tensors etc. which are described by several numbers that characterize their magnitude direction and so on. A vector is usually said to be a physical quantity that has magnitude and direction. Formally a scalar is unchanged by coordinate system rotations or reflections in Newtonian mechanics or by Lorentz transformations or spacetime translations in relativity. A related concept is a pseudoscalar which is invariant under proper rotations but like a pseudovector flips sign under improper rotations. The concept of a scalar in physics is essentially the same as in mathematics. A physical scalar field is one type of more general fields like vector fields spinor fields and tensor fields. An example of a scalar quantity is temperature the temperature at a given point is a single number. Velocity on the other hand is a vector quantity velocity in threedimensional space is specified by three values in a Cartesian coordinate system the values are the speeds relative to each coordinate axis. The associated fields describe the temperature and velocity in each point of some space. Considering the norms of the velocity vectors results in a scalar field of the speeds in each point of the space.  Arfken George 1985. Mathematical Methods for Physicists third ed.. Academic press. ISBN 0120598205. Feynman Richard P. Leighton Robert B. Sands Matthew 2006. The Feynman Lectures on Physics 1. ISBN 0805390456."}, {"topic": "Scattering", "content": "Scattering is a general physical process where some forms of radiation such as light sound or moving particles are forced to deviate from a straight trajectory by one or more paths due to localized nonuniformities in the medium through which they pass. In conventional use this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections that undergo scattering are often called diffuse reflections and unscattered reflections are called specular mirrorlike reflections. Scattering may also refer to particleparticle collisions between molecules atoms electrons photons and other particles. Examples are cosmic rays scattering by the Earths upper atmosphere particle collisions inside particle accelerators electron scattering by gas atoms in fluorescent lamps and neutron scattering inside nuclear reactors. The types of nonuniformities which can cause scattering sometimes known as scatterers or scattering centers are too numerous to list but a small sample includes particles bubbles droplets density fluctuations in fluids crystallites in polycrystalline solids defects in monocrystalline solids surface roughness cells in organisms and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory. Some areas where scattering and scattering theory are significant include radar sensing medical ultrasound semiconductor wafer inspection polymerization process monitoring acoustic tiling freespace communications and computergenerated imagery. Particleparticle scattering theory is important in areas such as particle physics atomic molecular and optical physics nuclear physics and astrophysics.  Research group on light scattering and diffusion in complex systems Multiple light scattering from a photonic science point of view Neutron Scattering Web World directory of neutron scattering instruments"}, {"topic": "Science", "content": "Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe. Contemporary science is typically subdivided into the natural sciences which study the material universe the social sciences which study people and societies and the formal sciences such as mathematics. The formal sciences are often excluded as they do not depend on empirical observations. Disciplines which use science like engineering and medicine may also be considered to be applied sciences. During the Middle Ages in the Middle East foundations for the scientific method were laid by Alhazen in his Book of Optics. From classical antiquity through the 19th century science as a type of knowledge was more closely linked to philosophy than it is now and in fact in the Western world the term natural philosophy encompassed fields of study that are today associated with science such as astronomy medicine and physics. While the classification of the material world by the ancient Indians and Greeks into air earth fire and water was more philosophical medieval Middle Eastern scientists used practical experimental observation to classify materials. In the 17th and 18th centuries scientists increasingly sought to formulate knowledge in terms of laws of nature. Over the course of the 19th century the word science became increasingly associated with the scientific method itself as a disciplined way to study the natural world. It was in the 19th century that scientific disciplines such as biology chemistry and physics reached their modern shapes. The same time period also included the origin of the terms scientist and scientific community the founding of scientific institutions and increasing significance of the interactions with society and other aspects of culture. Today science is counted among the ten function systems of modern societies.  Euroscience ESOF Euroscience Open Forum. Archived from the original on June 10 2010. Science Development in the Latin American docta Classification of the Sciences in Dictionary of the History of Ideas. Dictionarys new electronic format is badly botched entries after Design are inaccessible. Internet Archive old version. Nature of Science University of California Museum of Paleontology United States Science Initiative Selected science information provided by US Government agencies including research  development results How science works University of California Museum of Paleontology"}, {"topic": "Screw (simple machine)", "content": "A screw is a mechanism that converts rotational motion to linear motion and a torque rotational force to a linear force. It is one of the six classical simple machines. The most common form consists of a cylindrical shaft with helical grooves or ridges called threads around the outside. The screw passes through a hole in another object or medium with threads on the inside of the hole that mesh with the screws threads. When the shaft of the screw is rotated relative to the stationary threads the screw moves along its axis relative to the medium surrounding it for example rotating a wood screw forces it into wood. In screw mechanisms either the screw shaft can rotate through a threaded hole in a stationary object or a threaded collar such as a nut can rotate around a stationary screw shaft. Geometrically a screw can be viewed as a narrow inclined plane wrapped around a cylinder. Like the other simple machines a screw can amplify force a small rotational force torque on the shaft can exert a large axial force on a load. The smaller the pitch the distance between the screws threads the greater the mechanical advantage the ratio of output to input force. Screws are widely used in threaded fasteners to hold objects together and in devices such as screw tops for containers vises screw jacks and screw presses. Other mechanisms that use the same principle also called screws dont necessarily have a shaft or threads. For example a corkscrew is a helixshaped rod with a sharp point and an Archimedes screw is a water pump that uses a rotating helical chamber to move water uphill. The common principle of all screws is that a rotating helix can cause linear motion. "}, {"topic": "Shadow matter", "content": "In physics mirror matter also called shadow matter or Alice matter is a hypothetical counterpart to ordinary matter. Modern physics deals with three basic types of spatial symmetry reflection rotation and translation. The known elementary particles respect rotation and translation symmetry but do not respect mirror reflection symmetry also called Psymmetry or parity. Of the four fundamental interactionselectromagnetism the strong interaction the weak interaction and gravityonly the weak interaction breaks parity. Parity violation in weak interactions was first postulated by Tsung Dao Lee and Chen Ning Yang in 1956 as a solution to the  puzzle. They suggested a number of experiments to test if the weak interaction is invariant under parity. These experiments were performed half a year later and they confirmed that the weak interactions of the known particles violate parity. However parity symmetry can be restored as a fundamental symmetry of nature if the particle content is enlarged so that every particle has a mirror partner. The theory in its modern form was described in 1991 although the basic idea dates back further. Mirror particles interact amongst themselves in the same way as ordinary particles except where ordinary particles have lefthanded interactions mirror particles have righthanded interactions. In this way it turns out that mirror reflection symmetry can exist as an exact symmetry of nature provided that a mirror particle exists for every ordinary particle. Parity can also be spontaneously broken depending on the Higgs potential. While in the case of unbroken parity symmetry the masses of particles are the same as their mirror partners in case of broken parity symmetry the mirror partners are lighter or heavier. Mirror matter if it exists would need to interact weakly with ordinary matter. This is because the forces between mirror particles are mediated by mirror bosons. With the exception of the graviton none of the known bosons can be identical to their mirror partners. The only way mirror matter can interact with ordinary matter via forces other than gravity is via kinetic mixing of mirror bosons with ordinary bosons or via the exchange of Holdom particles. These interactions can only be very weak. Mirror particles have therefore been suggested as candidates for the inferred dark matter in the universe. In another context mirror matter has been proposed to give rise to an effective Higgs mechanism responsible for the electroweak symmetry breaking. In such a scenario mirror fermions have masses on the order of 1 TeV since they interact with an additional interaction while some of the mirror bosons are identical to the ordinary gauge bosons. In order to emphasize the distinction of this model from the ones above these mirror particles are usually called katoptrons.  A collection of scientific articles on various aspects of mirror matter theory Mirror matter article on h2g2 R. Foot. Mirror matter type dark matter. arXivastroph0407623. L.B. Okun. Mirror particles and mirror matter 50 years of speculation and search. arXivhepph0606202. Z.K. Silagadze. TeV scale gravity mirror universe and ... dinosaurs. arXivhepph0002255."}, {"topic": "Shear strength", "content": "In engineering shear strength is the strength of a material or component against the type of yield or structural failure where the material or component fails in shear. A shear load is a force that tends to produce a sliding failure on a material along a plane that is parallel to the direction of the force. When a paper is cut with scissors the paper fails in shear. In structural and mechanical engineering the shear strength of a component is important for designing the dimensions and materials to be used for the manufacture or construction of the component e.g. beams plates or bolts. In a reinforced concrete beam the main purpose of reinforcing bar rebar stirrups is to increase the shear strength. For shear stress  "}, {"topic": "Shear stress", "content": "A shear stress denoted Greek tau is defined as the component of stress coplanar with a material cross section. Shear stress arises from the force vector component parallel to the cross section. Normal stress on the other hand arises from the force vector component perpendicular to the material cross section on which it acts. "}, {"topic": "Shortwave radiation", "content": "Shortwave radiation SW is radiant energy with wavelengths in the visible VIS nearultraviolet UV and nearinfrared NIR spectra. There is no standard cutoff for the nearinfrared range therefore the shortwave radiation range is also variously defined. It may be broadly defined to include all radiation with a wavelength between 0.1m and 5.0m or narrowly defined so as to include only radiation between 0.2m and 3.0m. There is little radiation flux in terms of Wm to the Earths surface below 0.2m or above 3.0m although photon flux remains significant as far as 6.0m compared to shorter wavelength fluxes. UVC radiation spans from 0.1m to .28m UVB from 0.28m to 0.315m UVA from 0.315m to 0.4m the visible spectrum from 0.4m to 0.7m and NIR arguably from 0.7m to 5.0m beyond which the infrared is thermal. Shortwave radiation is distinguished from longwave radiation. Downward shortwave radiation is sensitive to solar zenith angle cloud cover and surface albedo.  Zhang Y. W. B. Rossow A. A. Lacis V. Oinas and M. I. Mischenko 2004. Calculation of radiative fluxes from the surface to top of atmosphere based on ISCCP and other global data sets Refinements of the radiative transfer model and the input data. Journal of Geophysical ResearchAtmospheres 109D19105. L. Chen G. Yan T. Wang H. Ren J. Calb J. Zhao R. McKenzie 2012 Estimation of surface shortwave radiation components under all sky conditions Modeling and sensitivity analysis Remote Sensing of Environment 123 457469."}, {"topic": "Shrodinger equation", "content": "In quantum mechanics the Schrdinger equation is a partial differential equation that describes how the quantum state of a quantum system changes with time. It was formulated in late 1925 and published in 1926 by the Austrian physicist Erwin Schrdinger. In classical mechanics Newtons second law F  ma is used to make a mathematical prediction as to what path a given system will take following a set of known initial conditions. In quantum mechanics the analogue of Newtons law is Schrdingers equation for a quantum system usually atoms molecules and subatomic particles whether free bound or localised. It is not a simple algebraic equation but in general a linear partial differential equation describing the timeevolution of the systems wave function also called a state function. The concept of a wavefunction is a fundamental postulate of quantum mechanics. Although Schrdingers equation is often presented as a separate postulate some authors show that some properties resulting from Schrdingers equation may be deduced just from symmetry principles alone for example the commutation relations. Generally derivations of the Schrdinger equation demonstrate its mathematical plausibility for describing waveparticle duality but to date there are no universally accepted derivations of Schrdingers equation from appropriate axioms. In the Copenhagen interpretation of quantum mechanics the wave function is the most complete description that can be given of a physical system. Solutions to Schrdingers equation describe not only molecular atomic and subatomic systems but also macroscopic systems possibly even the whole universe. The Schrdinger equation is consistent with classical mechanics but not with special relativity. Making quantum mechanics consistent with special relativity is one of the great achievements of Quantum Field Theory and requires the introduction of the creation and annihilation operators so that the number of particles can no longer be considered constant. The Schrdinger equation is not the only way to make predictions in quantum mechanicsother formulations can be used such as Werner Heisenbergs matrix mechanics and Richard Feynmans path integral formulation.  Hazewinkel Michiel ed. 2001 Schrdinger equation Encyclopedia of Mathematics Springer ISBN 9781556080104 Quantum Physics textbook by Benjamin Crowell with a treatment of the timeindependent Schrdinger equation Linear Schrdinger Equation at EqWorld The World of Mathematical Equations. Nonlinear Schrdinger Equation at EqWorld The World of Mathematical Equations. The Schrdinger Equation in One Dimension as well as the directory of the book. All about 3D Schrdinger Equation Mathematical aspects of Schrdinger equations are discussed on the Dispersive PDE Wiki. WebSchrdinger Interactive solution of the 2D timedependent and stationary Schrdinger equation An alternate reasoning behind the Schrdinger Equation Online softwarePeriodic Potential Lab Solves the timeindependent Schrdinger equation for arbitrary periodic potentials. What Do You Do With a Wavefunction The Young DoubleSlit Experiment"}, {"topic": "Simple harmonic motion", "content": "In mechanics and physics simple harmonic motion is a type of periodic motion or oscillation motion where the restoring force is directly proportional to the displacement and acts in the direction opposite to that of displacement. Simple harmonic motion can serve as a mathematical model for a variety of motions such as the oscillation of a spring. In addition other phenomena can be approximated by simple harmonic motion including the motion of a simple pendulum as well as molecular vibration. Simple harmonic motion is typified by the motion of a mass on a spring when it is subject to the linear elastic restoring force given by Hookes Law. The motion is sinusoidal in time and demonstrates a single resonant frequency. For simple harmonic motion to be an accurate model for a pendulum the net force on the object at the end of the pendulum must be proportional to the displacement. This will be a good approximation when the angle of swing is small. Simple harmonic motion provides a basis for the characterization of more complicated motions through the techniques of Fourier analysis.  Simple Harmonic Motion from HyperPhysics Java simulation of springmass oscillator"}, {"topic": "Simple machine", "content": "A simple machine is a mechanical device that changes the direction or magnitude of a force. In general they can be defined as the simplest mechanisms that use mechanical advantage also called leverage to multiply force. Usually the term refers to the six classical simple machines which were defined by Renaissance scientists Lever Wheel and axle Pulley Inclined plane Wedge Screw A simple machine uses a single applied force to do work against a single load force. Ignoring friction losses the work done on the load is equal to the work done by the applied force. The machine can increase the amount of the output force at the cost of a proportional decrease in the distance moved by the load. The ratio of the output to the applied force is called the mechanical advantage. Simple machines can be regarded as the elementary building blocks of which all more complicated machines sometimes called compound machines are composed. For example wheels levers and pulleys are all used in the mechanism of a bicycle. The mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed. Although they continue to be of great importance in mechanics and applied science modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed which arose in the Renaissance as a neoclassical amplification of ancient Greek texts on technology. The great variety and sophistication of modern machine linkages which arose during the Industrial Revolution is inadequately described by these six simple categories. As a result various postRenaissance authors have compiled expanded lists of simple machines often using terms like basic machines compound machines or machine elements to distinguish them from the classical simple machines above. By the late 1800s Franz Reuleaux had identified hundreds of machine elements calling them simple machines. Models of these devices may be found at Cornell Universitys Kinematic Models for Design KMODDL website. "}, {"topic": "Siphon", "content": "The word siphon from Ancient Greek pipe tube also called syphon is used to refer to a wide variety of devices that involve the flow of liquids through tubes. But in a narrower sense the word refers particularly to a tube in an inverted U shape which causes a liquid to flow upward above the surface of a reservoir with no pump but powered by the fall of the liquid as it flows down the tube under the pull of gravity then discharging at a level lower than the surface of the reservoir it came from. There are two leading theories about how siphons cause liquid to flow uphill against gravity without being pumped and powered only by gravity. The traditional theory for centuries was that gravity pulling the liquid down on the exit side of the siphon resulted in reduced pressure at the top of the siphon. Then atmospheric pressure was able to push the liquid from the upper reservoir up into the reduced pressure at the top of the siphon like in a barometer or drinking straw and then over. However it has been demonstrated that siphons can operate in a vacuum and to heights exceeding the barometric height of the liquid. Consequently the cohesion tension theory of siphon operation has been advocated where the liquid is pulled over the siphon in a way similar to the chain model. It need not be one theory or the other that is correct but rather both theories may be correct in different circumstances of ambient pressure. The atmospheric pressure with gravity theory obviously cannot explain siphons in vacuum where there is no significant atmospheric pressure. But the cohesion tension with gravity theory cannot explain CO2 gas siphons siphons working despite bubbles and the flying droplet siphon where gases do not exert significant pulling forces and liquids not in contact cannot exert a cohesive tension force. All known published theories in modern times recognize Bernoullis equation as a decent approximation to idealized frictionfree siphon operation.  A siphon in a vacuum. The Periodic Table of Videos. University of Nottingham."}, {"topic": "Snell's law", "content": "Snells law also known as the SnellDescartes law and the law of refraction is a formula used to describe the relationship between the angles of incidence and refraction when referring to light or other waves passing through a boundary between two different isotropic media such as water glass or air. In optics the law is used in ray tracing to compute the angles of incidence or refraction and in experimental optics to find the refractive index of a material. The law is also satisfied in metamaterials which allow light to be bent backward at a negative angle of refraction with a negative refractive index. Although named after Dutch astronomer Willebrord Snellius 15801626 the law was first accurately described by the scientist Ibn Sahl at the Baghdad court in 984. In the manuscript On Burning Mirrors and Lenses Sahl used the law to derive lens shapes that focus light with no geometric aberrations. Snells law states that the ratio of the sines of the angles of incidence and refraction is equivalent to the ratio of phase velocities in the two media or equivalent to the reciprocal of the ratio of the indices of refraction sin 1 sin 2  v 1 v 2  1 2  n 2 n 1   Discovery of the law of refraction Snells Law of Refraction Wave Fronts by Todd Rowland Wolfram Demonstrations Project."}, {"topic": "Soft condensed matter", "content": "Soft matter is a subfield of condensed matter comprising a variety of physical systems that are deformed or structurally altered by thermal or mechanical stress of the magnitude of thermal fluctuations. They include liquids colloids polymers foams gels granular materials liquid crystals and a number of biological materials. These materials share an important common feature in that predominant physical behaviors occur at an energy scale comparable with room temperature thermal energy. At these temperatures quantum aspects are generally unimportant. PierreGilles de Gennes who has been called the founding father of soft matter received the Nobel Prize in physics in 1991 for discovering that methods developed for studying order phenomena in simple systems can be generalized to the more complex cases found in soft matter in particular to the behaviors of liquid crystals and polymers. He is especially noted for inventing the concept of reptation.  PierreGilles de Gennes Nobel Lecture American Physical Society Topical Group on Soft Matter GSOFT Softmatterworld.org Softmatterresources.com SklogWiki  a wiki dedicated to simple liquids complex fluids and soft condensed matter. Harvard School of Engineering and Applied Sciences Soft Matter Wiki  organizes reviews and summarizes academic papers on soft matter. Soft Matter Engineering  A group dedicated to Soft Matter Engineering at the University of Florida Google Scholar page on soft matter"}, {"topic": "Soil physics", "content": "Soil physics is the study of soil physical properties and processes. It is applied to management and prediction under natural and managed ecosystems. Soil physics deals with the dynamics of physical soil components and their phases as solid liquids and gases. It draws on the principles of physics physical chemistry engineering and meteorology. It is especially important in this day and age because most farmers require an understanding of agroecosystems. Soil physics applies these principles to address practical problems of agriculture ecology and engineering.  SSSA Soil Physics Division"}, {"topic": "Solar cell", "content": "See also Photovoltaics A solar cell or photovoltaic cell in very early days also termed solar battery a denotation which nowadays has a totally different meaning see here is an electrical device that converts the energy of light directly into electricity by the photovoltaic effect which is a physical and chemical phenomenon. It is a form of photoelectric cell defined as a device whose electrical characteristics such as current voltage or resistance vary when exposed to light. Solar cells are the building blocks of photovoltaic modules otherwise known as solar panels. Solar cells are described as being photovoltaic irrespective of whether the source is sunlight or an artificial light. They are used as a photodetector for example infrared detectors detecting light or other electromagnetic radiation near the visible range or measuring light intensity. The operation of a photovoltaic PV cell requires 3 basic attributes The absorption of light generating either electronhole pairs or excitons. The separation of charge carriers of opposite types. The separate extraction of those carriers to an external circuit. In contrast a solar thermal collector supplies heat by absorbing sunlight for the purpose of either direct heating or indirect electrical power generation from heat. A photoelectrolytic cell photoelectrochemical cell on the other hand refers either to a type of photovoltaic cell like that developed by Edmond Becquerel and modern dyesensitized solar cells or to a device that splits water directly into hydrogen and oxygen using only solar illumination.  PV Lighthouse Calculators and Resources for photovoltaic scientists and engineers Photovoltaics CDROM online Solar cell manufacturing techniques Renewable Energy Solar at DMOZ Solar Energy Laboratory at University of Southampton NASAs Photovoltaic Info Green M. A. Emery K. Hishikawa Y. Warta W. 2010. Solar cell efficiency tables version 36. Progress in Photovoltaics Research and Applications 18 5 346. doi10.1002pip.1021. Electric Energy From Sun Produced by Light Cell Popular Mechanics July 1931 article on various 1930s research on solar cells Advantages and disadvantages different solar cells types"}, {"topic": "Solar physics", "content": "Solar physics is the branch of astrophysics that specializes in the study of the Sun. It deals with detailed measurements that are possible only for our closest star. It intersects with many disciplines of pure physics astrophysics and computer science including fluid dynamics plasma physics including magnetohydrodynamics seismology particle physics atomic physics nuclear physics stellar evolution space physics spectroscopy radiative transfer applied optics signal processing computer vision computational physics stellar physics and solar astronomy. Because the Sun is uniquely situated for closerange observing other stars cannot be resolved with anything like the spatial or temporal resolution that the Sun can there is a split between the related discipline of observational astrophysics of distant stars and observational solar physics. The study of solar physics is also important as it is believed that changes in the solar atmosphere and solar activity can have a major impact on Earths climate. The Sun also provides a physical laboratory for the study of plasma physics. "}, {"topic": "Solid-state physics", "content": "Solidstate physics is the study of rigid matter or solids through methods such as quantum mechanics crystallography electromagnetism and metallurgy. It is the largest branch of condensed matter physics. Solidstate physics studies how the largescale properties of solid materials result from their atomicscale properties. Thus solidstate physics forms a theoretical basis of materials science. It also has direct applications for example in the technology of transistors and semiconductors. "}, {"topic": "Solid mechanics", "content": "Solid mechanics is the branch of continuum mechanics that studies the behavior of solid materials especially their motion and deformation under the action of forces temperature changes phase changes and other external or internal agents. Solid mechanics is fundamental for civil aerospace nuclear and mechanical engineering for geology and for many branches of physics such as materials science. It has specific applications in many other areas such as understanding the anatomy of living beings and the design of dental prostheses and surgical implants. One of the most common practical applications of solid mechanics is the EulerBernoulli beam equation. Solid mechanics extensively uses tensors to describe stresses strains and the relationship between them. "}, {"topic": "Solubility", "content": "Solubility is the property of a solid liquid or gaseous chemical substance called solute to dissolve in a solid liquid or gaseous solvent. The solubility of a substance fundamentally depends on the physical and chemical properties of the solute and solvent as well as on temperature pressure and the pH of the solution. The extent of the solubility of a substance in a specific solvent is measured as the saturation concentration where adding more solute does not increase the concentration of the solution and begins to precipitate the excess amount of solute. The solubility of a substance is an entirely different property from the rate of solution which is how fast it dissolves. Most often the solvent is a liquid which can be a pure substance or a mixture. One may also speak of solid solution but rarely of solution in a gas see vaporliquid equilibrium instead. The extent of solubility ranges widely from infinitely soluble without limit fully miscible such as ethanol in water to poorly soluble such as silver chloride in water. The term insoluble is often applied to poorly or very poorly soluble compounds. A common threshold to describe something as insoluble is less than 0.1 g per 100 mL of solvent. Under certain conditions the equilibrium solubility can be exceeded to give a socalled supersaturated solution which is metastable. Metastability of crystals can also lead to apparent differences in the amount of a chemical that dissolves depending on its crystalline form or particle size. A supersaturated solution generally crystallises when seed crystals are introduced and rapid equilibration occurs. Phenylsalicylate is one such simple observable substance when fully melted and then cooled below its fusion point. Solubility is not to be confused with the ability to dissolve a substance because the solution might also occur because of a chemical reaction. For example zinc dissolves with effervescence in hydrochloric acid as a result of a chemical reaction releasing hydrogen gas in a displacement reaction. The zinc ions are soluble in the acid. The smaller a particle is the faster it dissolves although there are many factors to add to this generalization. Crucially solubility applies to all areas of chemistry geochemistry inorganic physical organic and biochemistry. In all cases it will depend on the physical conditions temperature pressure and concentration and the enthalpy and entropy directly relating to the solvents and solutes concerned. By far the most common solvent in chemistry is water which is a solvent for most ionic compounds as well as a wide range of organic substances. This is a crucial factor in acidityalkalinity and much environmental and geochemical work.  VCClab.org ALOGPS free interactive calculation of aqueous solubility of compounds at Virtual Computational Chemistry Laboratory using several algorithms. ACDlabs.com ACDSolubility DB aqueous solubility prediction Simulationsplus.com SSw an aqueous solubility prediction model. Techniques For Solubility Enhancement Of Poorly Soluble Drugs An Overview from Journal of Medical Pharmaceutical and Allied Sciences 2012 01 122 When Poor Solubility Becomes an Issue From Early Stage to Proof of Concept from European Journal of Pharmaceutical Sciences 31 2007 pp. 249261"}, {"topic": "Sound", "content": "In physics sound is a vibration that propagates as a typically audible mechanical wave of pressure and displacement through a medium such as air or water. In physiology and psychology sound is the reception of such waves and their perception by the brain.  Sounds Amazing a KS34 learning resource for sound and waves HyperPhysics Sound and Hearing Introduction to the Physics of Sound Hearing curves and online hearing test Audio for the 21st Century Conversion of sound units and levels Sound calculations Audio Check a free collection of audio tests and test tones playable online More Sounds Amazing a sixthform learning resource about sound waves"}, {"topic": "Space physics", "content": "Space physics is the study of plasmas as they occur naturally in the Earths upper atmosphere. As such it encompasses a farranging number of topics such as heliophysics which includes the solar physics of the sun the solar wind planetary magnetospheres and ionospheres auroras cosmic rays and synchrotron radiation. Space physics is a fundamental part of the study of space weather and has important implications not only to understanding the universe but also to practical everyday life including the operation of communications and weather satellites. Space physics is distinct from other fields of astrophysics which study similar phenomena in that space physics utilizes in situ measurements from high altitude rockets and spacecraft.  Kallenrode MayBritt 2004. Space Physics An Introduction to Plasmas and Particles in the Heliosphere and Magnetospheres. Springer. ISBN 3540206175. Gombosi Tamas 1998. Physics of the Space Environment. New York Cambridge University Press. ISBN 052159264X."}, {"topic": "Special relativity", "content": "In physics special relativity SR also known as the special theory of relativity or STR is the generally accepted and experimentally well confirmed physical theory regarding the relationship between space and time. In Albert Einsteins original pedagogical treatment it is based on two postulates The laws of physics are invariant i.e. identical in all inertial systems nonaccelerating frames of reference. The speed of light in a vacuum is the same for all observers regardless of the motion of the light source. It was originally proposed in 1905 by Albert Einstein in the paper On the Electrodynamics of Moving Bodies. The inconsistency of Newtonian mechanics with Maxwells equations of electromagnetism and the lack of experimental confirmation for a hypothesized luminiferous aether led to the development of special relativity which corrects mechanics to handle situations involving motions nearing the speed of light. As of today special relativity is the most accurate model of motion at any speed. Even so the Newtonian mechanics model is still useful due to its simplicity and high accuracy as an approximation at small velocities relative to the speed of light. Special relativity implies a wide range of consequences which have been experimentally verified including length contraction time dilation relativistic mass massenergy equivalence a universal speed limit and relativity of simultaneity. It has replaced the conventional notion of an absolute universal time with the notion of a time that is dependent on reference frame and spatial position. Rather than an invariant time interval between two events there is an invariant spacetime interval. Combined with other laws of physics the two postulates of special relativity predict the equivalence of mass and energy as expressed in the massenergy equivalence formula E  mc2 where c is the speed of light in a vacuum. A defining feature of special relativity is the replacement of the Galilean transformations of Newtonian mechanics with the Lorentz transformations. Time and space cannot be defined separately from each other. Rather space and time are interwoven into a single continuum known as spacetime. Events that occur at the same time for one observer can occur at different times for another. The theory is special in that it only applies in the special case where the curvature of spacetime due to gravity is negligible. In order to include gravity Einstein formulated general relativity in 1915. Special relativity contrary to some outdated descriptions is capable of handling accelerated frames of reference. As Galilean relativity is now considered an approximation of special relativity that is valid for low speeds special relativity is considered an approximation of general relativity that is valid for weak gravitational fields i.e. at a sufficiently small scale and in conditions of free fall. Whereas general relativity incorporates noneuclidean geometry in order to represent gravitational effects as the geometric curvature of spacetime special relativity is restricted to the flat spacetime known as Minkowski space. A locally Lorentzinvariant frame that abides by special relativity can be defined at sufficiently small scales even in curved spacetime. Galileo Galilei had already postulated that there is no absolute and welldefined state of rest no privileged reference frames a principle now called Galileos principle of relativity. Einstein extended this principle so that it accounted for the constant speed of light a phenomenon that had been recently observed in the MichelsonMorley experiment. He also postulated that it holds for all the laws of physics including both the laws of mechanics and of electrodynamics.  Raytracing Special Relativity Software visualizing several scenarios under the influence of special relativity. Real Time Relativity The Australian National University. Relativistic visual effects experienced through an interactive program. Spacetime travel A variety of visualizations of relativistic effects from relativistic motion to black holes. Through Einsteins Eyes The Australian National University. Relativistic visual effects explained with movies and images. Warp Special Relativity Simulator A computer program to show the effects of traveling close to the speed of light. Animation clip on YouTube visualizing the Lorentz transformation. Original interactive FLASH Animations from John de Pillis illustrating Lorentz and Galilean frames Train and Tunnel Paradox the Twin Paradox Wave Propagation Clock Synchronization etc. lightspeed An OpenGLbased program developed to illustrate the effects of special relativity on the appearance of moving objects. Animation showing the stars near Earth as seen from a spacecraft accelerating rapidly to light speed."}, {"topic": "Speed", "content": "In everyday use and in kinematics the speed of an object is the magnitude of its velocity the rate of change of its position it is thus a scalar quantity. The average speed of an object in an interval of time is the distance travelled by the object divided by the duration of the interval the instantaneous speed is the limit of the average speed as the duration of the time interval approaches zero. Speed has the dimensions of distance divided by time. The SI unit of speed is the metre per second but the most common unit of speed in everyday usage is the kilometre per hour or in the US and the UK miles per hour. For air and marine travel the knot is commonly used. The fastest possible speed at which energy or information can travel according to special relativity is the speed of light in a vacuum c  7008299792458000000299792458 metres per second approximately 70082997222222222221079000000 kmh or 7008299963840000000671000000 mph. Matter cannot quite reach the speed of light as this would require an infinite amount of energy. In relativity physics the concept of rapidity replaces the classical idea of speed.  Richard P. Feynman Robert B. Leighton Matthew Sands. The Feynman Lectures on Physics Volume I Section 82. AddisonWesley Reading Massachusetts 1963. ISBN 0201021161."}, {"topic": "Speed of light", "content": "The speed of light in vacuum commonly denoted c is a universal physical constant important in many areas of physics. Its precise value is 7008299792458000000299792458 metres per second approximately 70083000000000000003.00108 ms since the length of the metre is defined from this constant and the international standard for time. According to special relativity c is the maximum speed at which all matter and hence information in the universe can travel. It is the speed at which all massless particles and changes of the associated fields including electromagnetic radiation such as light and gravitational waves travel in vacuum. Such particles and waves travel at c regardless of the motion of the source or the inertial reference frame of the observer. In the theory of relativity c interrelates space and time and also appears in the famous equation of massenergy equivalence E  mc2. The speed at which light propagates through transparent materials such as glass or air is less than c similarly the speed of radio waves in wire cables is slower than c. The ratio between c and the speed v at which light travels in a material is called the refractive index n of the material n  c  v. For example for visible light the refractive index of glass is typically around 1.5 meaning that light in glass travels at c  1.5 7008200000000000000200000 kms the refractive index of air for visible light is about 1.0003 so the speed of light in air is about 7008299700000000000299700 kms about 700490000000000000090 kms slower than c. For many practical purposes light and other electromagnetic waves will appear to propagate instantaneously but for long distances and very sensitive measurements their finite speed has noticeable effects. In communicating with distant space probes it can take minutes to hours for a message to get from Earth to the spacecraft or vice versa. The light seen from stars left them many years ago allowing the study of the history of the universe by looking at distant objects. The finite speed of light also limits the theoretical maximum speed of computers since information must be sent within the computer from chip to chip. The speed of light can be used with time of flight measurements to measure large distances to high precision. Ole Rmer first demonstrated in 1676 that light travels at a finite speed as opposed to instantaneously by studying the apparent motion of Jupiters moon Io. In 1865 James Clerk Maxwell proposed that light was an electromagnetic wave and therefore travelled at the speed c appearing in his theory of electromagnetism. In 1905 Albert Einstein postulated that the speed of light c with respect to any inertial frame is a constant and is independent of the motion of the light source. He explored the consequences of that postulate by deriving the special theory of relativity and in doing so showed that the parameter c had relevance outside of the context of light and electromagnetism. After centuries of increasingly precise measurements in 1975 the speed of light was known to be 7008299792458000000299792458 ms with a measurement uncertainty of 4 parts per billion. In 1983 the metre was redefined in the International System of Units SI as the distance travelled by light in vacuum in 17008299792458000000299792458 of a second. As a result the numerical value of c in metres per second is now fixed exactly by the definition of the metre.  Test Light Speed in Mile Long Vacuum Tube. Popular Science Monthly September 1930 p. 1718. Definition of the metre International Bureau of Weights and Measures BIPM Speed of light in vacuum National Institute of Standards and Technology NIST Data Gallery Michelson Speed of Light Univariate Location Estimation download data gathered by Albert A. Michelson Subluminal Java applet demonstrating group velocity information limits De Mora Luminis at MathPages Light discussion on adding velocities Speed of Light University of Colorado Department of Physics c Speed of Light Sixty Symbols University of Nottingham Department of Physics video Usenet Physics FAQ Speed of light illustration Speed of light as LiveCounter"}, {"topic": "Spin quantum number", "content": "In atomic physics the spin quantum number is a quantum number that parameterizes the intrinsic angular momentum or spin angular momentum or simply spin of a given particle. The spin quantum number is the fourth of a set of quantum numbers the principal quantum number the azimuthal quantum number the magnetic quantum number and the spin quantum number which completely describe the quantum state of an electron. It is designated by the letter s. It describes the energy shape and orientation of orbitals.  Full treatment of Spinincluding origins evolution of Spin Theory and details of the Spin equations"}, {"topic": "Standard Model", "content": "The Standard Model of particle physics is a theory concerning the electromagnetic weak and strong nuclear interactions as well as classifying all the subatomic particles known. It was developed throughout the latter half of the 20th century as a collaborative effort of scientists around the world. The current formulation was finalized in the mid1970s upon experimental confirmation of the existence of quarks. Since then discoveries of the top quark 1995 the tau neutrino 2000 and the Higgs boson 2012 have given further credence to the Standard Model. Because of its success in explaining a wide variety of experimental results the Standard Model is sometimes regarded as the theory of almost everything. Although the Standard Model is believed to be theoretically selfconsistent and has demonstrated huge and continued successes in providing experimental predictions it does leave some phenomena unexplained and it falls short of being a complete theory of fundamental interactions. It does not incorporate the full theory of gravitation as described by general relativity or account for the accelerating expansion of the universe as possibly described by dark energy. The model does not contain any viable dark matter particle that possesses all of the required properties deduced from observational cosmology. It also does not incorporate neutrino oscillations and their nonzero masses. The development of the Standard Model was driven by theoretical and experimental particle physicists alike. For theorists the Standard Model is a paradigm of a quantum field theory which exhibits a wide range of physics including spontaneous symmetry breaking anomalies and nonperturbative behavior. It is used as a basis for building more exotic models that incorporate hypothetical particles extra dimensions and elaborate symmetries such as supersymmetry in an attempt to explain experimental results at variance with the Standard Model such as the existence of dark matter and neutrino oscillations.  The Standard Model explained in Detail by CERNs John Ellis omega tau podcast. The Standard Model The Standard Model on the CERN web site explains how the basic building blocks of matter interact governed by four fundamental forces. Standard Model on YouTube"}, {"topic": "State of matter", "content": "In physics a state of matter is one of the distinct forms that matter takes on. Four states of matter are observable in everyday life solid liquid gas and plasma. Many other states are known to exist only in extreme situations such as BoseEinstein condensates neutrondegenerate matter and quarkgluon plasma which occur in situations of extreme cold extreme density and extremely highenergy colorcharged matter respectively. Some other states are believed to be possible but remain theoretical for now. For a complete list of all exotic states of matter see the list of states of matter. Historically the distinction is made based on qualitative differences in properties. Matter in the solid state maintains a fixed volume and shape with component particles atoms molecules or ions close together and fixed into place. Matter in the liquid state maintains a fixed volume but has a variable shape that adapts to fit its container. Its particles are still close together but move freely. Matter in the gaseous state has both variable volume and shape adapting both to fit its container. Its particles are neither close together nor fixed in place. Matter in the plasma state has variable volume and shape but as well as neutral atoms it contains a significant number of ions and electrons both of which can move around freely. Plasma is the most common form of visible matter in the universe. The term phase is sometimes used as a synonym for state of matter but a system can contain several immiscible phases of the same state of matter see Phase matter for further discussion of the difference between the two terms.  20050622 MIT News MIT physicists create new form of matter Citat ... They have become the first to create a new type of matter a gas of atoms that shows hightemperature superfluidity. 20031010 Science Daily Metallic Phase For Bosons Implies New State Of Matter 20040115 ScienceDaily Probable Discovery Of A New Supersolid Phase Of Matter Citat ...We apparently have observed for the first time a solid material with the characteristics of a superfluid...but because all its particles are in the identical quantum state it remains a solid even though its component particles are continually flowing... 20040129 ScienceDaily NISTUniversity Of Colorado Scientists Create New Form Of Matter A Fermionic Condensate Short videos demonstrating of States of Matter solids liquids and gases by Prof. J M Murrell University of Sussex"}, {"topic": "Statics", "content": "Statics is the branch of mechanics that is concerned with the analysis of loads force and torque or moment acting on physical systems that do not experience an acceleration a0 but rather are in static equilibrium with their environment. When in static equilibrium the acceleration of the system is zero and the system is either at rest or its center of mass moves at constant velocity. The application of Newtons second law to a system gives F  m a .   Online test of statics conceptual knowledge meant for teachers Free engineering Statics courseware with about 300 interactive exercises with hints and feedback  Carnegie Mellon Open Learning Initiative Statics for Robotics 1 2"}, {"topic": "Statistical mechanics", "content": "Statistical mechanics is a branch of theoretical physics that studies using probability theory the average behaviour of a mechanical system where the state of the system is uncertain. A common use of statistical mechanics is in explaining the thermodynamic behaviour of large systems. This branch of statistical mechanics which treats and extends classical thermodynamics is known as statistical thermodynamics or equilibrium statistical mechanics. Microscopic mechanical laws do not contain concepts such as temperature heat or entropy however statistical mechanics shows how these concepts arise from the natural uncertainty about the state of a system when that system is prepared in practice. The benefit of using statistical mechanics is that it provides exact methods to connect thermodynamic quantities such as heat capacity to microscopic behaviour whereas in classical thermodynamics the only available option would be to just measure and tabulate such quantities for various materials. Statistical mechanics also makes it possible to extend the laws of thermodynamics to cases which are not considered in classical thermodynamics such as microscopic systems and other mechanical systems with few degrees of freedom. Statistical mechanics also finds use outside equilibrium. An important subbranch known as nonequilibrium statistical mechanics deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions or flows of particles and heat. Unlike with equilibrium there is no exact formalism that applies to nonequilibrium statistical mechanics in general and so this branch of statistical mechanics remains an active area of theoretical research.  Philosophy of Statistical Mechanics article by Lawrence Sklar for the Stanford Encyclopedia of Philosophy. Sklogwiki  Thermodynamics statistical mechanics and the computer simulation of materials. SklogWiki is particularly orientated towards liquids and soft condensed matter. Statistical Thermodynamics  Historical Timeline Thermodynamics and Statistical Mechanics by Richard Fitzpatrick Lecture Notes in Statistical Mechanics and Mesoscopics by Doron Cohen Videos of lecture series in statistical mechanics on YouTube taught by Leonard Susskind. VuQuoc L. Configuration integral statistical mechanics 2008. this wiki site is down see this article in the web archive on 2012 April 28."}, {"topic": "Statistical physics", "content": "Statistical physics is a branch of physics that uses methods of probability theory and statistics and particularly the mathematical tools for dealing with large populations and approximations in solving physical problems. It can describe a wide variety of fields with an inherently stochastic nature. Its applications include many problems in the fields of physics biology chemistry neurology and even some social sciences such as sociology. Its main purpose is to clarify the properties of matter in aggregate in terms of physical laws governing atomic motion. In particular statistical mechanics develops the phenomenological results of thermodynamics from a probabilistic examination of the underlying microscopic systems. Historically one of the first topics in physics where statistical methods were applied was the field of mechanics which is concerned with the motion of particles or objects when subjected to a force.  Thermal and Statistical Physics lecture notes Web draft 2001 by Mallett M. Blumler P. BASICS OF STATISTICAL PHYSICS Second Edition by Harald J W MllerKirsten University of Kaiserslautern Germany Statistical physics by Kadanoff L.P. Statistical Physics  Statics Dynamics and Renormalization by Kadanoff L.P. History and outlook of statistical physics by Dieter Flamm"}, {"topic": "Statistics", "content": "Statistics is the study of the collection analysis interpretation presentation and organization of data. In applying statistics to e.g. a scientific industrial or social problem it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as all people living in a country or every atom composing a crystal. Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments. Some popular definitions are MerriamWebster dictionary defines statistics as classified facts representing the conditions of a people in a state especially the facts that can be stated in numbers or any other tabular or classified arrangement. Statistician Sir Arthur Lyon Bowley defines statistics as Numerical statements of facts in any department of inquiry placed in relation to each other. When census data cannot be collected statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study manipulating the system and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast an observational study does not involve experimental manipulation. Two main statistical methodologies are used in data analysis descriptive statistics which summarizes data from a sample using indexes such as the mean or standard deviation and inferential statistics which draws conclusions from data that are subject to random variation e.g. observational errors sampling variation. Descriptive statistics are most often concerned with two sets of properties of a distribution sample or population central tendency or location seeks to characterize the distributions central or typical value while dispersion or variability characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory which deals with the analysis of random phenomena. A standard statistical procedure involves the test of the relationship between two statistical data sets or a data set and a synthetic data drawn from idealized model. An hypothesis is proposed for the statistical relationship between the two data sets and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false given the data that are used in the test. Working from a null hypothesis two basic forms of error are recognized Type I errors null hypothesis is falsely rejected giving a false positive and Type II errors null hypothesis fails to be rejected and an actual difference between populations is missed giving a false negative. Multiple problems have come to be associated with this framework ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis. Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random noise or systematic bias but other types of errors e.g. blunder such as when an analyst reports incorrect units can also be important. The presence of missing data andor censoring may result in biased estimates and specific techniques have been developed to address these problems. Statistics can be said to have begun in ancient civilization going back at least to the 5th century BC but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research for example on the problem of how to analyze Big data. "}, {"topic": "Stellar physics", "content": "Stellar physics is a term coined for the research concerning the formation evolution interior and the atmospheres of stars. The understanding of the birth and death of stars requires the application of almost all branches of modern physics. These areas include gravitation hydrodynamics atomic physics liquidsolid state theory superconductivity and superfluidity. This distinguishes it from stellar dynamics which concerns mainly gravitational interactions between stars. "}, {"topic": "Stiffness", "content": "Stiffness is the rigidity of an object the extent to which it resists deformation in response to an applied force. The complementary concept is flexibility or pliability the more flexible an object is the less stiff it is. "}, {"topic": "Strength of materials", "content": "Strength of materials also called mechanics of materials is a subject which deals with the behavior of solid objects subject to stresses and strains. The complete theory began with the consideration of the behavior of one and two dimensional members of structures whose states of stress can be approximated as two dimensional and was then generalized to three dimensions to develop a more complete theory of the elastic and plastic behavior of materials. An important founding pioneer in mechanics of materials was Stephen Timoshenko. The study of strength of materials often refers to various methods of calculating the stresses and strains in structural members such as beams columns and shafts. The methods employed to predict the response of a structure under loading and its susceptibility to various failure modes takes into account the properties of the materials such as its yield strength ultimate strength Youngs modulus and Poissons ratio in addition the mechanical elements macroscopic properties geometric properties such as its length width thickness boundary constraints and abrupt changes in geometry such as holes are considered.  Failure theories case studies in structural failure"}, {"topic": "Stress-strain curve", "content": "The relationship between the stress and strain that a particular material displays is known as that particular materials stressstrain curve. It is unique for each material and is found by recording the amount of deformation strain at distinct intervals of tensile or compressive loading stress. These curves reveal many of the properties of a material including data to establish the Modulus of Elasticity E. Stressstrain curves of various materials vary widely and different tensile tests conducted on the same material yield different results depending upon the temperature of the specimen and the speed of the loading. It is possible however to distinguish some common characteristics among the stressstrain curves of various groups of materials and on this basis to divide materials into two broad categories namely the ductile materials and the brittle materials. Consider a bar of cross sectional area A being subjected to equal and opposite forces F pulling at the ends so the bar is under tension. The material is experiencing a stress defined to be the ratio of the force to the cross sectional area of the bar s t r e s s  F A  Finally the shear modulus MS of a material is defined as the ratio of shear stress to shear strain at any point in an object made of that material. The shear modulus is also known as the torsion modulus.  British Society for Strain Measurement Stressstrain diagram Engineering stressstrain curve"}, {"topic": "Stress (mechanics)", "content": "In continuum mechanics stress is a physical quantity that expresses the internal forces that neighboring particles of a continuous material exert on each other while strain is the measure of the deformation of the material. For example when a solid vertical bar is supporting a weight each particle in the bar pushes on the particles immediately below it. When a liquid is in a closed container under pressure each particle gets pushed against by all the surrounding particles. The container walls and the pressureinducing surface such as a piston push against them in Newtonian reaction. These macroscopic forces are actually the average of a very large number of intermolecular forces and collisions between the particles in those molecules. Strain inside a material may arise by various mechanisms such as stress as applied by external forces to the bulk material like gravity or to its surface like contact forces external pressure or friction. Any strain deformation of a solid material generates an internal elastic stress analogous to the reaction force of a spring that tends to restore the material to its original nondeformed state. In liquids and gases only deformations that change the volume generate persistent elastic stress. However if the deformation is gradually changing with time even in fluids there will usually be some viscous stress opposing that change. Elastic and viscous stresses are usually combined under the name mechanical stress. Significant stress may exist even when deformation is negligible or nonexistent a common assumption when modeling the flow of water. Stress may exist in the absence of external forces such builtin stress is important for example in prestressed concrete and tempered glass. Stress may also be imposed on a material without the application of net forces for example by changes in temperature or chemical composition or by external electromagnetic fields as in piezoelectric and magnetostrictive materials. The relation between mechanical stress deformation and the rate of change of deformation can be quite complicated although a linear approximation may be adequate in practice if the quantities are small enough. Stress that exceeds certain strength limits of the material will result in permanent deformation such as plastic flow fracture cavitation or even change its crystal structure and chemical composition. In some branches of engineering the term stress is occasionally used in a looser sense as a synonym of internal force. For example in the analysis of trusses it may refer to the total traction or compression force acting on a beam rather than the force divided by the area of its crosssection. "}, {"topic": "String duality", "content": "String duality is a class of symmetries in physics that link different string theories theories which assume that the fundamental building blocks of the universe are strings instead of point particles.  Uduality Mirror Symmetry"}, {"topic": "String theory", "content": "In physics string theory is a theoretical framework in which the pointlike particles of particle physics are replaced by onedimensional objects called strings. It describes how these strings propagate through space and interact with each other. On distance scales larger than the string scale a string looks just like an ordinary particle with its mass charge and other properties determined by the vibrational state of the string. In string theory one of the many vibrational states of the string corresponds to the graviton a quantum mechanical particle that carries gravitational force. Thus string theory is a theory of quantum gravity. String theory is a broad and varied subject that attempts to address a number of deep questions of fundamental physics. String theory has been applied to a variety of problems in black hole physics early universe cosmology nuclear physics and condensed matter physics and it has stimulated a number of major developments in pure mathematics. Because string theory potentially provides a unified description of gravity and particle physics it is a candidate for a theory of everything a selfcontained mathematical model that describes all fundamental forces and forms of matter. Despite much work on these problems it is not known to what extent string theory describes the real world or how much freedom the theory allows to choose the details. String theory was first studied in the late 1960s as a theory of the strong nuclear force before being abandoned in favor of quantum chromodynamics. Subsequently it was realized that the very properties that made string theory unsuitable as a theory of nuclear physics made it a promising candidate for a quantum theory of gravity. The earliest version of string theory bosonic string theory incorporated only the class of particles known as bosons. It later developed into superstring theory which posits a connection called supersymmetry between bosons and the class of particles called fermions. Five consistent versions of superstring theory were developed before it was conjectured in the mid1990s that they were all different limiting cases of a single theory in eleven dimensions known as Mtheory. In late 1997 theorists discovered an important relationship called the AdSCFT correspondence which relates string theory to another type of physical theory called a quantum field theory. One of the challenges of string theory is that the full theory does not have a satisfactory definition in all circumstances. Another issue is that the theory is thought to describe an enormous landscape of possible universes and this has complicated efforts to develop theories of particle physics based on string theory. These issues have led some in the community to criticize these approaches to physics and question the value of continued research on string theory unification.  The Elegant UniverseA threehour miniseries with Brian Greene by NOVA original PBS Broadcast Dates October 28 810 p.m. and November 4 89 p.m. 2003. Various images texts videos and animations explaining string theory. Not Even WrongA blog critical of string theory The Official String Theory Web Site Why String TheoryAn introduction to string theory."}, {"topic": "Strong nuclear force", "content": "In particle physics the strong interaction is the mechanism responsible for the strong nuclear force also called the strong force nuclear strong force one of the four known fundamental interactions of nature the others being electromagnetism the weak interaction and gravitation. At the range of a femtometer it is the strongest force being approximately 137 times stronger than electromagnetism a million times stronger than weak interaction and 1038 times stronger than gravitation. The strong nuclear force ensures the stability of ordinary matter confining quarks into hadron particles such as the proton and neutron and the further binding of neutrons and protons into atomic nuclei. Most of the massenergy of a common proton or neutron is in the form of the strong force field energy the individual quarks provide only about 1 of the massenergy of a proton. The strong interaction is observable at two ranges on a larger scale about 1 to 3 femtometers fm it is the force that binds protons and neutrons nucleons together to form the nucleus of an atom. On the smaller scale less than about 0.8 fm the radius of a nucleon it is the force carried by gluons that holds quarks together to form protons neutrons and other hadron particles. In the latter context it is often known as the color force. The strong force inherently has such a high strength that hadrons bound by the strong force can produce new massive particles. Thus if hadrons are struck by highenergy particles they give rise to new hadrons instead of emitting freely moving radiation gluons. This property of the strong force is called color confinement and it prevents the free emission of the strong force instead in practice jets of massive particles are observed. In the context of binding protons and neutrons together to form atomic nuclei the strong interaction is called the nuclear force or residual strong force. In this case it is the residuum of the strong interaction between the quarks that make up the protons and neutrons. As such the residual strong interaction obeys a quite different distancedependent behavior between nucleons from when it is acting to bind quarks within nucleons. The binding energy that is partly released on the breakup of a nucleus is related to the residual strong force and is harnessed in nuclear power and fissiontype nuclear weapons. The strong interaction is hypothesized to be mediated by massless particles called gluons that are exchanged between quarks antiquarks and other gluons. Gluons in turn are thought to interact with quarks and gluons as all carry a type of charge called color charge. Color charge is analogous to electromagnetic charge but it comes in three types rather than one  red  green  blue that results in a different type of force with different rules of behavior. These rules are detailed in the theory of quantum chromodynamics QCD which is the theory of quarkgluon interactions. After the Big Bang during the electroweak epoch the electroweak force separated from the strong force. A Grand Unified Theory is hypothesized to exist to describe this no such theory has been successfully formulated yet and the unification remains an unsolved problem in physics.  Strong force at Encyclopdia Britannica"}, {"topic": "Structural load", "content": "Structural loads or actions are forces deformations or accelerations applied to a structure or its components. Loads cause stresses deformations and displacements in structures. Assessment of their effects is carried out by the methods of structural analysis. Excess load or overloading may cause structural failure and hence such possibility should be either considered in the design or strictly controlled. Mechanical structures such as aircraft satellites rockets space stations ships and submarines have their own particular structural loads and actions. Engineers often evaluate structural loads based upon published regulations contracts or specifications. Accepted technical standards are used for acceptance testing and inspection.  Luebkeman Chris H. and Donald Petting Lecture 17 Primary Loads. University of Oregon. 19961 Fisette Paul and the American Wood Council. Understanding Loads and Using Span Tables. 1997.2"}, {"topic": "Subatomic", "content": "In the physical sciences subatomic particles are particles much smaller than atoms. There are two types of subatomic particles elementary particles which according to current theories are not made of other particles and composite particles. Particle physics and nuclear physics study these particles and how they interact. In particle physics the concept of a particle is one of several concepts inherited from classical physics. But it also reflects the modern understanding that at the quantum scale matter and energy behave very differently from what much of everyday experience would lead us to expect. The idea of a particle underwent serious rethinking when experiments showed that light could behave like a stream of particles called photons as well as exhibit wavelike properties. This led to the new concept of waveparticle duality to reflect that quantumscale particles behave like both particles and waves also known as wavicles. Another new concept the uncertainty principle states that some of their properties taken together such as their simultaneous position and momentum cannot be measured exactly. In more recent times waveparticle duality has been shown to apply not only to photons but to increasingly massive particles as well. Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of quanta of corresponding fundamental interactions. This blends particle physics with field theory.  particleadventure.org The Standard Model. cpepweb.org Particle chart. University of California Particle Data Group. Annotated Physics Encyclopdia Quantum Field Theory. Jose Galvez Chapter 1 Electrodynamics pdf."}, {"topic": "Subatomic particle", "content": "In the physical sciences subatomic particles are particles much smaller than atoms. There are two types of subatomic particles elementary particles which according to current theories are not made of other particles and composite particles. Particle physics and nuclear physics study these particles and how they interact. In particle physics the concept of a particle is one of several concepts inherited from classical physics. But it also reflects the modern understanding that at the quantum scale matter and energy behave very differently from what much of everyday experience would lead us to expect. The idea of a particle underwent serious rethinking when experiments showed that light could behave like a stream of particles called photons as well as exhibit wavelike properties. This led to the new concept of waveparticle duality to reflect that quantumscale particles behave like both particles and waves also known as wavicles. Another new concept the uncertainty principle states that some of their properties taken together such as their simultaneous position and momentum cannot be measured exactly. In more recent times waveparticle duality has been shown to apply not only to photons but to increasingly massive particles as well. Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of quanta of corresponding fundamental interactions. This blends particle physics with field theory.  particleadventure.org The Standard Model. cpepweb.org Particle chart. University of California Particle Data Group. Annotated Physics Encyclopdia Quantum Field Theory. Jose Galvez Chapter 1 Electrodynamics pdf."}, {"topic": "Subatomic particles", "content": "In the physical sciences subatomic particles are particles much smaller than atoms. There are two types of subatomic particles elementary particles which according to current theories are not made of other particles and composite particles. Particle physics and nuclear physics study these particles and how they interact. In particle physics the concept of a particle is one of several concepts inherited from classical physics. But it also reflects the modern understanding that at the quantum scale matter and energy behave very differently from what much of everyday experience would lead us to expect. The idea of a particle underwent serious rethinking when experiments showed that light could behave like a stream of particles called photons as well as exhibit wavelike properties. This led to the new concept of waveparticle duality to reflect that quantumscale particles behave like both particles and waves also known as wavicles. Another new concept the uncertainty principle states that some of their properties taken together such as their simultaneous position and momentum cannot be measured exactly. In more recent times waveparticle duality has been shown to apply not only to photons but to increasingly massive particles as well. Interactions of particles in the framework of quantum field theory are understood as creation and annihilation of quanta of corresponding fundamental interactions. This blends particle physics with field theory.  particleadventure.org The Standard Model. cpepweb.org Particle chart. University of California Particle Data Group. Annotated Physics Encyclopdia Quantum Field Theory. Jose Galvez Chapter 1 Electrodynamics pdf."}, {"topic": "Sublimation (phase transition)", "content": "Sublimation is the transition of a substance directly from the solid to the gas phase without passing through the intermediate liquid phase. Sublimation is an endothermic phase transition that occurs at temperatures and pressures below a substances triple point in its phase diagram. The reverse process of sublimation is desublimation or deposition in which a substance passes directly from a gas to a solid phase. Sublimation has also been used as a generic term to describe phase changes between solid and gas that avoid the liquid state without specifying the direction of the transition. At normal pressures most chemical compounds and elements possess three different states at different temperatures. In these cases the transition from the solid to the gaseous state requires an intermediate liquid state. The pressure referred to is the partial pressure of the substance not the total e.g. atmospheric pressure of the entire system. So all solids that possess an appreciable vapor pressure at a certain temperature usually can sublimate in air e.g. water ice just below 0 C. For some substances such as carbon and arsenic sublimation is much easier than evaporation from the melt because the pressure of their triple point is very high and it is difficult to obtain them as liquids. The term sublimation refers to a physical change of state and is not used to describe transformation of a solid to a gas in a chemical reaction. For example the dissociation on heating of solid ammonium chloride into hydrogen chloride and ammonia is not sublimation but a chemical reaction. Similarly the combustion of candles containing paraffin wax to carbon dioxide and water vapor is not sublimation but a chemical reaction with oxygen. Sublimation requires additional energy and is an endothermic change. The enthalpy of sublimation also called heat of sublimation can be calculated by adding the enthalpy of fusion and the enthalpy of vaporization. "}, {"topic": "Superconductivity", "content": "Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic flux fields occurring in certain materials when cooled below a characteristic critical temperature. It was discovered by Dutch physicist Heike Kamerlingh Onnes on April 8 1911 in Leiden. Like ferromagnetism and atomic spectral lines superconductivity is a quantum mechanical phenomenon. It is characterized by the Meissner effect the complete ejection of magnetic field lines from the interior of the superconductor as it transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of perfect conductivity in classical physics. The electrical resistance of a metallic conductor decreases gradually as temperature is lowered. In ordinary conductors such as copper or silver this decrease is limited by impurities and other defects. Even near absolute zero a real sample of a normal conductor shows some resistance. In a superconductor the resistance drops abruptly to zero when the material is cooled below its critical temperature. An electric current flowing through a loop of superconducting wire can persist indefinitely with no power source. In 1986 it was discovered that some cuprateperovskite ceramic materials have a critical temperature above 90 K 183 C. Such a high transition temperature is theoretically impossible for a conventional superconductor leading the materials to be termed hightemperature superconductors. Liquid nitrogen boils at 77 K and superconduction at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.  Everything about superconductivity properties research applications with videos animations games Video about Type I Superconductors R0transition temperatures B is a state variable Meissner effect Energy gapGiaever BCS model Superconductivity Current in a Cape and Thermal Tights. An introduction to the topic for nonscientists National High Magnetic Field Laboratory Lectures on Superconductivity series of videos including interviews with leading experts Superconductivity News Update Superconductor Week Newsletter industry news links et cetera Superconducting Magnetic Levitation National Superconducting Cyclotron Laboratory at Michigan State University YouTube Video Levitating magnet International Workshop on superconductivity in Diamond and Related Materials free download papers New Diamond and Frontier Carbon Technology Volume 17 No.1 Special Issue on Superconductivity in CVD Diamond DoITPoMS Teaching and Learning Package Superconductivity The Nobel Prize for Physics 19012008 folding handson activities about superconductivity"}, {"topic": "Superconductor", "content": "Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic flux fields occurring in certain materials when cooled below a characteristic critical temperature. It was discovered by Dutch physicist Heike Kamerlingh Onnes on April 8 1911 in Leiden. Like ferromagnetism and atomic spectral lines superconductivity is a quantum mechanical phenomenon. It is characterized by the Meissner effect the complete ejection of magnetic field lines from the interior of the superconductor as it transitions into the superconducting state. The occurrence of the Meissner effect indicates that superconductivity cannot be understood simply as the idealization of perfect conductivity in classical physics. The electrical resistance of a metallic conductor decreases gradually as temperature is lowered. In ordinary conductors such as copper or silver this decrease is limited by impurities and other defects. Even near absolute zero a real sample of a normal conductor shows some resistance. In a superconductor the resistance drops abruptly to zero when the material is cooled below its critical temperature. An electric current flowing through a loop of superconducting wire can persist indefinitely with no power source. In 1986 it was discovered that some cuprateperovskite ceramic materials have a critical temperature above 90 K 183 C. Such a high transition temperature is theoretically impossible for a conventional superconductor leading the materials to be termed hightemperature superconductors. Liquid nitrogen boils at 77 K and superconduction at higher temperatures than this facilitates many experiments and applications that are less practical at lower temperatures.  Everything about superconductivity properties research applications with videos animations games Video about Type I Superconductors R0transition temperatures B is a state variable Meissner effect Energy gapGiaever BCS model Superconductivity Current in a Cape and Thermal Tights. An introduction to the topic for nonscientists National High Magnetic Field Laboratory Lectures on Superconductivity series of videos including interviews with leading experts Superconductivity News Update Superconductor Week Newsletter industry news links et cetera Superconducting Magnetic Levitation National Superconducting Cyclotron Laboratory at Michigan State University YouTube Video Levitating magnet International Workshop on superconductivity in Diamond and Related Materials free download papers New Diamond and Frontier Carbon Technology Volume 17 No.1 Special Issue on Superconductivity in CVD Diamond DoITPoMS Teaching and Learning Package Superconductivity The Nobel Prize for Physics 19012008 folding handson activities about superconductivity"}, {"topic": "Supernova", "content": "A supernova is an astronomical event that occurs during the last stellar evolutionary stages of a massive stars life whose dramatic and catastrophic destruction is marked by one final titanic explosion. For a short time this causes the sudden appearance of a new bright star before slowly fading from sight over several weeks or months. Only three Milky Way nakedeye supernova events have been observed during the last thousand years though many have been telescopically seen in other galaxies. The most recent directly observed supernova in the Milky Way was Keplers Star in 1604 SN 1604 but remnants of two more recent supernovae have been found retrospectively. Statistical observations of supernovae in other galaxies suggest they should occur on average about three times every century in the Milky Way and that any galactic supernova would almost certainly be observable in modern astronomical equipment. During maximum brightness the total equivalent radiant energies produced by supernovae may briefly outshine an entire output of a typical galaxy and emit energies equal to that created over the lifetime of any solarlike star. Such extreme catastrophes may also expel much if not all of its stellar material away from the star at velocities up to 700730000000000000030000 kms or 10 of the speed of light. This drives an expanding and fastmoving shock wave into the surrounding interstellar medium and in turn sweeping up an expanding shell of gas and dust which is observed as a supernova remnant. Supernovae create fuse and eject the bulk of the chemical elements produced by nucleosynthesis. Supernovae play a significant role in enriching the interstellar medium with the heavier atomic mass chemical elements. Furthermore the expanding shock waves from supernova explosions can trigger the formation of new stars. Supernova remnants are expected to accelerate a large fraction of galactic primary cosmic rays but direct evidence for cosmic ray production was found only in a few of them so far. They are also potentially strong galactic sources of gravitational waves. Theoretical studies of many supernovae indicate that most are triggered by one of two basic mechanisms the sudden reignition of nuclear fusion in a degenerate star or the sudden gravitational collapse of a massive stars core. In the first instance a degenerate white dwarf may accumulate sufficient material from a binary companion either through accretion or via a merger to raise its core temperature enough to trigger runaway nuclear fusion completely disrupting the star. In the second case the core of a massive star may undergo sudden gravitational collapse releasing gravitational potential energy as a supernova. While some observed supernovae are more complex than these two simplified theories the astrophysical collapse mechanics have been established and accepted by most astronomers for some time. Due to the wide range of astrophysical consequences of these events astronomers now deem supernovae research across the fields of stellar and galactic evolution as an especially important area for investigation.  RSS news feed RSS. The Astronomers Telegram. Retrieved 20061128. Tsvetkov D. Yu. Pavlyuk N. N. Bartunov O. S. Pskovskii Yu. P. Sternberg Astronomical Institute Supernova Catalogue. Sternberg Astronomical Institute Moscow University. Retrieved 20061128. A searchable catalog. The Open Supernova Catalog. Retrieved 20160202. An openaccess catalog of supernova light curves and spectra. List of Supernovae with IAU Designations. IAU Central Bureau for Astronomical Telegrams. Retrieved 20101025. Overbye Dennis 20080521. Scientists See Supernova in Action. The New York Times. Retrieved 20080521. subscription required"}, {"topic": "Superposition principle", "content": "In physics and systems theory the superposition principle also known as superposition property states that for all linear systems the net response at a given place and time caused by two or more stimuli is the sum of the responses which would have been caused by each stimulus individually. So that if input A produces response X and input B produces response Y then input A  B produces response X  Y. The homogeneity and additivity properties together are called the superposition principle. A linear function is one that satisfies the properties of superposition. Which is defined as F  x 1  x 2   F  x 1   F  x 2   This decomposition can help to simplify controller design. "}, {"topic": "Surface tension", "content": "Surface tension is the elastic tendency of a fluid surface which makes it acquire the least surface area possible. Surface tension allows insects e.g. water striders usually denser than water to float and stride on a water surface. At liquidair interfaces surface tension results from the greater attraction of liquid molecules to each other due to cohesion than to the molecules in the air due to adhesion. The net effect is an inward force at its surface that causes the liquid to behave as if its surface were covered with a stretched elastic membrane. Thus the surface becomes under tension from the imbalanced forces which is probably where the term surface tension came from. Because of the relatively high attraction of water molecules for each other through a web of hydrogen bonds water has a higher surface tension 72.8 millinewtons per meter at 20 C compared to that of most other liquids. Surface tension is an important factor in the phenomenon of capillarity. Surface tension has the dimension of force per unit length or of energy per unit area. The two are equivalent but when referring to energy per unit of area it is common to use the term surface energy which is a more general term in the sense that it applies also to solids. In materials science surface tension is used for either surface stress or surface free energy.  On surface tension and interesting realworld cases MIT Lecture Notes on Surface Tension Surface Tensions of Various Liquids Calculation of temperaturedependent surface tensions for some common components Surface Tension Calculator For Aqueous Solutions Containing the Ions H NH4 Na K Mg2 Ca2 SO42 NO3 Cl CO32 Br and OH. T. Proctor Hall 1893 New methods of measuring surface tension in liquids Philosophical Magazine series 5 36 385 415 link from Biodiversity Heritage Library. The Bubble Wall Audio slideshow from the National High Magnetic Field Laboratory explaining cohesion surface tension and hydrogen bonds C. Pfister Interface Free Energy. Scholarpedia 2010 from first principles of statistical mechanics"}, {"topic": "Technical standard", "content": "A technical standard is an established norm or requirement in regard to technical systems. It is usually a formal document that establishes uniform engineering or technical criteria methods processes and practices. In contrast a custom convention company product corporate standard etc. that becomes generally accepted and dominant is often called a de facto standard. A technical standard may be developed privately or unilaterally for example by a corporation regulatory body military etc. Standards can also be developed by groups such as trade unions and trade associations. Standards organizations often have more diverse input and usually develop voluntary standards these might become mandatory if adopted by a government i.e. through legislation business contract etc. The standardization process may be by edict or may involve the formal consensus of technical experts. "}, {"topic": "Temperature", "content": "A temperature is an objective comparative measure of hot or cold. It is measured by a thermometer which may work through the bulk behavior of a thermometric material detection of thermal radiation or particle kinetic energy. Several scales and units exist for measuring temperature the most common being Celsius denoted C formerly called centigrade Fahrenheit denoted F and especially in science Kelvin denoted K. The coldest theoretical temperature is absolute zero at which the thermal motion of atoms and molecules reaches its minimum classically this would be a state of motionlessness but quantum uncertainty dictates that the particles still possess a finite zeropoint energy. In addition to this a real system or object can never be brought to a temperature of absolute zero by thermodynamic means. Absolute zero is denoted as 0 K on the Kelvin scale 273.15 C on the Celsius scale and 459.67 F on the Fahrenheit scale. The kinetic theory offers a valuable but limited account of the behavior of the materials of macroscopic bodies especially of fluids. It indicates the absolute temperature as proportional to the average kinetic energy of the random microscopic motions of those of their constituent microscopic particles such as electrons atoms and molecules that move freely within the material. Temperature is important in all fields of natural science including physics geology chemistry atmospheric sciences medicine and biology as well as most aspects of daily life.  An elementary introduction to temperature aimed at a middle school audience from Oklahoma State University Average yearly temperature by country A tabular list of countries and Thermal Map displaying the average yearly temperature by country"}, {"topic": "Tensile Modulus", "content": "Youngs modulus which is also known as the elastic modulus is a mechanical property of linear elastic solid materials. It defines the relationship between stress force per unit area and strain proportional deformation in a material. Youngs modulus is named after the 19thcentury British scientist Thomas Young. However the concept was developed in 1727 by Leonhard Euler and the first experiments that used the concept of Youngs modulus in its current form were performed by the Italian scientist Giordano Riccati in 1782 predating Youngs work by 25 years. The term modulus is the diminutive of the Latin term modus which means measure. A solid body deforms when a load is applied to it. If the material is elastic the body returns to its original shape after the load is removed. The material is linear if the ratio of load to deformation remains constant during the loading process. Not many materials are linear and elastic beyond a small amount of deformation. A constant Youngs modulus applies only to linear elastic materials. A perfectly rigid material has an infinite Youngs modulus because an infinite force is needed to deform such a material. A material whose Youngs modulus is very high can be approximated as rigid. A stiff material needs more force to deform compared to a soft material. The Youngs modulus is a measure of the stiffness of a solid material. Do not confuse stiffness with strength the strength of material is the amount of force it can withstand and still recover its original shape material stiffness with geometric stiffness the geometric stiffness depends on shape e.g. the stiffness of an I beam is much higher than that of a spring made of the same steel thus having the same rigidity stiffness with hardness the hardness of a material defines the relative resistance that its surface imposes against the penetration of a harder body stiffness with toughness toughness is the amount of energy that a material can absorb before fracturing.  Matweb free database of engineering properties for over 63000 materials Youngs Modulus for groups of materials and their cost"}, {"topic": "Tensile strength", "content": "Ultimate tensile strength UTS often shortened to tensile strength TS or ultimate strength is the capacity of a material or structure to withstand loads tending to elongate as opposed to compressive strength which withstands loads tending to reduce size. In other words tensile strength resists tension being pulled apart whereas compressive strength resists compression being pushed together. Ultimate tensile strength is measured by the maximum stress that a material can withstand while being stretched or pulled before breaking. In the study of strength of materials tensile strength compressive strength and shear strength can be analyzed independently. Some materials break very sharply without plastic deformation in what is called a brittle failure. Others which are more ductile including most metals experience some plastic deformation and possibly necking before fracture. The UTS is usually found by performing a tensile test and recording the engineering stress versus strain. The highest point of the stressstrain curve see point 1 on the engineering stressstrain diagrams below is the UTS. It is an intensive property therefore its value does not depend on the size of the test specimen. However it is dependent on other factors such as the preparation of the specimen the presence or otherwise of surface defects and the temperature of the test environment and material. Tensile strengths are rarely used in the design of ductile members but they are important in brittle members. They are tabulated for common materials such as alloys composite materials ceramics plastics and wood. Tensile strength can be defined for liquids as well as solids under certain conditions. For example when a tree draws water from its roots to its upper leaves by transpiration the column of water is pulled upwards from the top by the cohesion of the water in the xylem and this force is transmitted down the column by its tensile strength. Air pressure osmotic pressure and capillary tension also plays a small part in a trees ability to draw up water but this alone would only be sufficient to push the column of water to a height of less than ten metres and trees can grow much higher than that over 100m. Tensile strength is defined as a stress which is measured as force per unit area. For some nonhomogeneous materials or for assembled components it can be reported just as a force or as a force per unit width. In the International System of Units SI the unit is the pascal Pa or a multiple thereof often megapascals MPa using the SI prefix mega or equivalently to pascals newtons per square metre Nm. A United States customary unit is pounds per square inch lbin or psi or kilopounds per square inch ksi or sometimes kpsi which is equal to 1000 psi kilopounds per square inch are commonly used in one country USA when measuring tensile strengths.  Giancoli Douglas Physics for Scientists  Engineers Third Edition 2000. Upper Saddle River Prentice Hall. Khler T Vollrath F 1995. Thread biomechanics in the two orbweaving spiders Araneus diadematus Araneae Araneidae and Uloboris walckenaerius Araneae Uloboridae. Journal of Experimental Zoology 271 117. doi10.1002jez.1402710102. T Follett Life without metals MinFeng Y Lourie O Dyer MJ Moloni K Kelly TF Ruoff RS 2000. Strength and Breaking Mechanism of Multiwalled Carbon Nanotubes Under Tensile Load. Science 287 5453 637640. Bibcode2000Sci...287..637Y. doi10.1126science.287.5453.637. PMID 10649994. George E. Dieter Mechanical Metallurgy 1988. McGrawHill UK"}, {"topic": "Tesla (unit)", "content": "The tesla symbol T is a unit of measurement of the strength of a flux density. It is a derived unit of the International System of Units the modern form of the metric system. One tesla is equal to one weber per square metre. The unit was announced during the General Conference on Weights and Measures in 1960 and is named in honour of Nikola Tesla. The strongest fields encountered from permanent magnets are from Halbach spheres which can be over 4.5 T. The strongest field trapped in a laboratory superconductor as of July 2014 is 17.6 T. The record magnetic field has been produced by scientists at the Los Alamos National Laboratory campus of the National High Magnetic Field Laboratory the worlds first 100tesla nondestructive magnetic field.  GaussTesla Conversion Tool"}, {"topic": "Test particle", "content": "In physical theories a test particle is an idealized model of an object whose physical properties usually mass charge or size are assumed to be negligible except for the property being studied which is considered to be insufficient to alter the behavior of the rest of the system. The concept of a test particle often simplifies problems and can provide a good approximation for physical phenomena. In addition to its uses in the simplification of the dynamics of a system in particular limits it is also used as a diagnostic in computer simulations of physical processes. "}, {"topic": "Theoretical physics", "content": "Theoretical physics is a branch of physics which employs mathematical models and abstractions of physical objects and systems to rationalize explain and predict natural phenomena. This is in contrast to experimental physics which uses experimental tools to probe these phenomena. The advancement of science depends in general on the interplay between experimental studies and theory. In some cases theoretical physics adheres to standards of mathematical rigor while giving little weight to experiments and observations. For example while developing special relativity Albert Einstein was concerned with the Lorentz transformation which left Maxwells equations invariant but was apparently uninterested in the MichelsonMorley experiment on Earths drift through a luminiferous ether. Conversely Einstein was awarded the Nobel Prize for explaining the photoelectric effect previously an experimental result lacking a theoretical formulation.  Timeline of Theoretical Physics MIT Center for Theoretical Physics How to become a GOOD Theoretical Physicist a website made by Gerard t Hooft Theory of longitudinal and transversal angular momentums"}, {"topic": "Theory of relativity", "content": "The theory of relativity usually encompasses two theories by Albert Einstein special relativity and general relativity. Concepts introduced by the theories of relativity include spacetime as a unified entity of space and time relativity of simultaneity kinematic and gravitational time dilation and length contraction. The term theory of relativity was based on the expression relative theory German Relativtheorie used in 1906 by Max Planck who emphasized how the theory uses the principle of relativity. In the discussion section of the same paper Alfred Bucherer used for the first time the expression theory of relativity German Relativittstheorie.  Theory of relativity at DMOZ Relativity Milestones Timeline of Notable Relativity Scientists and Contributions The dictionary definition of theory of relativity at Wiktionary Media related to Theory of relativity at Wikimedia Commons"}, {"topic": "Thermal conduction", "content": "Thermal conduction is the transfer of heat internal energy by microscopic collisions of particles and movement of electrons within a body. The microscopically colliding objects that include molecules atoms and electrons transfer disorganized microscopic kinetic and potential energy jointly known as internal energy. Conduction takes place in all phases of matter such as solids liquids gases and plasmas. The rate at which energy is conducted as heat between two bodies is a function of the temperature difference temperature gradient between the two bodies and the properties of the conductive medium through which the heat is transferred. Thermal conduction was originally called diffusion. Heat spontaneously flows from a hotter to a colder body. For example heat is conducted from the hotplate of an electric stove to the bottom of a saucepan in contact with it. In the absence of an external driving energy source to the contrary within a body or between bodies temperature differences decay over time and thermal equilibrium is approached temperature becoming more uniform. In conduction the heat flow is within and through the body itself. In contrast in heat transfer by thermal radiation the transfer is often between bodies which may be separated spatially. Also possible is transfer of heat by a combination of conduction and thermal radiation. In convection internal energy is carried between bodies by a moving material carrier. In solids conduction is mediated by the combination of vibrations and collisions of molecules of propagation and collisions of phonons and of diffusion and collisions of free electrons. In gases and liquids conduction is due to the collisions and diffusion of molecules during their random motion. Photons in this context do not collide with one another and so heat transport by electromagnetic radiation is conceptually distinct from heat conduction by microscopic diffusion and collisions of material particles and phonons. But the distinction is often not easily observed unless the material is semitransparent. In the engineering sciences heat transfer includes the processes of thermal radiation convection and sometimes mass transfer. Usually more than one of these processes occurs in a given situation. The conventional symbol for the material property thermal conductivity is k  . This varies according to the material.  Heat conduction ThermalFluidsPedia Newtons Law of Cooling by Jeff Bryant based on a program by Stephen Wolfram Wolfram Demonstrations Project."}, {"topic": "Thermal equilibrium", "content": "Two physical systems are in thermal equilibrium if no heat flows between them when they are connected by a path permeable to heat. Thermal equilibrium obeys the zeroth law of thermodynamics. A system is said to be in thermal equilibrium with itself if the temperature within the system is spatially and temporally uniform. Systems in thermodynamic equilibrium are always in thermal equilibrium but the converse is not always true. If the connection between the systems allows transfer of energy as heat but does not allow transfer of matter or transfer of energy as work the two systems may reach thermal equilibrium without reaching thermodynamic equilibrium.  Adkins C.J. 19681983. Equilibrium Thermodynamics third edition McGrawHill London ISBN 0521254450. Bailyn M. 1994. A Survey of Thermodynamics American Institute of Physics Press New York ISBN 0883187973. Boltzmann L. 18961964. Lectures on Gas Theory translated by S.G. Brush University of California Press Berkeley. Chapman S. Cowling T.G. 19391970. The Mathematical Theory of Nonuniform gases. An Account of the Kinetic Theory of Viscosity Thermal Conduction and Diffusion in Gases third edition 1970 Cambridge University Press London. Gibbs J.W. 18761878. On the equilibrium of heterogeneous substances Trans. Conn. Acad. 3 108248 343524 reprinted in The Collected Works of J. Willard Gibbs Ph.D LL. D. edited by W.R. Longley R.G. Van Name Longmans Green  Co. New York 1928 volume 1 pp. 55353. Maxwell J.C. 1867. On the dynamical theory of gases Phil. Trans. Roy. Soc. London 157 4988. Mnster A. 1970. Classical Thermodynamics translated by E.S. Halberstadt WileyInterscience London. Partington J.R. 1949. An Advanced Treatise on Physical Chemistry volume 1 Fundamental Principles. The Properties of Gases Longmans Green and Co. London. Planck M. 18971903. Treatise on Thermodynamics translated by A. Ogg first English edition Longmans Green and Co. London. Planck M. 1914. The Theory of Heat Radiation second edition translated by M. Masius P. Blakistons Son and Co. Philadelphia. ter Haar D. Wergeland H. 1966. Elements of Thermodynamics AddisonWesley Publishing Reading MA. Tisza L. 1966. Generalized Thermodynamics M.I.T. Press Cambridge MA."}, {"topic": "Thermal radiation", "content": "Thermal radiation is electromagnetic radiation generated by the thermal motion of charged particles in matter. All matter with a temperature greater than absolute zero emits thermal radiation. When the temperature of the body is greater than absolute zero interatomic collisions cause the kinetic energy of the atoms or molecules to change. This results in chargeacceleration andor dipole oscillation which produces electromagnetic radiation and the wide spectrum of radiation reflects the wide spectrum of energies and accelerations that occur even at a single temperature. Examples of thermal radiation include the visible light and infrared light emitted by an incandescent light bulb the infrared radiation emitted by animals and detectable with an infrared camera and the cosmic microwave background radiation. Thermal radiation is different from thermal convection and thermal conductiona person near a raging bonfire feels radiant heating from the fire even if the surrounding air is very cold. Sunlight is part of thermal radiation generated by the hot plasma of the Sun. The Earth also emits thermal radiation but at a much lower intensity and different spectral distribution infrared rather than visible because it is cooler. The Earths absorption of solar radiation followed by its outgoing thermal radiation are the two most important processes that determine the temperature and climate of the Earth. If a radiationemitting object meets the physical characteristics of a black body in thermodynamic equilibrium the radiation is called blackbody radiation. Plancks law describes the spectrum of blackbody radiation which depends only on the objects temperature. Wiens displacement law determines the most likely frequency of the emitted radiation and the StefanBoltzmann law gives the radiant intensity. Thermal radiation is one of the fundamental mechanisms of heat transfer.  Black Body Emission Calculator Heat transfer Atmospheric Radiation Infrared Temperature Calibration 101"}, {"topic": "Thermionic emission", "content": "Thermionic emission is the thermally induced flow of charge carriers from a surface or over a potentialenergy barrier. This occurs because the thermal energy given to the carrier overcomes the work function of the material. The charge carriers can be electrons or ions and in older literature are sometimes referred to as thermions. After emission a charge that is equal in magnitude and opposite in sign to the total charge emitted is initially left behind in the emitting region. But if the emitter is connected to a battery the charge left behind is neutralized by charge supplied by the battery as the emitted charge carriers move away from the emitter and finally the emitter will be in the same state as it was before emission. The classical example of thermionic emission is the emission of electrons from a hot cathode into a vacuum also known as thermal electron emission or the Edison effect in a vacuum tube. The hot cathode can be a metal filament a coated metal filament or a separate structure of metal or carbides or borides of transition metals. Vacuum emission from metals tends to become significant only for temperatures over 1000 K 730 C 1340 F. The science dealing with this phenomenon has been known as thermionics but this name seems to be gradually falling into disuse. The term thermionic emission is now also used to refer to any thermallyexcited charge emission process even when the charge is emitted from one solidstate region into another. This process is crucially important in the operation of a variety of electronic devices and can be used for electricity generation such as thermionic converters and electrodynamic tethers or cooling. The magnitude of the charge flow increases dramatically with increasing temperature.  How vacuum tubes really work with a section on thermionic emission with equations johnaharper.com. Owen Richardsons Nobel lecture on thermionics nobel.se December 12 1929. PDF Derivations of thermionic emission equations from an undergraduate lab csbsju.edu."}, {"topic": "Thermodynamic free energy", "content": "The thermodynamic free energy is the amount of work that a thermodynamic system can perform. The concept is useful in the thermodynamics of chemical or thermal processes in engineering and science. The free energy is the internal energy of a system minus the amount of energy that cannot be used to perform work. This unusable energy is given by the entropy of a system multiplied by the temperature of the system. Like the internal energy the free energy is a thermodynamic state function. Energy is a generalization of free energy since energy is the ability to do work which is actually free energy. "}, {"topic": "Thermodynamics", "content": "Thermodynamics is the branch of science concerned with heat and temperature and their relation to energy and work. It states that the behavior of these quantities is governed by the four laws of thermodynamics irrespective of the composition or specific properties of the material or system in question. The laws of thermodynamics are explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to a wide variety of topics in science and engineering especially physical chemistry chemical engineering and mechanical engineering. Historically thermodynamics developed out of a desire to increase the efficiency of early steam engines particularly through the work of French physicist Nicolas Lonard Sadi Carnot 1824 who believed that engine efficiency was the key that could help France win the Napoleonic Wars. Scottish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854 Thermodynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies and the relation of heat to electrical agency. The initial application of thermodynamics to mechanical heat engines was extended early on to the study of chemical systems. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged in the following decades. Statistical thermodynamics or statistical mechanics concerned itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909 Constantin Carathodory presented a purely mathematical approach to the field in his axiomatic formulation of thermodynamics a description often referred to as geometrical thermodynamics.  Thermodynamics Data  Property Calculation Websites Thermodynamics Educational Websites Thermodynamics at ScienceWorld Biochemistry Thermodynamics Thermodynamics and Statistical Mechanics Engineering Thermodynamics  A Graphical Approach Thermodynamics and Statistical Mechanics by Richard Fitzpatrick"}, {"topic": "Thermometer", "content": "A thermometer is a device that measures temperature or a temperature gradient. A thermometer has two important elements 1 a temperature sensor e.g. the bulb of a mercuryinglass thermometer in which some physical change occurs with temperature and 2 some means of converting this physical change into a numerical value e.g. the visible scale that is marked on a mercuryinglass thermometer. Thermometers are widely used in industry to control and regulate processes in the study of weather in medicine and in scientific research. There are various principles by which different thermometers operate. They include the thermal expansion of solids or liquids with temperature and the change in pressure of a gas on heating or cooling. Radiationtype thermometers measure the infrared energy emitted by an object allowing measurement of temperature without contact.Most metals are good conductors of heat and they are solids at room temperature. Mercury is the only one in liquid state at room temperature.It has high coefficient of expansion. Hence the slightest change in temperature is notable when its used in a thermometer.This is the reason behind mercury being used in thermometer. Some of the principles of the thermometer were known to Greek philosophers of two thousand years ago. The modern thermometer gradually evolved from the thermoscope with the addition of a scale in the early 17th century and standardisation through the 17th and 18th centuries.  History of Temperature and Thermometry The Chemical Educator Vol. 5 No. 2 2000 The ThermometerFrom The Feeling To The Instrument History Channel Invention Notable Modern Inventions and Discoveries About Thermometer Thermometers Early History Anders Celsius Gabriel Fahrenheit and Thomson Kelvin. Thermometers and Thermometric Liquids Mercury and Alcohol. The NIST Industrial Thermometer Calibration Laboratory Thermometry at the NanoscaleReview"}, {"topic": "Third law of thermodynamics", "content": "The third law of thermodynamics is sometimes stated as follows regarding the properties of systems in equilibrium at absolute zero temperature The entropy of a perfect crystal at absolute zero is exactly equal to zero. At absolute zero zero kelvin the system must be in a state with the minimum possible energy and the above statement of the third law holds true provided that the perfect crystal has only one minimum energy state. Entropy is related to the number of accessible microstates and for a system consisting of many particles quantum mechanics indicates that there is only one unique state called the ground state with minimum energy. If the system does not have a welldefined order if its order is glassy for example then in practice there will remain some finite entropy as the system is brought to very low temperatures as the system becomes locked into a configuration with nonminimal energy. The constant value is called the residual entropy of the system. The NernstSimon statement of the third law of thermodynamics concerns thermodynamic processes at a fixed low temperature The entropy change associated with any condensed system undergoing a reversible isothermal process approaches zero as the temperature at which it is performed approaches 0 K. Here a condensed system refers to liquids and solids. A classical formulation by Nernst actually a consequence of the Third Law is It is impossible for any process no matter how idealized to reduce the entropy of a system to its absolutezero value in a finite number of operations. Physically the NernstSimon statement implies that it is impossible for any procedure to bring a system to the absolute zero of temperature in a finite number of steps.  Goldstein Martin  Inge F. 1993 The Refrigerator and the Universe. Cambridge MA Harvard University Press. ISBN 0674753240. Chpt. 14 is a nontechnical discussion of the Third Law one including the requisite elementary quantum mechanics. Braun S. Ronzheimer J. P. Schreiber M. Hodgman S. S. Rom T. Bloch I. Schneider U. 2013. Negative Absolute Temperature for Motional Degrees of Freedom. Science 339 6115 525. arXiv1211.0545. Bibcode2013Sci...339...52B. doi10.1126science.1227831. PMID 23288533. Lay summary New Scientist 3 January 2013. Levy A. Alicki R. Kosloff R. 2012. Quantum refrigerators and the third law of thermodynamics. Phys. Rev. E 85 061126. arXiv1205.1347. Bibcode2012PhRvE..85f1126L. doi10.1103PhysRevE.85.061126."}, {"topic": "Torque", "content": "Torque moment or moment of force see the terminology below is the tendency of a force to rotate an object about an axis fulcrum or pivot. Just as a force is a push or a pull a torque can be thought of as a twist to an object. Mathematically torque is defined as the cross product of the vector by which the forces application point is offset relative to the fixed suspension point distance vector and the force vector which tends to produce rotation. Loosely speaking torque is a measure of the turning force on an object such as a bolt or a flywheel. For example pushing or pulling the handle of a wrench connected to a nut or bolt produces a torque turning force that loosens or tightens the nut or bolt. The symbol for torque is typically   Horsepower and Torque An article showing how power torque and gearing affect a vehicles performance. Torque vs. Horsepower Yet Another Argument An automotive perspective a discussion of torque and angular momentum in an online textbook Torque and Angular Momentum in Circular Motion on Project PHYSNET. An interactive simulation of torque Torque Unit Converter A feel for torque An orderofmagnitude interactive."}, {"topic": "Total internal reflection", "content": "Total internal reflection is a phenomenon which occurs when a propagating wave strikes a medium boundary at an angle larger than a particular critical angle with respect to the normal to the surface. If the refractive index is lower on the other side of the boundary and the incident angle is greater than the critical angle the wave cannot pass through and is entirely reflected. The critical angle is the angle of incidence above which the total internal reflection occurs. This is particularly common as an optical phenomenon where light waves are involved but it occurs with many types of waves such as electromagnetic waves in general or sound waves. When a wave reaches a boundary between different materials with different refractive indices the wave will in general be partially refracted at the boundary surface and partially reflected. However if the angle of incidence is greater i.e. the direction of propagation is closer to being parallel to the boundary than the critical angle the angle of incidence at which light is refracted such that it travels along the boundary then the wave will not cross the boundary but will instead be totally reflected back internally. This can only occur when the wave in a medium with a higher refractive index n1 reaches a boundary with a medium of lower refractive index n2. For example it will occur with light reaching air from glass but not when reaching glass from air.  FTIR Touch Sensing MultiTouch Interaction Research Georgia State University Total Internal Reflection by Michael Schreiber Wolfram Demonstrations Project Total Internal Reflection St. Marys Physics Online Notes Bowley Roger 2009. Total Internal Reflection. Sixty Symbols. Brady Haran for the University of Nottingham. TETM Reflection Coefficients interactive phase and magnitude plots"}, {"topic": "Toughness", "content": "In materials science and metallurgy toughness is the ability of a material to absorb energy and plastically deform without fracturing. One definition of material toughness is the amount of energy per unit volume that a material can absorb before rupturing. It is also defined as a materials resistance to fracture when stressed. Toughness requires a balance of strength and ductility. "}, {"topic": "Transverse wave", "content": "A transverse wave is a moving wave that consists of oscillations occurring perpendicular or right angled to the direction of energy transfer. If a transverse wave is moving in the positive xdirection its oscillations are in up and down directions that lie in the yz plane. Light is an example of a transverse wave. With regard to transverse waves in matter the displacement of the medium is perpendicular to the direction of propagation of the wave. A ripple in a pond and a wave on a string are easily visualized as transverse waves.  Interactive simulation of transverse wave Wave types explained with high speed film and animations Weisstein Eric W. Transverse Wave from ScienceWorld. Transverse and Longitudinal Waves Introductory module on these waves at Connexions"}, {"topic": "Trimean", "content": "In statistics the trimean TM or Tukeys trimean is a measure of a probability distributions location defined as a weighted average of the distributions median and its two quartiles T M  Q 1  2 Q 2  Q 3 4  The foundations of the trimean were part of Arthur Bowleys teachings and later popularized by statistician John Tukey in his 1977 book which has given its name to a set of techniques called Exploratory data analysis. Like the median and the midhinge but unlike the sample mean it is a statistically resistant Lestimator with a breakdown point of 25. This beneficial property has been described as follows An advantage of the trimean as a measure of the center of a distribution is that it combines the medians emphasis on center values with the midhinges attention to the extremes.  Trimean at MathWorld"}, {"topic": "Triple point", "content": "In thermodynamics the triple point of a substance is the temperature and pressure at which the three phases gas liquid and solid of that substance coexist in thermodynamic equilibrium. For example the triple point of mercury occurs at a temperature of 38.83440 C and a pressure of 0.2 mPa. In addition to the triple point for solid liquid and gas phases a triple point may involve more than one solid phase for substances with multiple polymorphs. Helium4 is a special case that presents a triple point involving two different fluid phases lambda point. The triple point of water is used to define the kelvin the base unit of thermodynamic temperature in the International System of Units SI. The value of the triple point of water is fixed by definition rather than measured. The triple points of several substances are used to define points in the ITS90 international temperature scale ranging from the triple point of hydrogen 13.8033 K to the triple point of water 273.16 K 0.01 C or 32.018 F. The term triple point was coined in 1873 by James Thomson brother of Lord Kelvin. "}, {"topic": "Uncertainty principle", "content": "In quantum mechanics the uncertainty principle also known as Heisenbergs uncertainty principle is any of a variety of mathematical inequalities asserting a fundamental limit to the precision with which certain pairs of physical properties of a particle known as complementary variables such as position x and momentum p can be known. Introduced first in 1927 by the German physicist Werner Heisenberg it states that the more precisely the position of some particle is determined the less precisely its momentum can be known and vice versa. The formal inequality relating the standard deviation of position x and the standard deviation of momentum p was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928  is the reduced Planck constant h  2. Historically the uncertainty principle has been confused with a somewhat similar effect in physics called the observer effect which notes that measurements of certain systems cannot be made without affecting the systems. Heisenberg offered such an observer effect at the quantum level see below as a physical explanation of quantum uncertainty. It has since become clear however that the uncertainty principle is inherent in the properties of all wavelike systems and that it arises in quantum mechanics simply due to the matter wave nature of all quantum objects. Thus the uncertainty principle actually states a fundamental property of quantum systems and is not a statement about the observational success of current technology. It must be emphasized that measurement does not mean only a process in which a physicistobserver takes part but rather any interaction between classical and quantum objects regardless of any observer. N.B. on precision If x and p are the precisions of position and momentum obtained in an individual measurement and x  for some convenient polynomial P and real positive definite matrix A of type d d.  Hazewinkel Michiel ed. 2001 Uncertainty principle Encyclopedia of Mathematics Springer ISBN 9781556080104 Matter as a Wave a chapter from an online textbook Quantum mechanics Myths and facts Stanford Encyclopedia of Philosophy entry Fourier Transforms and Uncertainty at MathPages aip.org Quantum mechanics 19251927 The uncertainty principle Eric Weissteins World of Physics Uncertainty principle John Baez on the timeenergy uncertainty relation The certainty principle Common Interpretation of Heisenbergs Uncertainty Principle Is Proved False"}, {"topic": "Uniform motion", "content": "Newtons laws of motion are three physical laws that together laid the foundation for classical mechanics. They describe the relationship between a body and the forces acting upon it and its motion in response to those forces. They have been expressed in several different ways over nearly three centuries and can be summarised as follows. The three laws of motion were first compiled by Isaac Newton in his Philosophi Naturalis Principia Mathematica Mathematical Principles of Natural Philosophy first published in 1687. Newton used them to explain and investigate the motion of many physical objects and systems. For example in the third volume of the text Newton showed that these laws of motion combined with his law of universal gravitation explained Keplers laws of planetary motion.  MIT Physics video lecture on Newtons three laws Light and Matter an online textbook Simulation on Newtons first law of motion Newtons Second Law by Enrique Zeleny Wolfram Demonstrations Project. Newtons 3rd Law demonstrated in a vacuum on YouTube"}, {"topic": "Unit vector", "content": "In mathematics a unit vector in a normed vector space is a vector often a spatial vector of length 1. A unit vector is often denoted by a lowercase letter with a circumflex or hat   is the LeviCivita symbol which is one for permutations ordered as ijk and minus one for permutations ordered as kji.  G. B. Arfken  H. J. Weber 2000. Mathematical Methods for Physicists 5th ed.. Academic Press. ISBN 0120598256. Spiegel Murray R. 1998. Schaums Outlines Mathematical Handbook of Formulas and Tables 2nd ed.. McGrawHill. ISBN 0070382034. Griffiths David J. 1998. Introduction to Electrodynamics 3rd ed.. Prentice Hall. ISBN 013805326X. Bierre Pierre 2010. Flexing the Power of Algorithmic Geometry 1st ed.. Spatial Thoughtware. ISBN 9780982752609."}, {"topic": "Units of measurement", "content": "A unit of measurement is a definite magnitude of a quantity defined and adopted by convention or by law that is used as a standard for measurement of the same quantity. Any other value of that quantity can be expressed as a simple multiple of the unit of measurement. For example length is a physical quantity. The metre is a unit of length that represents a definite predetermined length. When we say 10 metres or 10 m we actually mean 10 times the definite predetermined length called metre. The definition agreement and practical use of units of measurement have played a crucial role in human endeavour from early ages up to this day. Different systems of units used to be very common. Now there is a global standard the International System of Units SI the modern form of the metric system. In trade weights and measures is often a subject of governmental regulation to ensure fairness and transparency. The International Bureau of Weights and Measures BIPM is tasked with ensuring worldwide uniformity of measurements and their traceability to the International System of Units SI. Metrology is the science for developing nationally and internationally accepted units of weights and measures. In physics and metrology units are standards for measurement of physical quantities that need clear definitions to be useful. Reproducibility of experimental results is central to the scientific method. A standard system of units facilitates this. Scientific systems of units are a refinement of the concept of weights and measures developed long ago for commercial purposes. Science medicine and engineering often use larger and smaller units of measurement than those used in everyday life and indicate them more precisely. The judicious selection of the units of measurement can aid researchers in problem solving see for example dimensional analysis. In the social sciences there are no standard units of measurement and the theory and practice of measurement is studied in psychometrics and the theory of conjoint measurement.  British Weights and Measures Association"}, {"topic": "Utility frequency", "content": "The utility frequency power line frequency American English or mains frequency British English is the frequency of the oscillations of alternating current AC in an electric power grid transmitted from a power plant to the enduser. In large parts of the world this is 50 Hz although in the Americas and parts of Asia it is typically 60 Hz. Current usage by country or region is given in the list of mains power around the world. During the development of commercial electric power systems in the late 19th and early 20th centuries many different frequencies and voltages had been used. Large investment in equipment at one frequency made standardization a slow process. However as of the turn of the 21st century places that now use the 50 Hz frequency tend to use 220240 V and those that now use 60 Hz tend to use 100127 V. Both frequencies coexist today Japan uses both with no great technical reason to prefer one over the other and no apparent desire for complete worldwide standardization. Unless specified by the manufacturer to operate on both 50 and 60 Hz appliances may not operate efficiently or even safely if used on anything other than the intended frequency. "}, {"topic": "Vacuum", "content": "Vacuum is space void of matter. The word stems from the Latin adjective vacuus for vacant or void. An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a perfect vacuum which they sometimes simply call vacuum or free space and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand vacuum refers to any space in which the pressure is lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum. The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal lower gas pressure means higherquality vacuum. For example a typical vacuum cleaner produces enough suction to reduce air pressure by around 20. Much higherquality vacuums are possible. Ultrahigh vacuum chambers common in chemistry physics and engineering operate below one trillionth 1012 of atmospheric pressure 100 nPa and can reach around 100 particlescm3. Outer space is an even higherquality vacuum with the equivalent of just a few hydrogen atoms per cubic meter on average. According to modern understanding even if all matter could be removed from a volume it would still not be empty due to vacuum fluctuations dark energy transiting gamma rays cosmic rays neutrinos and other phenomena in quantum physics. In the electromagnetism in the 19th century vacuum was thought to be filled with a medium called aether. In modern particle physics the vacuum state is considered the ground state of matter. Vacuum has been a frequent topic of philosophical debate since ancient Greek times but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643 and other experimental techniques were developed as a result of his theories of atmospheric pressure. A torricellian vacuum is created by filling a tall glass container closed at one end with mercury and then inverting the container into a bowl to contain the mercury. Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes and a wide array of vacuum technology has since become available. The recent development of human spaceflight has raised interest in the impact of vacuum on human health and on life forms in general.  VIDEO on the nature of vacuum by Canadian astrophysicist Doctor P The Foundations of Vacuum Coating Technology American Vacuum Society Journal of Vacuum Science and Technology A Journal of Vacuum Science and Technology B FAQ on explosive decompression and vacuum exposure. Discussion of the effects on humans of exposure to hard vacuum. Roberts Mark D. 2000. Vacuum Energy. High Energy Physics Theory 12062. arXivhepth0012062. Bibcode2000hep.th...12062R. Vacuum Production of Space Much Ado About Nothing by Professor John D. Barrow Gresham College Free pdf copy of The Structured Vacuum thinking about nothing by Johann Rafelski and Berndt Muller 1985 ISBN 3871448893."}, {"topic": "Valence electron", "content": "In chemistry a valence electron is an electron that is associated with an atom and that can participate in the formation of a chemical bond in a single covalent bond both atoms in the bond contribute one valence electron in order to form a shared pair. The presence of valence electrons can determine the elements chemical properties and whether it may bond with other elements For a main group element a valence electron can only be in the outermost electron shell. In a transition metal a valence electron can also be in an inner shell. An atom with a closed shell of valence electrons corresponding to an electron configuration s2p6 tends to be chemically inert. Atoms with one or two more valence electrons than are needed for a closed shell are highly reactive because it requires relatively low energy compared to the lattice enthalpy to remove the extra valence electrons to form a positive ion. Atoms with one or two valence electrons fewer than are needed to form a closed shell are also highly reactive because of a tendency either to gain the missing valence electrons thereby forming a negative ion or to share valence electrons thereby forming a covalent bond. Like an electron in an inner shell a valence electron has the ability to absorb or release energy in the form of a photon. An energy gain can trigger an electron to move jump to an outer shell this is known as atomic excitation. Or the electron can even break free from its associated atoms valence shell this is ionization to form a positive ion. When an electron loses energy thereby causing a photon to be emitted then it can move to an inner shell which is not fully occupied. Valence energy levels correspond to the principal quantum numbers n  1 2 3 4 ... or are labeled alphabetically with letters used in the Xray notation K L M .  Francis Eden. Valence Electrons."}, {"topic": "Valence shell", "content": "In chemistry and atomic physics an electron shell or a principal energy level may be thought of as an orbit followed by electrons around an atoms nucleus. The closest shell to the nucleus is called the 1 shell also called K shell followed by the 2 shell or L shell then the 3 shell or M shell and so on farther and farther from the nucleus. The shells correspond with the principal quantum numbers n  1 2 3 4 ... or are labeled alphabetically with letters used in the Xray notation K L M . Each shell can contain only a fixed number of electrons The first shell can hold up to two electrons the second shell can hold up to eight 2  6 electrons the third shell can hold up to 18 2  6  10 and so on. The general formula is that the nth shell can in principle hold up to 2n2 electrons. Since electrons are electrically attracted to the nucleus an atoms electrons will generally occupy outer shells only if the more inner shells have already been completely filled by other electrons. However this is not a strict requirement atoms may have two or even three incomplete outer shells. See Madelung rule for more details. For an explanation of why electrons exist in these shells see electron configuration. The electrons in the outermost occupied shell or shells determine the chemical properties of the atom it is called the valence shell. Each shell consists of one or more subshells and each subshell consists of one or more atomic orbitals. "}, {"topic": "Variable capacitor", "content": "A variable capacitor is a capacitor whose capacitance may be intentionally and repeatedly changed mechanically or electronically. Variable capacitors are often used in LC circuits to set the resonance frequency e.g. to tune a radio therefore it is sometimes called a tuning capacitor or tuning condenser or as a variable reactance e.g. for impedance matching in antenna tuners.  Build your own air variable capacitor Highres images of historical variable capacitors Introduction to capacitors Introduction to Variable Vacuum Capacitor"}, {"topic": "Variable resistor", "content": "A potentiometer informally a pot is a threeterminal resistor with a sliding or rotating contact that forms an adjustable voltage divider. If only two terminals are used one end and the wiper it acts as a variable resistor or rheostat. The measuring instrument called a potentiometer is essentially a voltage divider used for measuring electric potential voltage the component is an implementation of the same principle hence its name. Potentiometers are commonly used to control electrical devices such as volume controls on audio equipment. Potentiometers operated by a mechanism can be used as position transducers for example in a joystick. Potentiometers are rarely used to directly control significant power more than a watt since the power dissipated in the potentiometer would be comparable to the power in the controlled load.  .PDF edition of Carl David Todd ed The Potentiometer HandbookMcGraw Hill New York 1975 ISBN 0070066906 Beginners Guide to Potentiometers Rheostat  Interactive Tutorial National High Magnetic Field Laboratory Pictures of measuring potentiometers Electrical calibration equipment including various measurement potentiometers The Secret Life of Pots  Dissecting and repairing potentiometers Making a rheostat Potentiometer calculations as voltage divider  loaded and open circuit unloaded"}, {"topic": "Vector (mathematics and physics)", "content": "When used without any further description vector refers either to Most generally an element of a vector space In physics and geometry a Euclidean vector used to represent physical quantities that have both magnitude and direction Vector can also have a variety of different meanings depending on context.  Vector disambiguation Ricci calculus"}, {"topic": "Vector space", "content": "A vector space also called a linear space is a collection of objects called vectors which may be added together and multiplied scaled by numbers called scalars in this context. Scalars are often taken to be real numbers but there are also vector spaces with scalar multiplication by complex numbers rational numbers or generally any field. The operations of vector addition and scalar multiplication must satisfy certain requirements called axioms listed below. Euclidean vectors are an example of a vector space. They represent physical quantities such as forces any two forces of the same type can be added to yield a third and the multiplication of a force vector by a real multiplier is another force vector. In the same vein but in a more geometric sense vectors representing displacements in the plane or in threedimensional space also form vector spaces. Vectors in vector spaces do not necessarily have to be arrowlike objects as they appear in the mentioned examples vectors are regarded as abstract mathematical objects with particular properties which in some cases can be visualized as arrows. Vector spaces are the subject of linear algebra and are well characterized by their dimension which roughly speaking specifies the number of independent directions in the space. Infinitedimensional vector spaces arise naturally in mathematical analysis as function spaces whose vectors are functions. These vector spaces are generally endowed with additional structure which may be a topology allowing the consideration of issues of proximity and continuity. Among these topologies those that are defined by a norm or inner product are more commonly used as having a notion of distance between two vectors. This is particularly the case of Banach spaces and Hilbert spaces which are fundamental in mathematical analysis. Historically the first ideas leading to vector spaces can be traced back as far as the 17th centurys analytic geometry matrices systems of linear equations and Euclidean vectors. The modern more abstract treatment first formulated by Giuseppe Peano in 1888 encompasses more general objects than Euclidean space but much of the theory can be seen as an extension of classical geometric ideas like lines planes and their higherdimensional analogs. Today vector spaces are applied throughout mathematics science and engineering. They are the appropriate linearalgebraic notion to deal with systems of linear equations offer a framework for Fourier expansion which is employed in image compression routines or provide an environment that can be used for solution techniques for partial differential equations. Furthermore vector spaces furnish an abstract coordinatefree way of dealing with geometrical and physical objects such as tensors. This in turn allows the examination of local properties of manifolds by linearization techniques. Vector spaces may be generalized in several ways leading to more advanced notions in geometry and abstract algebra.  Hazewinkel Michiel ed. 2001 Vector space Encyclopedia of Mathematics Springer ISBN 9781556080104 A lecture about fundamental concepts related to vector spaces given at MIT A graphical simulator for the concepts of span linear dependency base and dimension"}, {"topic": "Velocity", "content": "The velocity of an object is the rate of change of its position with respect to a frame of reference and is a function of time. Velocity is equivalent to a specification of its speed and direction of motion e.g. 700116666666666666660 kmh to the north. Velocity is an important concept in kinematics the branch of classical mechanics that describes the motion of bodies. Velocity is a physical vector quantity both magnitude and direction are needed to define it. The scalar absolute value magnitude of velocity is called speed being a coherent derived unit whose quantity is measured in the SI metric system as metres per second ms or as the SI base unit of ms1. For example 5 metres per second is a scalar not a vector whereas 5 metres per second east is a vector. If there is a change in speed direction or both then the object has a changing velocity and is said to be undergoing an acceleration.  physicsabout.com Speed and Velocity Velocity and Acceleration Introduction to Mechanisms Carnegie Mellon University"}, {"topic": "Virophysics", "content": "Virophysics is a branch of biophysics in which the theoretical concepts and experimental techniques of physics are applied to study the mechanics and dynamics driving the interactions between virus and cells. Virophysics can also be considered the theoretical counterpart to virology a field of research advanced almost exclusively through experimental investigation.  Virophysics 2015 2nd Workshop on Virus Dynamics"}, {"topic": "Virtual particle", "content": "In physics a virtual particle is a transient fluctuation that exhibits many of the characteristics of an ordinary particle but that exists for a limited time. The concept of virtual particles arises in perturbation theory of quantum field theory where interactions between ordinary particles are described in terms of exchanges of virtual particles. Any process involving virtual particles admits a schematic representation known as a Feynman diagram in which virtual particles are represented by internal lines. Virtual particles do not necessarily carry the same mass as the corresponding real particle although they always conserve energy and momentum. The longer the virtual particle exists the closer its characteristics come to those of ordinary particles. They are important in the physics of many processes including particle scattering and Casimir forces. In quantum field theory even classical forces such as the electromagnetic repulsion or attraction between two charges can be thought of as due to the exchange of many virtual photons between the charges. The term is somewhat loose and vaguely defined in that it refers to the view that the world is made up of real particles it is not rather real particles are better understood to be excitations of the underlying quantum fields. Virtual particles are also excitations of the underlying fields but are temporary in the sense that they appear in calculations of interactions but never as asymptotic states or indices to the scattering matrix. As such the accuracy and use of virtual particles in calculations is firmly established but their reality or existence is a question of philosophy rather than science. Antiparticles have been proven to exist and should not be confused with virtual particles or virtual antiparticles.  Are virtual particles really constantly popping in and out of existence Gordon Kane director of the Michigan Center for Theoretical Physics at the University of Michigan at Ann Arbor proposes an answer at the Scientific American website. Virtual Particles What are they D Kaiser 2005 American Scientist 93 p. 156 popular article"}, {"topic": "Viscoelasticity", "content": "Viscoelasticity is the property of materials that exhibit both viscous and elastic characteristics when undergoing deformation. Viscous materials like honey resist shear flow and strain linearly with time when a stress is applied. Elastic materials strain when stretched and quickly return to their original state once the stress is removed. Viscoelastic materials have elements of both of these properties and as such exhibit timedependent strain. Whereas elasticity is usually the result of bond stretching along crystallographic planes in an ordered solid viscosity is the result of the diffusion of atoms or molecules inside an amorphous material.  Silbey and Alberty 2001 Physical Chemistry 857. John Wiley  Sons Inc. Allen and Thomas 1999 The Structure of Materials 51. Crandal et al. 1999 An Introduction to the Mechanics of Solids 348 J. Lemaitre and J. L. Chaboche 1994 Mechanics of solid materials Yu. Dimitrienko 2011 Nonlinear continuum mechanics and Large Inelastic Deformations Springer 772p"}, {"topic": "Viscosity", "content": "The viscosity of a fluid is a measure of its resistance to gradual deformation by shear stress or tensile stress. For liquids it corresponds to the informal concept of thickness for example honey has a much higher viscosity than water. Viscosity is a property arising from collisions between neighboring particles in a fluid that are moving at different velocities. When the fluid is forced through a tube the particles which compose the fluid generally move more quickly near the tubes axis and more slowly near its walls therefore some stress such as a pressure difference between the two ends of the tube is needed to overcome the friction between particle layers to keep the fluid moving. For a given velocity pattern the stress required is proportional to the fluids viscosity. A fluid that has no resistance to shear stress is known as an ideal or inviscid fluid. Zero viscosity is observed only at very low temperatures in superfluids. Otherwise all fluids have positive viscosity and are technically said to be viscous or viscid. In common parlance however a liquid is said to be viscous if its viscosity is substantially greater than that of water and may be described as mobile if the viscosity is noticeably less than water. A fluid with a relatively high viscosity such as pitch may appear to be a solid.  Fluid properties High accuracy calculation of viscosity and other physical properties of frequent used pure liquids and gases. Gas viscosity calculator as function of temperature Air viscosity calculator as function of temperature and pressure Fluid Characteristics Chart A table of viscosities and vapor pressures for various fluids Gas Dynamics Toolbox Calculate coefficient of viscosity for mixtures of gases Glass Viscosity Measurement Viscosity measurement viscosity units and fixpoints glass viscosity calculation Kinematic Viscosity conversion between kinematic and dynamic viscosity. Physical Characteristics of Water A table of water viscosity as a function of temperature VogelTammannFulcher Equation Parameters Calculation of temperaturedependent dynamic viscosities for some common components Test Procedures for Testing Highway and Nonroad Engines and Omnibus Technical Amendments. United States Environmental Protection Agency Artificial viscosity"}, {"topic": "Visible light", "content": "Light is electromagnetic radiation within a certain portion of the electromagnetic spectrum. The word usually refers to visible light which is visible to the human eye and is responsible for the sense of sight. Visible light is usually defined as having wavelengths in the range of 400700 nanometres nm or 4.00 107 to 7.00 107 m between the infrared with longer wavelengths and the ultraviolet with shorter wavelengths. This wavelength means a frequency range of roughly 430750 terahertz THz. The main source of light on Earth is the Sun. Sunlight provides the energy that green plants use to create sugars mostly in the form of starches which release energy into the living things that digest them. This process of photosynthesis provides virtually all the energy used by living things. Historically another important source of light for humans has been fire from ancient campfires to modern kerosene lamps. With the development of electric lights and power systems electric lighting has effectively replaced firelight. Some species of animals generate their own light a process called bioluminescence. For example fireflies use light to locate mates and vampire squids use it to hide themselves from prey. The primary properties of visible light are intensity propagation direction frequency or wavelength spectrum and polarization while its speed in a vacuum 299792458 metres per second is one of the fundamental constants of nature. Visible light as with all types of electromagnetic radiation EMR is experimentally found to always move at this speed in a vacuum. In physics the term light sometimes refers to electromagnetic radiation of any wavelength whether visible or not. In this sense gamma rays Xrays microwaves and radio waves are also light. Like all types of light visible light is emitted and absorbed in tiny packets called photons and exhibits properties of both waves and particles. This property is referred to as the waveparticle duality. The study of light known as optics is an important research area in modern physics.  Media related to Light at Wikimedia Commons The dictionary definition of light at Wiktionary Quotations related to Light at Wikiquote"}, {"topic": "Volta potential", "content": "The Volta potential also called Volta potential difference contact potential difference outer potential difference  or delta psi in electrochemistry is the electrostatic potential difference between two metals or one metal and one electrolyte that are in contact and are in thermodynamic equilibrium. Specifically it is the potential difference between a point close to the surface of the first metal and a point close to the surface of the second metal or electrolyte. The Volta potential is named after Alessandro Volta. "}, {"topic": "Voltmeter", "content": "A voltmeter is an instrument used for measuring electrical potential difference between two points in an electric circuit. Analog voltmeters move a pointer across a scale in proportion to the voltage of the circuit digital voltmeters give a numerical display of voltage by use of an analog to digital converter. A voltmeter in a circuit diagram is represented by the letter V in a circle. Voltmeters are made in a wide range of styles. Instruments permanently mounted in a panel are used to monitor generators or other fixed apparatus. Portable instruments usually equipped to also measure current and resistance in the form of a multimeter are standard test instruments used in electrical and electronics work. Any measurement that can be converted to a voltage can be displayed on a meter that is suitably calibrated for example pressure temperature flow or level in a chemical process plant. General purpose analog voltmeters may have an accuracy of a few percent of full scale and are used with voltages from a fraction of a volt to several thousand volts. Digital meters can be made with high accuracy typically better than 1. Specially calibrated test instruments have higher accuracies with laboratory instruments capable of measuring to accuracies of a few parts per million. Meters using amplifiers can measure tiny voltages of microvolts or less. Part of the problem of making an accurate voltmeter is that of calibration to check its accuracy. In laboratories the Weston Cell is used as a standard voltage for precision work. Precision voltage references are available based on electronic circuits. "}, {"topic": "Watt", "content": "The watt symbol W is a derived unit of power in the International System of Units SI named after the Scottish engineer James Watt 17361819. The unit is defined as 1 joule per second and can be used to express the rate of energy conversion or transfer with respect to time. It has dimensions of MassLength2Time3.  Borvon Grard History of the electrical units FR Free . Nelson Robert A February 2000 The International System of Units Its History and Use in Science and Industry Via Satellite ATI courses ."}, {"topic": "Wave", "content": "In physics a wave is an oscillation accompanied by a transfer of energy that travels through medium space or mass. Frequency refers to the addition of time. Wave motion transfers energy from one point to another which displace particles of the transmission medium that is with little or no associated mass transport. Waves consist instead of oscillations or vibrations of a physical quantity around almost fixed locations. There are two main types of waves. Mechanical waves propagate through a medium and the substance of this medium is deformed. The deformation reverses itself owing to restoring forces resulting from its deformation. For example sound waves propagate via air molecules colliding with their neighbors. When air molecules collide they also bounce away from each other a restoring force. This keeps the molecules from continuing to travel in the direction of the wave. The second main type of wave electromagnetic waves do not require a medium. Instead they consist of periodic oscillations of electrical and magnetic fields originally generated by charged particles and can therefore travel through a vacuum. These types of waves vary in wavelength and include radio waves microwaves infrared radiation visible light ultraviolet radiation Xrays and gamma rays. Waves are described by a wave equation which sets out how the disturbance proceeds over time. The mathematical form of this equation varies depending on the type of wave. Further the behavior of particles in quantum mechanics are described by waves. In addition gravitational waves also travel through space which are a result of a vibration or movement in gravitational fields. A wave can be transverse or longitudinal. Transverse waves occur when a disturbance creates oscillations that are perpendicular to the propagation of energy transfer. Longitudinal waves occur when the oscillations are parallel to the direction of energy propagation. While mechanical waves can be both transverse and longitudinal all electromagnetic waves are transverse in free space.  Interactive Visual Representation of Waves Linear and nonlinear waves Science Aid Wave propertiesConcise guide aimed at teens Easy JavaScript Simulation Model of One Dimensional Wave Interference Simulation of diffraction of water wave passing through a gap Simulation of interference of water waves Simulation of longitudinal traveling wave Simulation of stationary wave on a string Simulation of transverse traveling wave Sounds AmazingAS and ALevel learning resource for sound and waves chapter from an online textbook Simulation of waves on a string of longitudinal and transverse mechanical wave MIT OpenCourseWare 8.03 Vibrations and Waves Free independent study course with video lectures assignments lecture notes and exams."}, {"topic": "Wave equation", "content": "The wave equation is an important secondorder linear partial differential equation for the description of wavesas they occur in physicssuch as sound waves light waves and water waves. It arises in fields like acoustics electromagnetics and fluid dynamics. Historically the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond dAlembert Leonhard Euler Daniel Bernoulli and JosephLouis Lagrange. In 1746 dAlembert discovered the onedimensional wave equation and within ten years Euler discovered the threedimensional wave equation.  Nonlinear Wave Equations by Stephen Wolfram and Rob Knapp Nonlinear Wave Equation Explorer by Wolfram Demonstrations Project. Mathematical aspects of wave equations are discussed on the Dispersive PDE Wiki. Graham W Griffiths and William E. Schiesser 2009. Linear and nonlinear waves. Scholarpedia 474308. doi10.4249scholarpedia.4308"}, {"topic": "Wave function", "content": "A wave function in quantum mechanics is a description of the quantum state of a system. The wave function is a complexvalued probability amplitude and the probabilities for the possible results of measurements made on the system can be derived from it. The most common symbols for a wave function are the Greek letters or lowercase and capital psi. The wave function is a function of the degrees of freedom corresponding to some maximal set of commuting observables. Once such a representation is chosen the wave function can be derived from the quantum state. For a given system the choice of which commuting degrees of freedom to use is not unique and correspondingly the domain of the wave function is not unique. For instance it may be taken to be a function of all the position coordinates of the particles over position space or the momenta of all the particles over momentum space the two are related by a Fourier transform. Some particles like electrons and photons have nonzero spin and the wave function for such particles includes spin as an intrinsic discrete degree of freedom. Other discrete variables can also be included such as isospin. When a system has internal degrees of freedom the wave function at each point in the continuous degrees of freedom e.g. a point in space assigns a complex number for each possible value of the discrete degrees of freedom e.g. zcomponent of spin. These values are often displayed in a column matrix e.g. a 2 1 column vector for a nonrelativistic electron with spin 12. According to the superposition principle of quantum mechanics wave functions can be added together and multiplied by complex numbers to form new wave functions and form a Hilbert space. The inner product between two wave functions is a measure of the overlap between the corresponding physical states and is used in the foundational probabilistic interpretation of quantum mechanics the Born rule relating transition probabilities to inner products. The Schrdinger equation determines how wave functions evolve over time. A wave function behaves qualitatively like other waves such as water waves or waves on a string because the Schrdinger equation is mathematically a type of wave equation. This explains the name wave function and gives rise to waveparticle duality. However the wave function in quantum mechanics describes a kind of physical phenomenons still open to different interpretations which fundamentally differs from that of classic mechanical waves. In Borns statistical interpretation the squared modulus of the wave function 2 is a real number interpreted as the probability density of measuring a particles being detected at a given place or having a given momentum at a given time and possibly having definite values for discrete degrees of freedom. The integral of this quantity over all the systems degrees of freedom must be 1 in accordance with the probability interpretation this general requirement a wave function must satisfy is called the normalization condition. Since the wave function is complex valued only its relative phase and relative magnitude can be measured. Its value does not in isolation tell anything about the magnitudes or directions of measurable observables one has to apply quantum operators whose eigenvalues correspond to sets of possible results of measurements to the wave function and calculate the statistical distributions for measurable quantities.  1 2 3 4 5 Normalization. 6 Quantum Mechanics and Quantum Computation at BerkeleyX Einstein The quantum theory of radiation"}, {"topic": "Wavelength", "content": "In physics the wavelength of a sinusoidal wave is the spatial period of the wavethe distance over which the waves shape repeats and the inverse of the spatial frequency. It is usually determined by considering the distance between consecutive corresponding points of the same phase such as crests troughs or zero crossings and is a characteristic of both traveling waves and standing waves as well as other spatial wave patterns. Wavelength is commonly designated by the Greek letter lambda . The concept can also be applied to periodic waves of nonsinusoidal shape. The term wavelength is also sometimes applied to modulated waves and to the sinusoidal envelopes of modulated waves or waves formed by interference of several sinusoids. Assuming a sinusoidal wave moving at a fixed wave speed wavelength is inversely proportional to frequency of the wave waves with higher frequencies have shorter wavelengths and lower frequencies have longer wavelengths. Wavelength depends on the medium for example vacuum air or water that a wave travels through. Examples of wavelike phenomena are sound waves light and water waves. A sound wave is a variation in air pressure while in light and other electromagnetic radiation the strength of the electric and the magnetic field vary. Water waves are variations in the height of a body of water. In a crystal lattice vibration atomic positions vary. Wavelength is a measure of the distance between repetitions of a shape feature such as peaks valleys or zerocrossings not a measure of how far any given particle moves. For example in sinusoidal waves over deep water a particle near the waters surface moves in a circle of the same diameter as the wave height unrelated to wavelength. The range of wavelengths or frequencies for wave phenomena is called a spectrum. The name originated with the visible light spectrum but now can be applied to the entire electromagnetic spectrum as well as to a sound spectrum or vibration spectrum.  Conversion Wavelength to Frequency and vice versa Sound waves and radio waves Teaching resource for 1416 years on sound including wavelength The visible electromagnetic spectrum displayed in web colors with according wavelengths"}, {"topic": "Weak interaction", "content": "In particle physics the weak interaction the weak force or weak nuclear force is one of the four known fundamental interactions of nature alongside the strong interaction electromagnetism and gravitation. The weak interaction is responsible for radioactive decay which plays an essential role in nuclear fission. The theory of the weak interaction is sometimes called quantum flavordynamics QFD in analogy with the terms QCD and QED but the term is rarely used because the weak force is best understood in terms of electroweak theory EWT. In the Standard Model of particle physics the weak interaction is caused by the emission or absorption of the force carriers the W and Z bosons. All known fermions interact through the weak interaction. Fermions are particles that have halfinteger spin. Spin is one of the fundamental properties of particles. A fermion can be an elementary particle such as the electron or it can be a composite particle such as the proton. The masses of W W and Z bosons are each far greater than that of interacting protons or neutrons which is consistent with the short range of the weak force. The force is termed weak because its field strength over a given distance is typically several orders of magnitude less than that of the strong nuclear force and electromagnetic force. During the quark epoch of the early universe the electroweak force separated into the electromagnetic and weak forces. Important examples of the weak interaction include beta decay and the fusion of hydrogen into deuterium that powers the Suns thermonuclear process. Most fermions will decay by a weak interaction over time. Such decay makes radiocarbon dating possible as carbon14 decays through the weak interaction to nitrogen14. It can also create radioluminescence commonly used in tritium illumination and in the related field of betavoltaics. Quarks which make up composite particles like neutrons and protons come in six flavours up down strange charm top and bottom which give those composite particles their properties. The weak interaction is unique in that it allows for quarks to swap their flavour for another. The swapping of those properties is mediated by the force carrier bosons. For example during beta minus decay a down quark within a neutron is changed into an up quark converting the neutron to a proton and resulting in the emission of an electron and an electron antineutrino. Also the weak interaction is the only fundamental interaction that breaks paritysymmetry and similarly the only one to break charge parity symmetry.  D.A. Bromley 2000. Gauge Theory of Weak Interactions. Springer. ISBN 3540676724. G.D. Coughlan J.E. Dodd B.M. Gripaios 2006. The Ideas of Particle Physics An Introduction for Scientists 3rd ed.. Cambridge University Press. ISBN 9780521677752. W. N. Cottingham D. A. Greenwood 2001 1986. An introduction to nuclear physics 2nd ed.. Cambridge University Press. p. 30. ISBN 9780521657334. D.J. Griffiths 1987. Introduction to Elementary Particles. John Wiley  Sons. ISBN 0471603864. G.L. Kane 1987. Modern Elementary Particle Physics. Perseus Books. ISBN 0201117495. D.H. Perkins 2000. Introduction to High Energy Physics. Cambridge University Press. ISBN 0521621968."}, {"topic": "Weber (unit)", "content": "In physics the weber vebr symbol Wb is the SI unit of magnetic flux. A flux density of one Wbm2 one weber per square metre is one tesla. The weber is named after the German physicist Wilhelm Eduard Weber 18041891. "}, {"topic": "Wedge (mechanical device)", "content": "A wedge is a triangular shaped tool and is a portable inclined plane and one of the six classical simple machines. It can be used to separate two objects or portions of an object lift up an object or hold an object in place. It functions by converting a force applied to its blunt end into forces perpendicular normal to its inclined surfaces. The mechanical advantage of a wedge is given by the ratio of the length of its slope to its width. Although a short wedge with a wide angle may do a job faster it requires more force than a long wedge with a narrow angle. "}, {"topic": "Wheel and axle", "content": "The wheel and axle is one of six simple machines identified by Renaissance scientists drawing from Greek texts on technology. The wheel and axle is generally considered to be a wheel attached to an axle so that these two parts rotate together in which a force is transferred from one to the other. In this configuration a hinge or bearing supports the rotation of the axle. Hero of Alexandria identified the wheel and axle as one of the six simple machines used to lift weights. This is thought to have been in the form of windlass which consists of a crank or pulley connected to a cylindrical barrel that provides mechanical advantage to wind up a rope and lift a load such as a bucket from the well. This system is a version of the lever with loads applied tangentially to the perimeter of the wheel and axle respectively that are balanced around the hinge which is the fulcrum. The mechanical advantage of the wheel and axle is the ratio of the distances from the fulcrum to the applied loads or what is the same thing the ratio of the radial dimensions of the wheel and axle.  Basic Machines and How They Work United States. Bureau of Naval Personnel Courier Dover Publications 1965 pp. 31 and following preview online"}, {"topic": "White dwarf", "content": "A white dwarf also called a degenerate dwarf is a stellar remnant composed mostly of electrondegenerate matter. A white dwarf is very dense its mass is comparable to that of the Sun while its volume is comparable to that of Earth. A white dwarfs faint luminosity comes from the emission of stored thermal energy no fusion takes place in a white dwarf wherein mass is converted to energy. The nearest known white dwarf is Sirius B at 8.6 light years the smaller component of the Sirius binary star. There are currently thought to be eight white dwarfs among the hundred star systems nearest the Sun. The unusual faintness of white dwarfs was first recognized in 1910. The name white dwarf was coined by Willem Luyten in 1922. The universe has not existed long enough to experience a white dwarf releasing all of its energy as it will take close to a trillion years. White dwarfs are thought to be the final evolutionary state of stars whose mass is not high enough to become a neutron star including the Earths Sun and over 97 of the other stars in the Milky Way. 1. After the hydrogenfusing period of a mainsequence star of low or medium mass ends such a star will expand to a red giant during which it fuses helium to carbon and oxygen in its core by the triplealpha process. If a red giant has insufficient mass to generate the core temperatures required to fuse carbon around 1 billion K an inert mass of carbon and oxygen will build up at its center. After a star sheds its outer layers and forms a planetary nebula it will leave behind this core which is the remnant white dwarf. Usually therefore white dwarfs are composed of carbon and oxygen. If the mass of the progenitor is between 8 and 10.5 solar masses M the core temperature is sufficient to fuse carbon but not neon in which case an oxygenneonmagnesium white dwarf may form. Stars of very low mass will not be able to fuse helium hence a helium white dwarf may form by mass loss in binary systems. The material in a white dwarf no longer undergoes fusion reactions so the star has no source of energy. As a result it cannot support itself by the heat generated by fusion against gravitational collapse but is supported only by electron degeneracy pressure causing it to be extremely dense. The physics of degeneracy yields a maximum mass for a nonrotating white dwarf the Chandrasekhar limitapproximately 1.46 Mbeyond which it cannot be supported by electron degeneracy pressure. A carbonoxygen white dwarf that approaches this mass limit typically by mass transfer from a companion star may explode as a type Ia supernova via a process known as carbon detonation. SN 1006 is thought to be a famous example. A white dwarf is very hot when it forms but because it has no source of energy it will gradually radiate its energy and cool. This means that its radiation which initially has a high color temperature will lessen and redden with time. Over a very long time a white dwarf will cool and its material will begin to crystallize starting with the core. The stars low temperature means it will no longer emit significant heat or light and it will become a cold black dwarf. The length of time it takes for a white dwarf to reach this state is calculated to be longer than the current age of the universe approximately 13.8 billion years and it is thought that no black dwarfs yet exist. The oldest white dwarfs still radiate at temperatures of a few thousand kelvin.  Astronomy Picture of the Day NGC 2440 Cocoon of a New White Dwarf 2010 February 21 Dust and the Helix Nebula 2009 December 31 The Helix Nebula from La Silla Observatory 2009 March 3 IC 4406 A Seemingly Square Nebula 2008 July 27 A Nearby Supernova in Spiral Galaxy M100 2006 March 7 Astronomy Picture of the Day White Dwarf Star Spiral 2005 June 1"}, {"topic": "Wind", "content": "Wind is the flow of gases on a large scale. On the surface of the Earth wind consists of the bulk movement of air. In outer space solar wind is the movement of gases or charged particles from the Sun through space while planetary wind is the outgassing of light chemical elements from a planets atmosphere into space. Winds are commonly classified by their spatial scale their speed the types of forces that cause them the regions in which they occur and their effect. The strongest observed winds on a planet in the Solar System occur on Neptune and Saturn. Winds have various aspects an important one being its velocity another the density of the gas involved another its energy content or wind energy. In meteorology winds are often referred to according to their strength and the direction from which the wind is blowing. Short bursts of high speed wind are termed gusts. Strong winds of intermediate duration around one minute are termed squalls. Longduration winds have various names associated with their average strength such as breeze gale storm and hurricane. Wind occurs on a range of scales from thunderstorm flows lasting tens of minutes to local breezes generated by heating of land surfaces and lasting a few hours to global winds resulting from the difference in absorption of solar energy between the climate zones on Earth. The two main causes of largescale atmospheric circulation are the differential heating between the equator and the poles and the rotation of the planet Coriolis effect. Within the tropics thermal low circulations over terrain and high plateaus can drive monsoon circulations. In coastal areas the sea breezeland breeze cycle can define local winds in areas that have variable terrain mountain and valley breezes can dominate local winds. In human civilization wind has inspired mythology influenced the events of history expanded the range of transport and warfare and provided a power source for mechanical work electricity and recreation. Wind powers the voyages of sailing ships across Earths oceans. Hot air balloons use the wind to take short trips and powered flight uses it to increase lift and reduce fuel consumption. Areas of wind shear caused by various weather phenomena can lead to dangerous situations for aircraft. When winds become strong trees and manmade structures are damaged or destroyed. Winds can shape landforms via a variety of aeolian processes such as the formation of fertile soils such as loess and by erosion. Dust from large deserts can be moved great distances from its source region by the prevailing winds winds that are accelerated by rough topography and associated with dust outbreaks have been assigned regional names in various parts of the world because of their significant effects on those regions. Wind also affects the spread of wildfires. Winds can disperse seeds from various plants enabling the survival and dispersal of those plant species as well as flying insect populations. When combined with cold temperatures wind has a negative impact on livestock. Wind affects animals food stores as well as their hunting and defensive strategies.  Meteorology Guides Forces and Winds Instructional module from the University of Illinois Names of Winds A list from Golden Gate Weather Services Wind Atlases of the World Lists of wind atlases and wind surveys from all over the world Winds of Mars Aeolian Activity and Landforms Paper with slides that illustrate the wind activity on the planet Mars Classification of Wind Speeds Windspeed chart The Bibliography of Aeolian Research"}, {"topic": "Wind gradient", "content": "In common usage wind gradient more specifically wind speed gradient or wind velocity gradient or alternatively shear wind is the vertical gradient of the mean horizontal wind speed in the lower atmosphere. It is the rate of increase of wind strength with unit increase in height above ground level. In metric units it is often measured in units of meters per second of speed per kilometer of height mskm which reduces to the standard unit of shear rate inverse seconds s1. "}, {"topic": "Wind shear", "content": "Wind shear sometimes referred to as windshear or wind gradient is a difference in wind speed andor direction over a relatively short distance in the atmosphere. Atmospheric wind shear is normally described as either vertical or horizontal wind shear. Vertical wind shear is a change in wind speed or direction with change in altitude. Horizontal wind shear is a change in wind speed with change in lateral position for a given altitude. Wind shear is a microscale meteorological phenomenon occurring over a very small distance but it can be associated with mesoscale or synoptic scale weather features such as squall lines and cold fronts. It is commonly observed near microbursts and downbursts caused by thunderstorms fronts areas of locally higher lowlevel winds referred to as low level jets near mountains radiation inversions that occur due to clear skies and calm winds buildings wind turbines and sailboats. Wind shear has a significant effect during takeoff and landing of aircraft due to its effects on control of the aircraft and it has been a sole or contributing cause of many aircraft accidents. Wind shear is sometimes experienced by pedestrians at ground level when walking across a plaza towards a tower block and suddenly encountering a strong wind stream that is flowing around the base of the tower. This phenomenon is a concern for architects. Sound movement through the atmosphere is affected by wind shear which can bend the wave front causing sounds to be heard where they normally would not or vice versa. Strong vertical wind shear within the troposphere also inhibits tropical cyclone development but helps to organize individual thunderstorms into longer life cycles which can then produce severe weather. The thermal wind concept explains how differences in wind speed at different heights are dependent on horizontal temperature differences and explains the existence of the jet stream.  National Science Digital Library  Wind shear"}, {"topic": "Work hardening", "content": "Work hardening also known as strain hardening or cold working is the strengthening of a metal by plastic deformation. This strengthening occurs because of dislocation movements and dislocation generation within the crystal structure of the material. Many nonbrittle metals with a reasonably high melting point as well as several polymers can be strengthened in this fashion. Alloys not amenable to heat treatment including lowcarbon steel are often workhardened. Some materials cannot be workhardened at low temperatures such as indium however others can only be strengthened via work hardening such as pure copper and aluminum. Work hardening may be desirable or undesirable depending on the context. An example of undesirable work hardening is during machining when early passes of a cutter inadvertently workharden the workpiece surface causing damage to the cutter during the later passes. Certain alloys are more prone to this than others superalloys such as Inconel require machining strategies that take it into account. An example of desirable work hardening is that which occurs in metalworking processes that intentionally induce plastic deformation to exact a shape change. These processes are known as cold working or cold forming processes. They are characterized by shaping the workpiece at a temperature below its recrystallization temperature usually at ambient temperature. Cold forming techniques are usually classified into four major groups squeezing bending drawing and shearing. Applications include the heading of bolts and cap screws and the finishing of cold rolled steel. In cold forming metal is formed at high speed and high pressure using tool steel or carbide dies. The cold working of the metal increasing the hardness yield strength and tensile strength.  Engineers Edge  Work hardening Nuclear Power Fundamentals  Work hardening"}]